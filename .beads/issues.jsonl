{"id":"engram-0bo","title":"Tests: QA generation, QA recall, chain recall e2e","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-25T14:04:59.31101+01:00","updated_at":"2026-02-25T14:32:49.410953+01:00","closed_at":"2026-02-25T14:32:49.410953+01:00","close_reason":"QA tests complete, all 971 tests pass","dependencies":[{"issue_id":"engram-0bo","depends_on_id":"engram-o6m","type":"blocks","created_at":"2026-02-25T14:05:43.870115+01:00","created_by":"daemon"}]}
{"id":"engram-0lr","title":"Retroactive Enrichment on Subspace Update","description":"# Retroactive Enrichment on Subspace Update\n\n## Problem\nWhen the knowledge subspace is updated (new principal direction added, or re-merged), **old facts** still have coefficients from the old subspace. They might benefit from re-projection onto the updated subspace — previously unrelated facts may now cluster meaningfully.\n\n## Solution (from SHARE Paper - Backward Transfer)\nAfter subspace update, **re-project recent facts** onto new subspace:\n1. When subspace version increments (new axis added or SVD re-merge)\n2. Re-compute coefficients for facts from last 30 days\n3. Check if any facts now score higher for common queries\n4. Surface \"retroactively relevant\" facts in recall\n\n## Algorithm\n```typescript\n// convex/subspace/retroactive-enrich.ts\nexport const enrichAfterUpdate = mutation({\n  args: { \n    subspaceId: v.id(\"knowledge_subspaces\"),\n    daysBack: v.number().optional().default(30),\n  },\n  handler: async (ctx, args) =\u003e {\n    const subspace = await ctx.db.get(args.subspaceId);\n    if (!subspace) return;\n    \n    const since = Date.now() - args.daysBack * 24 * 60 * 60 * 1000;\n    \n    // 1. Get recent facts in this scope\n    const facts = await ctx.db\n      .query(\"facts\")\n      .withIndex(\"by_scope\", q =\u003e q.eq(\"scopeId\", subspace.scopeId))\n      .filter(q =\u003e q.gte(q.field(\"timestamp\"), since))\n      .filter(q =\u003e q.neq(q.field(\"compactEmbedding\"), undefined))\n      .collect();\n    \n    // 2. Re-project each fact onto updated subspace\n    const Vk = subspace.principalVectors;\n    const enrichedFacts = [];\n    \n    for (const fact of facts) {\n      // Reconstruct full embedding from old coefficients\n      const oldEmbedding = matmul(fact.compactEmbedding, Vk); // approximate\n      \n      // Re-project onto new subspace\n      const newCoefficients = matmul(oldEmbedding, transpose(Vk));\n      \n      // Check if representation changed significantly\n      const coeffChange = norm(subtract(newCoefficients, fact.compactEmbedding));\n      \n      if (coeffChange \u003e 0.1) {\n        // Significant change → update\n        await ctx.db.patch(fact._id, {\n          compactEmbedding: newCoefficients,\n        });\n        enrichedFacts.push(fact._id);\n      }\n    }\n    \n    // 3. Log enrichment event\n    await ctx.db.insert(\"memory_events\", {\n      eventType: \"retroactive_enrichment\",\n      scopeId: subspace.scopeId,\n      payload: {\n        subspaceVersion: subspace.version,\n        factsEnriched: enrichedFacts.length,\n        totalCandidates: facts.length,\n      },\n      watermark: Date.now(),\n      createdAt: Date.now(),\n    });\n    \n    return { enriched: enrichedFacts.length };\n  },\n});\n```\n\n## Trigger Points\n1. **After streaming integration expands subspace** (engram-rq3):\n   ```typescript\n   if (integration.mode === \"expand\") {\n     await ctx.runMutation(internal.subspace.enrichAfterUpdate, {\n       subspaceId: subspace._id,\n       daysBack: 30,\n     });\n   }\n   ```\n\n2. **After periodic SVD re-merge**:\n   ```typescript\n   // After consolidateScope completes:\n   await ctx.runMutation(internal.subspace.enrichAfterUpdate, {\n     subspaceId: newSubspaceId,\n     daysBack: 60, // Larger window for full re-merge\n   });\n   ```\n\n3. **Manual trigger via MCP tool** (for debugging/analysis)\n\n## Surfacing Retroactively Relevant Facts\n```typescript\n// In recall, optionally boost recently enriched facts:\nconst recentlyEnriched = await ctx.db\n  .query(\"memory_events\")\n  .withIndex(\"by_scope_created\", q =\u003e \n    q.eq(\"scopeId\", scopeId).gte(\"createdAt\", Date.now() - 7 * 24 * 60 * 60 * 1000))\n  .filter(q =\u003e q.eq(q.field(\"eventType\"), \"retroactive_enrichment\"))\n  .first();\n\nif (recentlyEnriched) {\n  // Boost facts in recentlyEnriched.payload.factsEnriched\n  for (const fact of facts) {\n    if (recentlyEnriched.payload.factsEnriched.includes(fact._id)) {\n      fact._score *= 1.2; // 20% boost\n      fact._retroactivelyRelevant = true;\n    }\n  }\n}\n```\n\n## Impact\n- **Backward knowledge transfer** (new knowledge improves old facts)\n- **Emergent clustering** (facts group together as subspace evolves)\n- **Principled re-indexing** (SVD-based, not heuristic)\n- **Minimal cost** (only re-project recent facts, not entire history)\n\n## Dependencies\n- **Requires**: Subspace consolidation (engram-2vw)\n- **Requires**: Streaming integration (engram-rq3)\n\n## References\n- SHARE paper section 4.4 (backward transfer)\n- Analysis doc: docs/SHARE-PAPER-ANALYSIS.md (section 3)","status":"closed","priority":3,"issue_type":"task","created_at":"2026-02-24T09:33:14.967056+01:00","updated_at":"2026-02-25T11:42:40.059509+01:00","closed_at":"2026-02-25T11:42:40.059509+01:00","close_reason":"All 5 children closed. Retroactive enrichment fully implemented and tested.","dependencies":[{"issue_id":"engram-0lr","depends_on_id":"engram-rq3","type":"blocks","created_at":"2026-02-24T09:41:00.138313+01:00","created_by":"daemon"}]}
{"id":"engram-0lr.1","title":"Implement retroactive re-projection","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-24T09:41:30.586985+01:00","updated_at":"2026-02-25T03:13:51.195381+01:00","closed_at":"2026-02-25T03:13:51.195381+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-0lr.1","depends_on_id":"engram-0lr","type":"parent-child","created_at":"2026-02-24T09:41:30.588408+01:00","created_by":"daemon"}]}
{"id":"engram-0lr.2","title":"Trigger on subspace updates","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-24T09:41:30.780318+01:00","updated_at":"2026-02-25T03:15:02.095516+01:00","closed_at":"2026-02-25T03:15:02.095516+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-0lr.2","depends_on_id":"engram-0lr","type":"parent-child","created_at":"2026-02-24T09:41:30.781535+01:00","created_by":"daemon"}]}
{"id":"engram-0lr.3","title":"Surface enriched facts in recall","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-24T09:41:30.968857+01:00","updated_at":"2026-02-25T03:11:59.056553+01:00","closed_at":"2026-02-25T03:11:59.056553+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-0lr.3","depends_on_id":"engram-0lr","type":"parent-child","created_at":"2026-02-24T09:41:30.969958+01:00","created_by":"daemon"}]}
{"id":"engram-0lr.4","title":"Unit tests: Retroactive enrichment","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-24T09:41:31.15934+01:00","updated_at":"2026-02-25T11:42:35.322583+01:00","closed_at":"2026-02-25T11:42:35.322583+01:00","close_reason":"Unit-level coverage exists in backward-transfer-e2e.test.ts (835 lines, 10+ edge cases for retroactiveReproject)","dependencies":[{"issue_id":"engram-0lr.4","depends_on_id":"engram-0lr","type":"parent-child","created_at":"2026-02-24T09:41:31.160693+01:00","created_by":"daemon"}]}
{"id":"engram-0lr.5","title":"E2E test: Backward transfer","status":"closed","priority":3,"issue_type":"task","created_at":"2026-02-24T09:41:31.344377+01:00","updated_at":"2026-02-25T03:43:47.930648+01:00","closed_at":"2026-02-25T03:43:47.930648+01:00","close_reason":"34-test backward transfer E2E: k→k+1 expansion, retroactive re-projection, threshold=0.1, compact reconstruction, enrichment event structure, 20% recall boost, edge cases. All 755 mcp-server tests passing.","dependencies":[{"issue_id":"engram-0lr.5","depends_on_id":"engram-0lr","type":"parent-child","created_at":"2026-02-24T09:41:31.345912+01:00","created_by":"daemon"}]}
{"id":"engram-0ro","title":"Validation suite: golden tests + regression benchmarks","description":"Comprehensive validation suite: golden tests, regression, benchmarks:\n\n**Test structure:**\ntests/\n  ├── unit/ (9 files)\n  │   ├── vault-writer.test.ts — File creation, atomic writes, folder structure\n  │   ├── vault-reader.test.ts — Parse frontmatter, handle malformed YAML\n  │   ├── vault-reconciler.test.ts — Three-way merge, conflict detection\n  │   ├── wiki-link-parser.test.ts — Extract links, edge cases\n  │   ├── auto-linker.test.ts — Entity matching, duplicate avoidance\n  │   ├── vault-indexer.test.ts — Index generation, sorting\n  │   ├── budget-aware-loader.test.ts — Token counting, tier allocation\n  │   ├── frontmatter-generator.test.ts — All fact types, valid YAML\n  │   └── slug-generator.test.ts — Filename sanitization\n  ├── integration/ (7 files)\n  │   ├── write-through-e2e.test.ts — Store fact → file appears \u003c5s\n  │   ├── reconcile-e2e.test.ts — Edit file → DB updates\n  │   ├── conflict-e2e.test.ts — Concurrent edits → conflict file\n  │   ├── auto-linking-e2e.test.ts — Entity mentions → wiki-links\n  │   ├── index-first-e2e.test.ts — Query → index scan → results\n  │   ├── observation-compression-e2e.test.ts — Classify → compress\n  │   └── budget-aware-context-e2e.test.ts — Budget enforcement\n  ├── golden/ (4 files)\n  │   ├── format/decision.golden.md — Expected markdown for decision\n  │   ├── format/lesson.golden.md — Expected markdown for lesson\n  │   ├── format/entity.golden.md — Expected markdown for entity\n  │   └── retrieval/evaluate-relevance.test.ts — Query set, relevance@5\n  ├── benchmarks/ (1 file)\n  │   └── before-after.test.ts — Latency, relevance, token efficiency\n  └── regression/ (5 files)\n      ├── core-api.test.ts — All MCP tools still work\n      ├── enrichment.test.ts — Embeddings, entities, importance\n      ├── decay.test.ts — Decay cron still runs\n      ├── consolidation.test.ts — Theme merging still works\n      └── sync.test.ts — LanceDB sync still works\n\n**Golden query set (in retrieval/):**\n- 20+ queries with expected fact IDs\n- Min relevance thresholds per query\n- Cover all fact types and priority tiers\n\n**Performance benchmarks:**\n- Retrieval latency: before (450ms) vs after (\u003c200ms index, \u003c500ms semantic)\n- Relevance@5: before (0.72) vs after (\u003e0.85)\n- Token efficiency: before (2000) vs after (\u003c1400)\n- Sync reliability: 99.99% over 10k ops\n\n**CI Integration:**\n- Run on every PR, block merge if fails\n- Coverage target \u003e80%\n- Performance regression gate: \u003e10% slower = fail\n\nRef: VAULT_INTEGRATION_PLAN.md Phase 8 (sections 8.3-8.9)","status":"closed","priority":4,"issue_type":"task","created_at":"2026-02-14T22:57:11.869078+01:00","updated_at":"2026-02-14T23:25:57.557919+01:00","closed_at":"2026-02-14T23:25:57.557919+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-0ro","depends_on_id":"engram-dxj","type":"blocks","created_at":"2026-02-14T22:57:25.38331+01:00","created_by":"daemon"}]}
{"id":"engram-0v6","title":"Update build_system_prompt to use manifest + pinned facts","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-25T14:04:55.151308+01:00","updated_at":"2026-02-25T14:23:59.770804+01:00","closed_at":"2026-02-25T14:23:59.770804+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-0v6","depends_on_id":"engram-adj","type":"blocks","created_at":"2026-02-25T14:05:40.386261+01:00","created_by":"daemon"},{"issue_id":"engram-0v6","depends_on_id":"engram-li6","type":"blocks","created_at":"2026-02-25T14:05:40.56561+01:00","created_by":"daemon"}]}
{"id":"engram-0wc","title":"Observability and production operations readiness","description":"Implement remaining operations hardening tasks: health check endpoint/tooling, structured logging standardization, enrichment/vault-lag telemetry hooks, and optional metrics scaffolding. Document alert thresholds and incident triage flow consistent with deployment checklist.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-15T00:33:58.532486+01:00","updated_at":"2026-02-15T00:49:13.001202+01:00","closed_at":"2026-02-15T00:49:13.001202+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-0wc","depends_on_id":"engram-hx4","type":"blocks","created_at":"2026-02-15T00:34:11.579734+01:00","created_by":"daemon"}]}
{"id":"engram-140","title":"Token Budget on Recall Responses","description":"# Token Budget on Recall Responses\n\n## Problem\nCurrent recall has no token accounting:\n- Memory injection can silently blow up context windows\n- No way to limit recall results by token budget (only by count)\n- Cannot track cost savings (memory vs full history)\n- No visibility into actual token consumption\n\n## Solution (from GenAITech Analysis)\nAdd token estimation and budgeting to recall:\n1. Track `tokenEstimate` on every fact (~content.length / 4)\n2. Return total token estimate in recall responses\n3. Add `tokenBudget` parameter: \"give me best memories that fit in N tokens\"\n4. Log token savings metrics\n\n## Schema Changes\n```typescript\n// Add to facts table (already has optional fields for this):\n// tokenEstimate: v.optional(v.number()),  // estimated tokens in content\n\n// No new table needed! Use existing optional field.\n```\n\n## Implementation\n1. **Fact storage**: Compute tokenEstimate on insert/update\n2. **Recall**: Accept tokenBudget parameter, enforce limit during result assembly\n3. **Response**: Return token metrics\n\n```typescript\n// Response format:\n{\n  facts: [...],\n  recallId: \"uuid\",\n  tokenEstimate: 2340,      // total tokens of returned facts\n  tokenBudget: 4000,        // requested budget (if set)\n  tokensUsed: 2340,         // actual usage\n  tokensAvailable: 1660,    // budget - used\n  factsTruncated: false,    // whether budget caused truncation\n}\n```\n\n## Token Estimation Logic\n```typescript\nfunction estimateTokens(text: string): number {\n  // GPT-4 tokenizer approximation: ~4 chars per token\n  // More accurate: use tiktoken library\n  // For now: simple heuristic\n  return Math.ceil(text.length / 4);\n}\n```\n\n## Budget Enforcement\n```typescript\n// In recall.ts, after ranking:\nlet totalTokens = 0;\nconst budgetedFacts = [];\n\nfor (const fact of rankedFacts) {\n  const tokens = fact.tokenEstimate ?? estimateTokens(fact.content);\n  if (tokenBudget \u0026\u0026 totalTokens + tokens \u003e tokenBudget) {\n    // Budget exceeded, stop here\n    break;\n  }\n  totalTokens += tokens;\n  budgetedFacts.push(fact);\n}\n\nreturn {\n  facts: budgetedFacts,\n  tokenEstimate: totalTokens,\n  tokenBudget,\n  tokensUsed: totalTokens,\n  factsTruncated: budgetedFacts.length \u003c rankedFacts.length,\n};\n```\n\n## Metrics Logging\nTrack in `memory_events`:\n```typescript\nawait ctx.runMutation(internal.events.log, {\n  eventType: \"recall_completed\",\n  payload: {\n    recallId,\n    tokensUsed,\n    tokenBudget,\n    factCount: facts.length,\n    truncated: factsTruncated,\n  },\n});\n```\n\n## Impact\n- Prevents context bloat\n- Enables cost tracking per-agent per-scope\n- Allows budget-constrained recall (\"get me 2K tokens of context\")\n- Supports token savings metrics (memory vs full history)\n\n## References\n- GenAITech article on memory-as-metered-infrastructure\n- Current schema: tokenEstimate field already exists (optional)\n- Recall implementation: mcp-server/src/tools/recall.ts","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-24T09:33:12.859294+01:00","updated_at":"2026-02-25T01:42:07.957617+01:00","closed_at":"2026-02-25T01:42:07.957617+01:00","close_reason":"Closed"}
{"id":"engram-140.1","title":"Schema: Activate tokenEstimate field","description":"Schema already has `tokenEstimate: v.optional(v.number())` in facts table. No migration needed.\n\n**Task**: Document that this field will be populated going forward.\n\n**File**: Update `convex/schema.ts` comment:\n```typescript\ntokenEstimate: v.optional(v.number()), // Estimated tokens (~content.length / 4), computed on insert\n```\n\n**Verification**: Check schema.ts line ~50 for existing field.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-24T09:35:39.819445+01:00","updated_at":"2026-02-25T01:42:07.937258+01:00","closed_at":"2026-02-25T01:42:07.937258+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-140.1","depends_on_id":"engram-140","type":"parent-child","created_at":"2026-02-24T09:35:39.827618+01:00","created_by":"daemon"}]}
{"id":"engram-140.2","title":"Convex: Add token estimation to fact mutations","description":"Add token estimation to fact insertion/update in `convex/facts.ts`.\n\n**File**: `convex/facts.ts` (or wherever fact mutations are defined)\n\n**Function**: `estimateTokens(text: string): number`\n```typescript\nexport function estimateTokens(text: string): number {\n  // GPT-4 tokenizer approximation: ~4 chars per token\n  // TODO: Use tiktoken library for accuracy in v2\n  return Math.ceil(text.length / 4);\n}\n```\n\n**Integration**:\n1. **storeFact mutation**: Compute tokenEstimate before insert\n   ```typescript\n   const tokenEstimate = estimateTokens(args.content);\n   const factId = await ctx.db.insert(\"facts\", {\n     ...args,\n     tokenEstimate,\n   });\n   ```\n\n2. **updateFact mutation**: Recompute if content changes\n   ```typescript\n   if (args.content) {\n     await ctx.db.patch(factId, {\n       content: args.content,\n       tokenEstimate: estimateTokens(args.content),\n     });\n   }\n   ```\n\n**Backfill (optional)**: Create migration script to populate tokenEstimate for existing facts:\n```typescript\n// convex/migrations/backfill-tokens.ts\nexport default mutation(async (ctx) =\u003e {\n  const facts = await ctx.db.query(\"facts\").collect();\n  for (const fact of facts) {\n    if (fact.tokenEstimate === undefined) {\n      await ctx.db.patch(fact._id, {\n        tokenEstimate: estimateTokens(fact.content),\n      });\n    }\n  }\n});\n```","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-24T09:35:40.046008+01:00","updated_at":"2026-02-25T02:49:22.612447+01:00","closed_at":"2026-02-25T02:49:22.612447+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-140.2","depends_on_id":"engram-140","type":"parent-child","created_at":"2026-02-24T09:35:40.048095+01:00","created_by":"daemon"}]}
{"id":"engram-140.3","title":"MCP Tool: Add tokenBudget parameter to recall","description":"Add `tokenBudget` parameter to recall and enforce token limits.\n\n**File**: `mcp-server/src/tools/recall.ts`\n\n**Schema Change**:\n```typescript\nexport const recallSchema = z.object({\n  // ... existing fields\n  tokenBudget: z.number().optional().describe(\"Max tokens to return (soft limit)\"),\n});\n```\n\n**Budget Enforcement Logic**:\n```typescript\n// After ranking, before return:\nlet totalTokens = 0;\nconst budgetedFacts = [];\nconst tokenBudget = input.tokenBudget;\n\nfor (const fact of rankedFacts) {\n  const factTokens = fact.tokenEstimate ?? estimateTokens(fact.content);\n  \n  if (tokenBudget \u0026\u0026 totalTokens + factTokens \u003e tokenBudget) {\n    // Budget exceeded, stop adding facts\n    console.log(`[recall] Token budget exceeded: ${totalTokens} + ${factTokens} \u003e ${tokenBudget}`);\n    break;\n  }\n  \n  totalTokens += factTokens;\n  budgetedFacts.push(fact);\n}\n\nreturn {\n  facts: budgetedFacts,\n  recallId,\n  tokenEstimate: totalTokens,\n  tokenBudget: tokenBudget ?? null,\n  tokensUsed: totalTokens,\n  tokensAvailable: tokenBudget ? tokenBudget - totalTokens : null,\n  factsTruncated: budgetedFacts.length \u003c rankedFacts.length,\n};\n```\n\n**Import**: Add `estimateTokens` from Convex if not already available locally.\n\n**Testing**: Recall with tokenBudget=500 and verify total tokens \u003c= 500.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-24T09:35:40.25936+01:00","updated_at":"2026-02-25T02:49:22.987504+01:00","closed_at":"2026-02-25T02:49:22.987504+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-140.3","depends_on_id":"engram-140","type":"parent-child","created_at":"2026-02-24T09:35:40.261484+01:00","created_by":"daemon"}]}
{"id":"engram-140.4","title":"Convex: Token metrics logging","description":"Log token metrics to `memory_events` table for analytics.\n\n**File**: `mcp-server/src/tools/recall.ts`\n\n**Event Logging**:\n```typescript\n// After recall completion:\nawait ctx.runMutation(internal.events.log, {\n  eventType: \"recall_completed\",\n  agentId,\n  scopeId: resolvedScopeId,\n  payload: {\n    recallId,\n    query: input.query,\n    factCount: budgetedFacts.length,\n    tokensUsed: totalTokens,\n    tokenBudget: input.tokenBudget ?? null,\n    factsTruncated: budgetedFacts.length \u003c rankedFacts.length,\n    searchStrategy: input.searchStrategy,\n  },\n  watermark: Date.now(),\n  createdAt: Date.now(),\n});\n```\n\n**Analytics Query** (optional, create later):\n```typescript\n// convex/analytics/token-savings.ts\nexport const tokenSavings = query({\n  args: { agentId: v.string(), days: v.number() },\n  handler: async (ctx, args) =\u003e {\n    const since = Date.now() - args.days * 24 * 60 * 60 * 1000;\n    const events = await ctx.db\n      .query(\"memory_events\")\n      .withIndex(\"by_agent_watermark\", q =\u003e \n        q.eq(\"agentId\", args.agentId).gte(\"watermark\", since))\n      .filter(q =\u003e q.eq(q.field(\"eventType\"), \"recall_completed\"))\n      .collect();\n    \n    const totalRecalls = events.length;\n    const totalTokens = events.reduce((sum, e) =\u003e sum + (e.payload?.tokensUsed ?? 0), 0);\n    const avgTokensPerRecall = totalTokens / totalRecalls;\n    \n    return { totalRecalls, totalTokens, avgTokensPerRecall };\n  },\n});\n```\n\n**Testing**: Verify events logged with correct payload structure.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-24T09:35:40.470597+01:00","updated_at":"2026-02-25T02:49:23.352483+01:00","closed_at":"2026-02-25T02:49:23.352483+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-140.4","depends_on_id":"engram-140","type":"parent-child","created_at":"2026-02-24T09:35:40.472717+01:00","created_by":"daemon"}]}
{"id":"engram-140.5","title":"Unit Tests: Token estimation accuracy","description":"Unit tests for token estimation accuracy and budget enforcement.\n\n**File**: `mcp-server/test/unit/token-estimation.test.ts` (new file)\n\n**Test Cases**:\n1. **estimateTokens - Basic accuracy**:\n   - Test string: \"Hello world\" (11 chars) → ~3 tokens\n   - Test string: Long paragraph (1000 chars) → ~250 tokens\n   - Verify estimate within 20% of actual (use tiktoken as ground truth)\n\n2. **estimateTokens - Edge cases**:\n   - Empty string → 0 tokens\n   - Unicode/emoji → handle correctly\n   - Code blocks → higher token density\n\n3. **Budget enforcement - Exact limit**:\n   - 3 facts: 100, 200, 150 tokens\n   - Budget: 300 tokens\n   - Expected: First 2 facts (100 + 200), third truncated\n\n4. **Budget enforcement - No truncation**:\n   - 3 facts: 100, 100, 100 tokens\n   - Budget: 400 tokens\n   - Expected: All 3 facts returned\n\n5. **Budget enforcement - First fact exceeds**:\n   - 1 fact: 500 tokens\n   - Budget: 300 tokens\n   - Expected: Return that fact anyway (at least one result)\n\n**Framework**: Jest or similar\n\n**Run**: `npm test -- token-estimation`","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-24T09:35:40.680242+01:00","updated_at":"2026-02-25T03:09:23.94117+01:00","closed_at":"2026-02-25T03:09:23.94117+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-140.5","depends_on_id":"engram-140","type":"parent-child","created_at":"2026-02-24T09:35:40.681903+01:00","created_by":"daemon"}]}
{"id":"engram-140.6","title":"E2E Test: Budget-constrained recall","description":"E2E test for budget-constrained recall with real facts.\n\n**File**: `mcp-server/test/e2e/token-budget.test.ts` (new file)\n\n**Test Scenario**:\n1. **Setup**:\n   - Create test scope\n   - Store 10 facts of varying lengths:\n     - Fact 1: 50 chars (~13 tokens)\n     - Fact 2: 100 chars (~25 tokens)\n     - Fact 3: 200 chars (~50 tokens)\n     - ... up to Fact 10: 1000 chars (~250 tokens)\n   - Total: ~1000 tokens if all returned\n\n2. **Recall without budget**:\n   - Call recall with limit=10\n   - Verify all 10 facts returned\n   - Verify `tokenEstimate` ~1000\n\n3. **Recall with budget=500**:\n   - Call recall with tokenBudget=500, limit=10\n   - Verify returned facts sum to \u003c= 500 tokens\n   - Verify `factsTruncated: true`\n   - Verify `tokensAvailable` \u003e= 0\n\n4. **Recall with budget=50**:\n   - Call recall with tokenBudget=50\n   - Verify only 1-2 facts returned\n   - Verify total \u003c= 50 tokens\n\n5. **Metrics logging**:\n   - Query `memory_events` for `recall_completed` events\n   - Verify payload contains tokenBudget, tokensUsed, factsTruncated\n\n**Assertions**:\n- Budget never exceeded (except first fact edge case)\n- Token estimates accurate within 20%\n- Truncation flag correct\n\n**Run**: `npm run test:e2e -- token-budget`","status":"closed","priority":3,"issue_type":"task","created_at":"2026-02-24T09:35:40.89388+01:00","updated_at":"2026-02-25T03:19:17.498646+01:00","closed_at":"2026-02-25T03:19:17.498646+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-140.6","depends_on_id":"engram-140","type":"parent-child","created_at":"2026-02-24T09:35:40.895193+01:00","created_by":"daemon"}]}
{"id":"engram-14c","title":"MCP tool: pin_fact and unpin_fact","description":"Two new MCP tools to toggle the pinned status of a fact. Agent controls its own disclosure by pinning/unpinning facts. Register in tool-registry.ts.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-25T14:14:23.485853+01:00","updated_at":"2026-02-25T14:23:16.904121+01:00","closed_at":"2026-02-25T14:23:16.904121+01:00","close_reason":"Closed"}
{"id":"engram-17d","title":"Implement config resolver library","description":"Create convex/lib/config-resolver.ts with resolveConfig function. Priority: scope policy \u003e system config \u003e fallback. Add caching layer for \u003c1ms p99 lookups","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-15T00:38:23.031362+01:00","updated_at":"2026-02-15T00:47:30.955371+01:00","closed_at":"2026-02-15T00:47:30.955371+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-17d","depends_on_id":"engram-26p","type":"blocks","created_at":"2026-02-15T00:38:56.099535+01:00","created_by":"daemon"}]}
{"id":"engram-1le","title":"Phase 3: Real-Time \u0026 Identity","description":"Add memory_events table for streaming updates. Implement event polling tool with watermark-based pagination. Create agent identity context injection tool. Update enrichment pipeline to emit events.","status":"closed","priority":0,"issue_type":"epic","created_at":"2026-02-15T00:38:10.294183+01:00","updated_at":"2026-02-15T00:58:51.248092+01:00","closed_at":"2026-02-15T00:58:51.248092+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-1le","depends_on_id":"engram-qcu","type":"blocks","created_at":"2026-02-15T00:39:14.186016+01:00","created_by":"daemon"},{"issue_id":"engram-1le","depends_on_id":"engram-34c","type":"blocks","created_at":"2026-02-15T00:39:15.102871+01:00","created_by":"daemon"},{"issue_id":"engram-1le","depends_on_id":"engram-25m","type":"blocks","created_at":"2026-02-15T00:39:15.516796+01:00","created_by":"daemon"},{"issue_id":"engram-1le","depends_on_id":"engram-tsn","type":"blocks","created_at":"2026-02-15T00:39:16.052089+01:00","created_by":"daemon"}]}
{"id":"engram-1ob","title":"Phase 6: Filesystem Mirror","description":"File-based memory access alongside API. Sync Engram to local ~/.engram/memory/ as markdown with YAML frontmatter. Progressive disclosure via directory structure (system/, preferences/, projects/, corrections/, archive/). Two-way sync. Ref: memory/2026-02-25-letta-context-repos.md Phase 6","status":"open","priority":3,"issue_type":"feature","created_at":"2026-02-25T14:14:02.900939+01:00","updated_at":"2026-02-25T14:14:02.900939+01:00"}
{"id":"engram-1pn","title":"Wrap mutation functions to create version snapshots","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-25T14:04:57.244422+01:00","updated_at":"2026-02-25T14:24:11.606292+01:00","closed_at":"2026-02-25T14:24:11.606292+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-1pn","depends_on_id":"engram-rqm","type":"blocks","created_at":"2026-02-25T14:05:42.310011+01:00","created_by":"daemon"}]}
{"id":"engram-1qx","title":"Auto-summarize facts on creation","description":"In the enrichment pipeline (convex/actions/enrich.ts), generate a 1-line summary for each new fact using LLM inference. Store in the summary field. Used for progressive disclosure manifests.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T14:14:23.716318+01:00","updated_at":"2026-02-25T14:14:23.716318+01:00"}
{"id":"engram-1vt","title":"Git integration: auto-init + auto-commit on sync","status":"open","priority":3,"issue_type":"task","created_at":"2026-02-25T14:05:01.234432+01:00","updated_at":"2026-02-25T14:05:01.234432+01:00","dependencies":[{"issue_id":"engram-1vt","depends_on_id":"engram-g3o","type":"blocks","created_at":"2026-02-25T14:05:45.346631+01:00","created_by":"daemon"}]}
{"id":"engram-1vx","title":"Add backwards compatibility wrappers","description":"Keep old tool names as thin wrappers over new primitives. Add deprecation warnings. Update mcp-server/src/tools/recall.ts and get-context.ts to compose primitives","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-15T00:38:36.682242+01:00","updated_at":"2026-02-15T00:58:50.439207+01:00","closed_at":"2026-02-15T00:58:50.439207+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-1vx","depends_on_id":"engram-dt4","type":"blocks","created_at":"2026-02-15T00:39:06.179401+01:00","created_by":"daemon"},{"issue_id":"engram-1vx","depends_on_id":"engram-95y","type":"blocks","created_at":"2026-02-15T00:39:06.581751+01:00","created_by":"daemon"},{"issue_id":"engram-1vx","depends_on_id":"engram-vgb","type":"blocks","created_at":"2026-02-15T00:39:07.008382+01:00","created_by":"daemon"}]}
{"id":"engram-1wl","title":"Complete context injection and agent identity tools","description":"Extend memory_get_context with explicit agentContext payload including agent metadata, telos, capabilities, permitted scopes, and current policy hints. Add memory_get_agent_info tool and optional memory_get_system_prompt tool for discoverability. Ensure tool descriptions are user-centric and include usage examples for composition patterns.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-15T00:33:54.34893+01:00","updated_at":"2026-02-15T00:47:31.137386+01:00","closed_at":"2026-02-15T00:47:31.137386+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-1wl","depends_on_id":"engram-wii","type":"blocks","created_at":"2026-02-15T00:34:07.061292+01:00","created_by":"daemon"}]}
{"id":"engram-20z","title":"MCP Tool: memory_defrag (reorganize + split/merge facts)","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-25T14:04:56.471722+01:00","updated_at":"2026-02-25T14:29:36.961989+01:00","closed_at":"2026-02-25T14:29:36.961989+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-20z","depends_on_id":"engram-2s3","type":"blocks","created_at":"2026-02-25T14:05:41.975259+01:00","created_by":"daemon"}]}
{"id":"engram-25m","title":"Create agent identity context tool","description":"Build memory_get_agent_context tool returning agent metadata (name, telos, capabilities, settings), permitted scopes with policies, and system health (sync status, queue depth)","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-15T00:38:42.527926+01:00","updated_at":"2026-02-15T00:58:49.623018+01:00","closed_at":"2026-02-15T00:58:49.623018+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-25m","depends_on_id":"engram-c48","type":"blocks","created_at":"2026-02-15T00:39:12.808445+01:00","created_by":"daemon"}]}
{"id":"engram-26p","title":"Create config schema tables","description":"Add system_config table (key, value, category, description, version, updatedAt, updatedBy) and memory_policies table (scopeId, policyKey, policyValue, priority) to convex/schema.ts with proper indices","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-15T00:38:21.411995+01:00","updated_at":"2026-02-15T00:45:34.3061+01:00","closed_at":"2026-02-15T00:45:34.3061+01:00","close_reason":"Closed"}
{"id":"engram-28g","title":"Integrate manifest into build_system_prompt","description":"Modify mcp-server/src/tools/system-prompt-builder.ts to use progressive disclosure: inject pinned facts fully, show category summaries for non-pinned, let agent recall on-demand.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-25T14:14:23.276836+01:00","updated_at":"2026-02-25T14:24:12.106305+01:00","closed_at":"2026-02-25T14:24:12.106305+01:00","close_reason":"Covered by engram-0v6 (manifest in build_system_prompt)"}
{"id":"engram-2dp","title":"Phase 2: Sleep-Time Reflection Agent","description":"Background memory consolidation without blocking main agent. Create reflection cron job (4-6h), extract facts/preferences/corrections from recent history, consolidation pass to merge near-duplicates, weekly defrag. Ref: memory/2026-02-25-letta-context-repos.md Phase 2","status":"closed","priority":0,"issue_type":"feature","created_at":"2026-02-25T14:13:52.199086+01:00","updated_at":"2026-02-25T14:17:26.809189+01:00","closed_at":"2026-02-25T14:17:26.809189+01:00","close_reason":"Duplicate of P1 structured set (engram-adw, engram-sws, engram-2kn, engram-adj, engram-aeb, engram-2s3) which has proper dependency chains"}
{"id":"engram-2kn","title":"Schema: Add pinned + summary fields to facts table","description":"## Background\nProgressive disclosure requires two new fields on the facts table:\n- `pinned: v.optional(v.boolean())` — marks facts always loaded into agent system prompt\n- `summary: v.optional(v.string())` — 1-line description for manifest disclosure\n\n## Technical Approach\nEdit `convex/schema.ts`, add fields to the `facts` table definition.\nAdd index: `.index('by_pinned', ['pinned', 'scopeId'])`\nBoth fields are optional so no data migration needed.\n\n## Files to Edit\n- `convex/schema.ts` — add fields + index\n\n## Success Criteria\n- `npx convex dev` deploys without errors\n- New fields visible in Convex dashboard\n- Existing facts unaffected (optional fields)\n\n## Context\nInspired by Letta Context Repos `system/` directory pattern.\nPinned facts = always in context. Summary = frontmatter for progressive disclosure.\nSee PLAN-CONTEXT-REPOS.md Phase 1.1","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-25T14:04:54.588698+01:00","updated_at":"2026-02-25T14:21:15.878391+01:00","closed_at":"2026-02-25T14:21:15.878391+01:00","close_reason":"Closed"}
{"id":"engram-2pt","title":"CLI command: engram bootstrap --source openclaw|claude-code","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T14:05:00.287918+01:00","updated_at":"2026-02-25T14:05:00.287918+01:00","dependencies":[{"issue_id":"engram-2pt","depends_on_id":"engram-9jp","type":"blocks","created_at":"2026-02-25T14:05:44.651475+01:00","created_by":"daemon"}]}
{"id":"engram-2s3","title":"Enhance memory_reflect tool with depth + timeWindow params","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-25T14:04:56.09675+01:00","updated_at":"2026-02-25T14:24:45.13063+01:00","closed_at":"2026-02-25T14:24:45.13063+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-2s3","depends_on_id":"engram-aeb","type":"blocks","created_at":"2026-02-25T14:05:41.418301+01:00","created_by":"daemon"}]}
{"id":"engram-2ui","title":"Obsidian-Compatible Vault Mirror","description":"Add bidirectional markdown mirror layer: Convex remains system of record, markdown vault provides human-inspectable Obsidian-compatible files.\n\n**Implementation order (dependency chain):**\nP1 Core: engram-ywx (schema) → engram-waf (format) → engram-7yr (sync engine) → engram-dxj (MCP tool)\nP1 Parallel: engram-ri0 (Convex actions, depends on ywx)\nP2 Enhancements: engram-doo (observation pipeline), engram-43e (graph+autolinker), engram-gxr (index pipeline), engram-vj6 (query tools), engram-ykh (backlinks), engram-4ul (checkpoint/wake), engram-sll (budgeted recall)\nP3 Advanced: engram-uah (file watcher), engram-8nz (auditability)\nP4 Quality: engram-0ro (validation suite)\n\n**Key corrections applied (2026-02-14):**\n- Flattened observation.* fields to observationTier/observationCompressed/observationOriginalContent (Convex compatibility)\n- Fixed index references: timestamp (not createdAt), lifecycleState (not status)\n- Added missing spec fields: confidence, importanceTier\n- Clarified ri0 vs 8nz layer boundaries (Convex actions vs MCP lib)\n\nSee specs/obsidian-mirror-plan.md and specs/clawvault-learnings.md for full details.","status":"closed","priority":0,"issue_type":"epic","created_at":"2026-02-14T22:55:59.277332+01:00","updated_at":"2026-02-14T23:26:02.615802+01:00","closed_at":"2026-02-14T23:26:02.615802+01:00","close_reason":"Closed"}
{"id":"engram-2vw","title":"Embedding Subspace Consolidation via SVD","description":"# Embedding Subspace Consolidation via SVD\n\n## Problem\nCurrent storage: 10K facts × 1024 floats × 8 bytes = **80MB of embeddings**. Every fact stores full 1024-dimensional Cohere embedding. This is expensive and redundant — most facts share common knowledge dimensions.\n\n## Solution (from SHARE Paper)\n**Shared foundational subspace**: All facts in a scope converge to a shared low-rank subspace. Store:\n- **k principal directions** (basis vectors, k \u003c\u003c 1024)\n- **Lightweight coefficients** per fact (k-dimensional, not 1024)\n- **Reconstruction**: embedding ≈ V_k @ coefficients\n\n## Math Background\n```\nGiven N facts with embeddings E (N × 1024):\n1. Run SVD: E = U Σ V^T\n2. Keep top-k singular vectors: V_k (k × 1024)\n3. Project facts onto subspace: coefficients = E @ V_k^T (N × k)\n4. Store: V_k (shared) + coefficients (per-fact)\n\nCompression: (k × 1024) + (N × k) \u003c\u003c N × 1024\nExample: k=32, N=10000 → 33KB + 2.5MB = 2.53MB (vs 80MB) = **32x reduction**\n```\n\n## Schema Addition\n```typescript\n// New table: knowledge_subspaces\nknowledge_subspaces: defineTable({\n  scopeId: v.id(\"memory_scopes\"),\n  principalVectors: v.array(v.array(v.float64())),  // V_k: k × 1024 matrix\n  singularValues: v.array(v.float64()),             // Σ: k values (for variance)\n  explainedVariance: v.array(v.float64()),          // per-axis variance\n  k: v.number(),                                     // current rank (dimensionality)\n  factCount: v.number(),                             // # facts consolidated\n  totalVariance: v.float64(),                        // sum of all variance\n  updatedAt: v.number(),\n  version: v.number(),                               // increments on re-merge\n}).index(\"by_scope\", [\"scopeId\"])\n\n// Modify facts table (use existing optional field):\n// compactEmbedding: v.optional(v.array(v.float64()))  // k-dimensional coefficients\n```\n\n## Implementation Phases\n\n### Phase 1: Initial Consolidation\n```typescript\n// convex/subspace/consolidate.ts\nexport const consolidateScope = mutation({\n  args: { scopeId: v.id(\"memory_scopes\"), k: v.number() },\n  handler: async (ctx, args) =\u003e {\n    // 1. Gather all fact embeddings in scope\n    const facts = await ctx.db\n      .query(\"facts\")\n      .withIndex(\"by_scope\", q =\u003e q.eq(\"scopeId\", args.scopeId))\n      .filter(q =\u003e q.neq(q.field(\"embedding\"), undefined))\n      .collect();\n    \n    const embeddings = facts.map(f =\u003e f.embedding); // N × 1024\n    \n    // 2. Run SVD (use external lib: svd-js, ml-matrix, or Python subprocess)\n    const { U, S, V } = await computeSVD(embeddings);\n    \n    // 3. Keep top-k\n    const Vk = V.slice(0, args.k); // k × 1024\n    const Sk = S.slice(0, args.k);\n    \n    // 4. Compute coefficients for each fact\n    for (let i = 0; i \u003c facts.length; i++) {\n      const coefficients = matmul(embeddings[i], transpose(Vk)); // 1 × k\n      await ctx.db.patch(facts[i]._id, {\n        compactEmbedding: coefficients,\n      });\n    }\n    \n    // 5. Store subspace\n    const subspaceId = await ctx.db.insert(\"knowledge_subspaces\", {\n      scopeId: args.scopeId,\n      principalVectors: Vk,\n      singularValues: Sk,\n      explainedVariance: Sk.map(s =\u003e s*s / sum(S.map(x =\u003e x*x))),\n      k: args.k,\n      factCount: facts.length,\n      totalVariance: sum(Sk.map(s =\u003e s*s)),\n      updatedAt: Date.now(),\n      version: 1,\n    });\n    \n    return { subspaceId, compression: (1 - (args.k / 1024)).toFixed(2) };\n  },\n});\n```\n\n### Phase 2: Compact Vector Search\n```typescript\n// In recall.ts vector search:\n// Option A: Reconstruct embeddings on-the-fly\nconst reconstructed = matmul(fact.compactEmbedding, subspace.principalVectors);\nconst similarity = cosineSimilarity(queryEmbedding, reconstructed);\n\n// Option B: Store query in compact form\nconst queryCompact = matmul(queryEmbedding, transpose(subspace.principalVectors));\nconst similarity = cosineSimilarity(queryCompact, fact.compactEmbedding); // k-dimensional\n```\n\n## Explained Variance Threshold\n```typescript\n// From paper: 60% explained variance is sufficient\n// Monitor per-scope:\nconst variance = subspace.explainedVariance.reduce((a, b) =\u003e a + b, 0);\n\nif (variance \u003c 0.6) {\n  console.warn(`Subspace under-fitted: ${variance.toFixed(2)} \u003c 0.6. Increase k.`);\n} else if (variance \u003e 0.9) {\n  console.warn(`Subspace over-fitted: ${variance.toFixed(2)} \u003e 0.9. Reduce k.`);\n}\n\n// Sweet spot: 0.7-0.8\n```\n\n## Impact\n- **32x embedding storage reduction** (k=32 vs 1024)\n- **Faster vector search** (k-dimensional vs 1024)\n- **Principled memory consolidation** (SVD vs heuristics)\n- **Adaptive capacity** (k grows with knowledge diversity)\n\n## SVD Implementation\nUse existing JS library or call Python:\n- **ml-matrix** (npm): SVD implementation in JS\n- **svd-js** (npm): Lightweight SVD\n- **Python subprocess**: `numpy.linalg.svd` (most accurate)\n\n## References\n- SHARE paper: https://arxiv.org/abs/2602.06043\n- Current schema: compactEmbedding field exists (optional)\n- Analysis doc: docs/SHARE-PAPER-ANALYSIS.md","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-24T09:33:14.240074+01:00","updated_at":"2026-02-25T01:42:07.982889+01:00","closed_at":"2026-02-25T01:42:07.982889+01:00","close_reason":"Closed"}
{"id":"engram-2vw.1","title":"Schema: Add knowledge_subspaces table","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-24T09:41:26.434985+01:00","updated_at":"2026-02-25T01:42:07.947582+01:00","closed_at":"2026-02-25T01:42:07.947582+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-2vw.1","depends_on_id":"engram-2vw","type":"parent-child","created_at":"2026-02-24T09:41:26.436105+01:00","created_by":"daemon"}]}
{"id":"engram-2vw.2","title":"Implement SVD consolidation logic","description":"Implement SVD consolidation to compress fact embeddings.\n\n**File**: `convex/subspace/consolidate.ts` (new file)\n\n**SVD Implementation**:\n```typescript\n// Use ml-matrix or svd-js library\nimport { Matrix } from \"ml-matrix\";\n\nexport async function runSVD(embeddings: number[][], k: number) {\n  const matrix = new Matrix(embeddings);\n  const svd = new SVD(matrix);\n  const V = svd.rightSingularVectors;\n  const S = svd.diagonalValue;\n  return { V: V.slice(0, k), S: S.slice(0, k) };\n}\n\nexport const consolidateScope = mutation({\n  args: { scopeId: v.id(\"memory_scopes\"), k: v.number() },\n  handler: async (ctx, args) =\u003e {\n    const facts = await ctx.db\n      .query(\"facts\")\n      .withIndex(\"by_scope\", q =\u003e q.eq(\"scopeId\", args.scopeId))\n      .filter(q =\u003e q.neq(q.field(\"embedding\"), undefined))\n      .collect();\n    \n    const embeddings = facts.map(f =\u003e f.embedding!);\n    const { V: Vk, S: Sk } = await runSVD(embeddings, args.k);\n    \n    // Store subspace\n    const subspaceId = await ctx.db.insert(\"knowledge_subspaces\", {\n      scopeId: args.scopeId,\n      principalVectors: Vk.toArray(),\n      singularValues: Sk,\n      explainedVariance: Sk.map(s =\u003e (s*s) / Sk.reduce((a, b) =\u003e a + b*b, 0)),\n      k: args.k,\n      factCount: facts.length,\n      totalVariance: Sk.reduce((a, b) =\u003e a + b*b, 0),\n      updatedAt: Date.now(),\n      version: 1,\n    });\n    \n    return { subspaceId, compression: Math.round((1 - args.k/1024) * 100) };\n  },\n});\n```\n\n**Dependencies**: Uses existing embedding field from facts table.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-24T09:41:26.648368+01:00","updated_at":"2026-02-25T11:41:20.089826+01:00","closed_at":"2026-02-25T11:41:20.089826+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-2vw.2","depends_on_id":"engram-2vw","type":"parent-child","created_at":"2026-02-24T09:41:26.64912+01:00","created_by":"daemon"}]}
{"id":"engram-2vw.3","title":"Update facts with compact embeddings","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-24T09:41:26.894529+01:00","updated_at":"2026-02-25T11:41:20.334991+01:00","closed_at":"2026-02-25T11:41:20.334991+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-2vw.3","depends_on_id":"engram-2vw","type":"parent-child","created_at":"2026-02-24T09:41:26.896007+01:00","created_by":"daemon"}]}
{"id":"engram-2vw.4","title":"Compact vector search implementation","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-24T09:41:27.185418+01:00","updated_at":"2026-02-25T11:41:20.593571+01:00","closed_at":"2026-02-25T11:41:20.593571+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-2vw.4","depends_on_id":"engram-2vw","type":"parent-child","created_at":"2026-02-24T09:41:27.186474+01:00","created_by":"daemon"}]}
{"id":"engram-2vw.5","title":"Monitor explained variance","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-24T09:41:27.400648+01:00","updated_at":"2026-02-25T03:12:03.127951+01:00","closed_at":"2026-02-25T03:12:03.127951+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-2vw.5","depends_on_id":"engram-2vw","type":"parent-child","created_at":"2026-02-24T09:41:27.402041+01:00","created_by":"daemon"}]}
{"id":"engram-2vw.6","title":"Unit tests: SVD consolidation","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-24T09:41:27.642946+01:00","updated_at":"2026-02-25T03:14:29.688551+01:00","closed_at":"2026-02-25T03:14:29.688551+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-2vw.6","depends_on_id":"engram-2vw","type":"parent-child","created_at":"2026-02-24T09:41:27.644635+01:00","created_by":"daemon"}]}
{"id":"engram-2vw.7","title":"E2E test: Subspace-based recall","status":"closed","priority":3,"issue_type":"task","created_at":"2026-02-24T09:41:27.864145+01:00","updated_at":"2026-02-25T03:32:04.921952+01:00","closed_at":"2026-02-25T03:32:04.921952+01:00","close_reason":"29-test E2E suite for subspace-recall created; all 721 tests passing after fixing stale JS files, mock gaps, and test data","dependencies":[{"issue_id":"engram-2vw.7","depends_on_id":"engram-2vw","type":"parent-child","created_at":"2026-02-24T09:41:27.865854+01:00","created_by":"daemon"}]}
{"id":"engram-34c","title":"Implement event polling tool","description":"Create memory_poll_events MCP tool with watermark-based pagination. Add Convex query for pollByAgent with efficient indexing","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-15T00:38:39.654975+01:00","updated_at":"2026-02-15T00:58:49.27364+01:00","closed_at":"2026-02-15T00:58:49.27364+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-34c","depends_on_id":"engram-qcu","type":"blocks","created_at":"2026-02-15T00:39:12.095126+01:00","created_by":"daemon"}]}
{"id":"engram-3p7","title":"Implement get_memory_manifest MCP tool","description":"New MCP tool that returns: (1) all pinned facts with full content, (2) category summaries (fact types + counts + 1-line descriptions), (3) on-demand loading hint. Register in tool-registry.ts. Handler in mcp-server/src/tools/.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-25T14:14:22.823543+01:00","updated_at":"2026-02-25T14:17:26.815688+01:00","closed_at":"2026-02-25T14:17:26.815688+01:00","close_reason":"Duplicate of P1 structured set (engram-adw, engram-sws, engram-2kn, engram-adj, engram-aeb, engram-2s3) which has proper dependency chains"}
{"id":"engram-3sg","title":"Security hardening implementation pass","description":"Implement remaining security tasks from plan: strong input length/format validation, structured error boundaries, optional API key guard, ENGRAM_AGENT_ID verification against registered agent, admin operation audit logging, and per-agent rate limiting. Ensure no stack traces leak to MCP clients and add boundary/injection tests.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-15T00:33:56.744334+01:00","updated_at":"2026-02-15T00:50:22.628385+01:00","closed_at":"2026-02-15T00:50:22.628385+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-3sg","depends_on_id":"engram-wii","type":"blocks","created_at":"2026-02-15T00:34:08.68395+01:00","created_by":"daemon"}]}
{"id":"engram-3vy","title":"Tests: Reflection dedup, consolidation, defrag e2e","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-25T14:04:56.658+01:00","updated_at":"2026-02-25T14:30:26.942314+01:00","closed_at":"2026-02-25T14:30:26.942314+01:00","close_reason":"Completed: Created consolidation-defrag.test.ts with 39 comprehensive tests covering word set tokenization, Jaccard similarity, merge candidate detection, content generation, consolidation plans, reflection dedup, and defrag behavior. All tests passing.","dependencies":[{"issue_id":"engram-3vy","depends_on_id":"engram-20z","type":"blocks","created_at":"2026-02-25T14:05:42.142456+01:00","created_by":"daemon"}]}
{"id":"engram-43e","title":"Knowledge graph index + auto-linker for wiki-links","description":"Knowledge graph index + wiki-link auto-linking:\n\n**New files:**\n1. mcp-server/src/lib/wiki-link-parser.ts:\n   - extractWikiLinks(content) → WikiLink[] — Parse [[Name]] links, return {name, startIndex, endIndex}\n   - hasWikiLink(content, entityName) → boolean — Check if entity already linked\n   - Edge cases: nested brackets (parse outer), escaped \\[[...]] (ignore), whitespace [[  Name  ]] (trim)\n\n2. mcp-server/src/lib/auto-linker.ts:\n   - autoLinkEntities(content, scopeId, convex) → string — Auto-wrap entity mentions in [[...]]\n   - Algorithm: (1) Get all entities in scope, (2) Extract existing links, (3) Sort entities by name length (longest first), (4) Replace first mention of each entity with [[Name]], (5) Skip if already linked\n   - Case-insensitive matching, word boundaries only (\\b...\\b)\n\n3. mcp-server/src/lib/graph-exporter.ts:\n   - exportGraph(convex, outputPath) → void — Generate Obsidian-compatible graph JSON\n   - Format: {nodes: [{id, label, type, group}], links: [{source, target, type}]}\n   - Include fact→entity edges (type: 'mentions') and entity→entity edges (type: rel.relationshipType)\n\n**Convex schema changes (entities table):**\n- Add backlinks array: v.array(v.object({factId: v.id('facts'), factType: v.string(), linkedAt: v.number()}))\n\n**Integration in store-fact.ts:**\n- Before storing: linkedContent = await autoLinkEntities(content, scopeId, convex)\n- After storing: Update entity backlinks for each [[Name]] found\n\n**New MCP tool (in engram-vj6):**\n- memory_export_graph — Export graph to vault/.obsidian/graph.json\n\n**Tests:** auto-linking-e2e.test.ts (store fact mentioning 'Convex' → content has [[Convex]]), graph-export-e2e.test.ts (valid Obsidian JSON)\n**Performance:** Auto-link \u003c50ms, graph export \u003c2s for 10k facts\nRef: VAULT_INTEGRATION_PLAN.md Phase 4 (sections 4.2-4.6)","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-14T22:56:39.209329+01:00","updated_at":"2026-02-14T23:24:51.499865+01:00","closed_at":"2026-02-14T23:24:51.499865+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-43e","depends_on_id":"engram-7yr","type":"blocks","created_at":"2026-02-14T22:57:20.586856+01:00","created_by":"daemon"}]}
{"id":"engram-46g","title":"Install vault integration npm dependencies","description":"Install required npm packages for vault integration:\n\n**MCP server dependencies:**\n```bash\ncd mcp-server\nbun add chokidar js-yaml marked slugify\nbun add -d @types/js-yaml @types/marked\n```\n\n**Packages:**\n- chokidar ^4.0.1 — File watching for vault-sync daemon\n- js-yaml ^4.1.0 — YAML frontmatter parsing\n- marked ^15.0.4 — Markdown parsing (if needed)\n- slugify ^1.6.6 — Filename slug generation\n- @types/js-yaml ^4.0.9 — TypeScript types\n- @types/marked ^7.0.0 — TypeScript types\n\n**Verification:**\n- Run bun install to ensure lock file updated\n- Verify imports work: import yaml from 'js-yaml', import chokidar from 'chokidar'\n- No version conflicts with existing packages\n\n**This blocks:** engram-waf (needs slugify + js-yaml), engram-7yr (needs chokidar)\n\nRef: VAULT_INTEGRATION_PLAN.md Appendix B","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-14T23:07:22.072674+01:00","updated_at":"2026-02-14T23:24:06.553393+01:00","closed_at":"2026-02-14T23:24:06.553393+01:00","close_reason":"Closed"}
{"id":"engram-46p","title":"Refactor enrichment to use config resolver","description":"Update convex/actions/importance.ts, convex/crons/decay.ts, convex/crons/forget.ts, mcp-server/src/lib/ranking.ts to call resolveConfig instead of hardcoded constants","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-15T00:38:28.997364+01:00","updated_at":"2026-02-15T00:50:22.520377+01:00","closed_at":"2026-02-15T00:50:22.520377+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-46p","depends_on_id":"engram-jsf","type":"blocks","created_at":"2026-02-15T00:38:56.817255+01:00","created_by":"daemon"}]}
{"id":"engram-4k3","title":"Active Forgetting Pipeline (ALMA)","description":"# Active Forgetting Pipeline (ALMA)\n\n## Problem\n`forgetScore` field exists in schema but no active forgetting pipeline runs. Facts accumulate forever:\n- Old, unused facts clutter search results\n- No automatic archival of stale knowledge\n- No compression of redundant observations\n- Storage costs grow unbounded\n\n## Solution (from Letta/ALMA)\nImplement **active forgetting pipeline** via cron job that:\n1. Identifies facts to forget (high forgetScore + low access + old)\n2. Archives superseded facts (newer fact contradicts old one)\n3. Compresses background observations into episode summaries\n4. Provides explicit `memory_forget` MCP tool for agent-driven forgetting\n\n## Forgetting Criteria\n```typescript\n// Auto-archive if ALL of:\n- forgetScore \u003e 0.8\n- accessedCount \u003c 2\n- age \u003e 30 days\n- NOT referenced in temporalLinks\n- NOT in active episode\n\n// Archive if superseded:\n- supersededBy field is set\n- Newer fact has higher confidence\n\n// Compress if background noise:\n- observationTier === \"background\"\n- Never accessed after compression window (7 days)\n- Can be merged into episode summary\n```\n\n## Cron Pipeline\n```typescript\n// convex/crons/forget-pipeline.ts\nexport default cron(\"forget-pipeline\", {\n  schedule: \"0 2 * * *\", // Run at 2 AM daily\n  handler: async (ctx) =\u003e {\n    const candidates = await ctx.db\n      .query(\"facts\")\n      .filter(q =\u003e \n        q.and(\n          q.gt(q.field(\"forgetScore\"), 0.8),\n          q.lt(q.field(\"accessedCount\"), 2),\n          q.lt(q.field(\"timestamp\"), Date.now() - 30 * 24 * 60 * 60 * 1000)\n        )\n      )\n      .collect();\n    \n    for (const fact of candidates) {\n      // Check temporal links\n      const hasLinks = await checkTemporalLinks(ctx, fact._id);\n      if (hasLinks) continue;\n      \n      // Archive (set lifecycleState = \"archived\")\n      await ctx.db.patch(fact._id, {\n        lifecycleState: \"archived\",\n        vaultPath: null, // Remove from vault mirror\n      });\n    }\n    \n    return { archived: candidates.length };\n  },\n});\n```\n\n## MCP Tool: memory_forget\n```typescript\n// mcp-server/src/tools/forget.ts\nexport const forgetSchema = z.object({\n  factId: z.string().optional(),\n  query: z.string().optional(), // Find facts to forget by query\n  reason: z.string().describe(\"Why forgetting this\"),\n});\n\nexport async function forget(input, agentId) {\n  // Soft delete: set lifecycleState = \"archived\"\n  // Hard delete: only via admin action\n  const factId = input.factId || (await findFactByQuery(input.query))?._id;\n  \n  await ctx.runMutation(internal.facts.archive, { factId, reason: input.reason });\n  \n  // Log event\n  await ctx.runMutation(internal.events.log, {\n    eventType: \"fact_forgotten\",\n    factId,\n    agentId,\n    payload: { reason: input.reason },\n  });\n}\n```\n\n## Reconstruction Error (from SHARE paper)\nFor facts in subspace (after optimization #8):\n- Forgetting threshold based on reconstruction error\n- If fact can be perfectly reconstructed from subspace → safe to forget\n- If error \u003e threshold → keep original\n\n```typescript\nconst reconstructionError = ||original_embedding - reconstructed_embedding||;\nif (reconstructionError \u003c 0.1 \u0026\u0026 forgetScore \u003e 0.8) {\n  // Archive original, rely on subspace representation\n}\n```\n\n## Impact\n- Reduces fact count by 30-50% over time\n- Keeps search results relevant (no stale facts)\n- Enables storage quotas per-scope\n- Preserves important knowledge (linked facts protected)\n\n## Dependencies\n- Subspace consolidation (engram-2vw) for reconstruction-based forgetting\n\n## References\n- ALMA paper (active forgetting)\n- Letta memory editing\n- Current forgetScore field: convex/schema.ts (line ~80)\n- Optimization doc: docs/OPTIMIZATION-2026-02-24.md (section 5)","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-24T09:33:13.55217+01:00","updated_at":"2026-02-25T11:41:21.207946+01:00","closed_at":"2026-02-25T11:41:21.207946+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-4k3","depends_on_id":"engram-2vw","type":"blocks","created_at":"2026-02-24T09:38:29.902168+01:00","created_by":"daemon"}]}
{"id":"engram-4k3.1","title":"Implement forget criteria logic","description":"Implement forget criteria logic to identify candidates for archival.\n\n**File**: `convex/forget.ts` (new file)\n\n**Criteria Function**:\n```typescript\nexport async function identifyForgetCandidates(ctx: any) {\n  const thirtyDaysAgo = Date.now() - 30 * 24 * 60 * 60 * 1000;\n  \n  // Facts that meet ALL forgetting criteria\n  const candidates = await ctx.db\n    .query(\"facts\")\n    .filter(q =\u003e\n      q.and(\n        q.gt(q.field(\"forgetScore\"), 0.8),     // High forget score\n        q.lt(q.field(\"accessedCount\"), 2),     // Rarely accessed\n        q.lt(q.field(\"timestamp\"), thirtyDaysAgo), // Old\n        q.eq(q.field(\"lifecycleState\"), \"active\") // Still active\n      )\n    )\n    .collect();\n  \n  // Filter out facts with temporal links (important)\n  const results = [];\n  for (const fact of candidates) {\n    const hasLinks = (fact.temporalLinks?.length ?? 0) \u003e 0;\n    if (!hasLinks) {\n      results.push(fact);\n    }\n  }\n  \n  return results;\n}\n\n// Superseded facts (newer version exists)\nexport async function identifySuperseded(ctx: any) {\n  return await ctx.db\n    .query(\"facts\")\n    .filter(q =\u003e q.neq(q.field(\"supersededBy\"), undefined))\n    .filter(q =\u003e q.eq(q.field(\"lifecycleState\"), \"active\"))\n    .collect();\n}\n```\n\n**Testing**: Unit tests verify criteria match expected candidates.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-24T09:41:23.085006+01:00","updated_at":"2026-02-25T02:56:04.163213+01:00","closed_at":"2026-02-25T02:56:04.163213+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-4k3.1","depends_on_id":"engram-4k3","type":"parent-child","created_at":"2026-02-24T09:41:23.089987+01:00","created_by":"daemon"}]}
{"id":"engram-4k3.2","title":"Create forget-pipeline cron job","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-24T09:41:23.314276+01:00","updated_at":"2026-02-25T02:56:04.567717+01:00","closed_at":"2026-02-25T02:56:04.567717+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-4k3.2","depends_on_id":"engram-4k3","type":"parent-child","created_at":"2026-02-24T09:41:23.315145+01:00","created_by":"daemon"}]}
{"id":"engram-4k3.3","title":"MCP Tool: memory_forget","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-24T09:41:23.505115+01:00","updated_at":"2026-02-25T02:56:04.966199+01:00","closed_at":"2026-02-25T02:56:04.966199+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-4k3.3","depends_on_id":"engram-4k3","type":"parent-child","created_at":"2026-02-24T09:41:23.506223+01:00","created_by":"daemon"}]}
{"id":"engram-4k3.4","title":"Unit tests: Forget criteria","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-24T09:41:23.695172+01:00","updated_at":"2026-02-25T03:11:43.932203+01:00","closed_at":"2026-02-25T03:11:43.932212+01:00","dependencies":[{"issue_id":"engram-4k3.4","depends_on_id":"engram-4k3","type":"parent-child","created_at":"2026-02-24T09:41:23.696413+01:00","created_by":"daemon"}]}
{"id":"engram-4k3.5","title":"E2E test: Active forgetting pipeline","status":"closed","priority":3,"issue_type":"task","created_at":"2026-02-24T09:41:23.876881+01:00","updated_at":"2026-02-25T03:18:16.003496+01:00","closed_at":"2026-02-25T03:18:16.003503+01:00","dependencies":[{"issue_id":"engram-4k3.5","depends_on_id":"engram-4k3","type":"parent-child","created_at":"2026-02-24T09:41:23.878272+01:00","created_by":"daemon"}]}
{"id":"engram-4rr","title":"Finish primitive decomposition for recall/context workflows","description":"Add missing primitive tools required by plan decomposition: memory_vector_search, memory_text_search, memory_bump_access, memory_record_recall, memory_get_observations, memory_get_entities, memory_get_themes, memory_get_handoffs, memory_get_notifications, memory_mark_notifications_read. Keep memory_recall and memory_get_context as orchestration wrappers for backwards compatibility. Add contract tests to verify wrappers are equivalent to primitive composition.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-15T00:33:55.017797+01:00","updated_at":"2026-02-15T00:47:31.140075+01:00","closed_at":"2026-02-15T00:47:31.140075+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-4rr","depends_on_id":"engram-wii","type":"blocks","created_at":"2026-02-15T00:34:07.491423+01:00","created_by":"daemon"}]}
{"id":"engram-4ul","title":"Checkpoint/wake tools for context death resilience","description":"Add two new MCP tools for session continuity across context deaths:\n\n(1) memory_checkpoint: params workingOn (string), focus (optional), blocked (optional), urgent (bool). Writes to .engram/last-checkpoint.json + checkpoint history. Sets dirty-death flag. If urgent, triggers immediate wake.\n\n(2) memory_wake: detects dirty-death flag, loads checkpoint data, loads recent observations with temporal decay (today=all structural+potential, yesterday=structural+top5 potential, 2-3 days=structural only, 4-6 days=top 3 structural), builds session recap from handoffs/projects/commitments, returns bootstrap context.\n\n(3) memory_end_session should clear the dirty-death flag on clean exit.\n\nRequires observationTier field from engram-ywx (via engram-doo classification) to filter by importance tier during wake context loading.\n\nRef: specs/obsidian-mirror-plan.md §6.1-6.3, ClawVault checkpoint.ts + wake.ts pattern","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-14T22:56:57.086118+01:00","updated_at":"2026-02-14T23:25:41.793376+01:00","closed_at":"2026-02-14T23:25:41.793376+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-4ul","depends_on_id":"engram-doo","type":"blocks","created_at":"2026-02-14T22:57:23.00886+01:00","created_by":"daemon"}]}
{"id":"engram-5id","title":"Phase 4: QA-Pair Representation (Panini-inspired)","status":"closed","priority":1,"issue_type":"epic","created_at":"2026-02-25T14:04:58.182345+01:00","updated_at":"2026-02-25T14:32:56.452201+01:00","closed_at":"2026-02-25T14:32:56.452201+01:00","close_reason":"Phase 4 QA-Pair complete: schema fields, heuristic QA generation, QA-aware recall with RRF fusion, chain recall"}
{"id":"engram-5v2","title":"Deployment verification automation and evidence artifacts","description":"Translate DEPLOYMENT-VERIFICATION-CHECKLIST.md into executable or scriptable checks where possible: pre/post record-count verification, schema verification, tool count verification, and invariant validation helpers. Produce machine-runnable checks plus a concise evidence report format for go/no-go decisions.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-15T00:33:58.061194+01:00","updated_at":"2026-02-15T00:49:13.005283+01:00","closed_at":"2026-02-15T00:49:13.005283+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-5v2","depends_on_id":"engram-wii","type":"blocks","created_at":"2026-02-15T00:34:10.716927+01:00","created_by":"daemon"},{"issue_id":"engram-5v2","depends_on_id":"engram-hx4","type":"blocks","created_at":"2026-02-15T00:34:11.124424+01:00","created_by":"daemon"}]}
{"id":"engram-5y2","title":"Seed and manage runtime config values","description":"Create and validate seed path for extracting hardcoded values to database-backed config. Include seedSystemConfig plus resolver tests and admin-facing primitives for get/set/list config and set scope policy. Must include rollback-safe versioning/update metadata and acceptance checks that existing behavior remains unchanged when config rows are absent.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-15T00:33:53.315767+01:00","updated_at":"2026-02-15T00:47:31.137318+01:00","closed_at":"2026-02-15T00:47:31.137318+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-5y2","depends_on_id":"engram-wii","type":"blocks","created_at":"2026-02-15T00:34:06.003235+01:00","created_by":"daemon"}]}
{"id":"engram-62t","title":"BUG: hasRecentEnrichment scopeId filter always returns false","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-02-25T11:43:34.380063+01:00","updated_at":"2026-02-25T14:22:25.152526+01:00","closed_at":"2026-02-25T14:22:25.152526+01:00","close_reason":"Fixed: scopeId now set on enrichment events in logRetroactiveEnrichment and retroactiveReproject"}
{"id":"engram-6fx","title":"Staging rehearsal, rollback drill, and release readiness gate","description":"Create a release-readiness bead that captures staging deployment, rollback rehearsal, on-call checklist validation, and sign-off artifacts. Include explicit pass/fail criteria mapped to deployment checklist rows and final GO/NO-GO recording procedure.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-15T00:33:59.477669+01:00","updated_at":"2026-02-15T00:58:48.52275+01:00","closed_at":"2026-02-15T00:58:48.52275+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-6fx","depends_on_id":"engram-5v2","type":"blocks","created_at":"2026-02-15T00:34:13.102491+01:00","created_by":"daemon"},{"issue_id":"engram-6fx","depends_on_id":"engram-9q5","type":"blocks","created_at":"2026-02-15T00:34:13.490513+01:00","created_by":"daemon"}]}
{"id":"engram-6v6","title":"Add DELETE mutations for 7 entities","description":"Implement deleteEntity, deleteScope, deleteConversation, deleteSession, deleteTheme mutations in Convex. Soft delete via lifecycleState='archived' or hard delete with cascade options","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-15T00:38:27.085241+01:00","updated_at":"2026-02-15T00:45:34.332474+01:00","closed_at":"2026-02-15T00:45:34.332474+01:00","close_reason":"Closed"}
{"id":"engram-7eb","title":"Dashboard: Version timeline view component","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T14:04:57.810584+01:00","updated_at":"2026-02-25T14:04:57.810584+01:00","dependencies":[{"issue_id":"engram-7eb","depends_on_id":"engram-amx","type":"blocks","created_at":"2026-02-25T14:05:42.814784+01:00","created_by":"daemon"}]}
{"id":"engram-7g0","title":"Tests: Session parser, dedup, bootstrap e2e","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T14:05:00.476311+01:00","updated_at":"2026-02-25T14:05:00.476311+01:00","dependencies":[{"issue_id":"engram-7g0","depends_on_id":"engram-2pt","type":"blocks","created_at":"2026-02-25T14:05:44.831002+01:00","created_by":"daemon"}]}
{"id":"engram-7hb","title":"Schema: Add QA fields to facts table","description":"## Background\nPanini paper (arxiv 2602.15156) shows QA-pair representation dramatically improves\nretrieval accuracy (98.7% on multi-hop). Instead of raw text, facts also have a\nstructured question-answer representation optimized for retrieval.\n\n## Technical Approach\nAdd to facts table in `convex/schema.ts`:\n```typescript\nqaQuestion: v.optional(v.string()),\nqaAnswer: v.optional(v.string()),\nqaEntities: v.optional(v.array(v.string())),\nqaConfidence: v.optional(v.float64()),\n```\nAdd search index: `.searchIndex('search_qa', { searchField: 'qaQuestion' })`\n\n## Files to Edit\n- `convex/schema.ts` — add QA fields + search index\n\n## Success Criteria\n- Schema deploys cleanly\n- qaQuestion searchable via text search\n- See PLAN-CONTEXT-REPOS.md Phase 4.1","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-25T14:04:58.368617+01:00","updated_at":"2026-02-25T14:20:03.282899+01:00","closed_at":"2026-02-25T14:20:03.282899+01:00","close_reason":"Closed"}
{"id":"engram-7yr","title":"Create vault-sync.ts: Convex→MD export engine","description":"Create mcp-server/src/lib/vault-writer.ts + mcp-server/src/daemons/vault-sync.ts:\n\n**vault-writer.ts:**\n- writeFactToVault(fact, vaultRoot) — Generate frontmatter + body, compute folder/filename, ensure folder exists (mkdir -p), atomic write (tempfile + rename), return {success, path, error}\n- Error handling: disk full (retry w/ backoff), permission denied (log + mark unmirrored), invalid filename (sanitize + retry)\n- Performance: \u003c500ms p95 for write\n\n**vault-sync daemon (daemons/vault-sync.ts):**\n- Poll Convex every 5s for facts with vaultPath==null (getUnmirrored query, limit 100)\n- Write files using vault-writer.ts\n- Update fact.vaultPath + vaultSyncedAt in Convex after success\n- Watch vault/ using chokidar for file changes (ignoreInitial: true)\n- On file change: trigger reconcileFromVault action (to be implemented in engram-8nz)\n- Graceful shutdown on SIGTERM/SIGINT\n- Exponential backoff on Convex connection errors\n- Logging: JSON structured logs with event, factId, latency, timestamp\n\n**Lifecycle:**\n- Starts with MCP server (bun run mcp-server/src/index.ts)\n- Restarts automatically on crash\n\n**Dependencies:** chokidar, ConvexHttpClient\n**Tests:** write-through-e2e.test.ts (store fact → file appears \u003c5s), high-volume-writes.test.ts (10k facts in 60s)\n**Performance:** Mirror lag \u003c5s p95, sync reliability 99.99%\nRef: VAULT_INTEGRATION_PLAN.md Phase 3 (sections 3.1-3.3, 3.5-3.7)","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-14T22:56:18.15318+01:00","updated_at":"2026-02-14T23:24:13.000427+01:00","closed_at":"2026-02-14T23:24:13.000427+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-7yr","depends_on_id":"engram-waf","type":"blocks","created_at":"2026-02-14T22:57:18.458469+01:00","created_by":"daemon"},{"issue_id":"engram-7yr","depends_on_id":"engram-46g","type":"blocks","created_at":"2026-02-14T23:07:29.701337+01:00","created_by":"daemon"}]}
{"id":"engram-7zs","title":"Production docs overhaul for v2 architecture","description":"Create/update docs required by plan: USAGE-EXAMPLES.md (\u003e=10 scenarios), API-REFERENCE.md (all tools), INSTALLATION.md, CONFIGURATION.md, TROUBLESHOOTING.md, and README quick-start refresh. Ensure docs are operationally accurate against live code and include deployment verification references.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-15T00:33:57.627603+01:00","updated_at":"2026-02-15T00:49:13.002132+01:00","closed_at":"2026-02-15T00:49:13.002132+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-7zs","depends_on_id":"engram-1wl","type":"blocks","created_at":"2026-02-15T00:34:09.897525+01:00","created_by":"daemon"},{"issue_id":"engram-7zs","depends_on_id":"engram-4rr","type":"blocks","created_at":"2026-02-15T00:34:10.308605+01:00","created_by":"daemon"}]}
{"id":"engram-8nz","title":"Auditability: diffs, provenance metadata, edit reconciliation","description":"MCP-side auditability layer: reconciliation lib, diffs, provenance metadata. This is the MCP lib layer; the Convex-side reconciliation action is in engram-ri0.\n\n**New file: mcp-server/src/lib/vault-reconciler.ts**\n\n1. reconcileFileEdit(filePath, convex):\n   - Read file from disk, parse frontmatter + body\n   - Fetch DB fact by convexId from frontmatter\n   - Compare updatedAt timestamps\n   - If DB newer: check for conflicts using detectConflicts()\n   - Merge human edits using mergeHumanEdits()\n   - Call Convex reconcileFromVault action (engram-ri0) with merged data\n   - Return {success, conflicts[]}\n\n2. detectConflicts(dbFact, fileFact) returns ConflictField[]:\n   - Check HUMAN_EDITABLE_FIELDS for divergent changes\n   - Return array of {field, dbValue, fileValue}\n\n3. writeConflictFile(filePath, dbFact, fileFact, conflicts):\n   - Create {filename}.conflict.md in vault/.meta/conflicts/\n   - Show both values side-by-side with resolution instructions\n\n**Provenance tracking:**\n- All markdown files include provenance footer: agent, timestamp, accessedCount, sessionId\n- Frontmatter includes: source (session/import/migration), sessionId\n\n**New MCP tool: memory_vault_diff**\n- Show pending changes between vault and Convex before sync\n\n**Tests:** reconcile-e2e.test.ts, conflict-e2e.test.ts, roundtrip.test.ts\n**Performance:** Reconcile \u003c200ms p95, zero data loss over 10k ops\nRef: specs/obsidian-mirror-plan.md §2.4","status":"closed","priority":3,"issue_type":"task","created_at":"2026-02-14T22:57:07.001323+01:00","updated_at":"2026-02-14T23:25:41.908183+01:00","closed_at":"2026-02-14T23:25:41.908183+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-8nz","depends_on_id":"engram-waf","type":"blocks","created_at":"2026-02-14T23:08:15.003764+01:00","created_by":"daemon"},{"issue_id":"engram-8nz","depends_on_id":"engram-ri0","type":"blocks","created_at":"2026-02-14T23:08:15.449398+01:00","created_by":"daemon"}]}
{"id":"engram-8ro","title":"Phase 5: Local Sync — LanceDB daemon, offline vector search fallback","status":"closed","priority":0,"issue_type":"epic","created_at":"2026-02-12T10:02:08.462183+01:00","updated_at":"2026-02-12T10:06:05.733398+01:00","closed_at":"2026-02-12T10:06:05.733398+01:00","close_reason":"Completed"}
{"id":"engram-95y","title":"Decompose memory_get_context into primitives","description":"Create 6+ tools: memory_search_facts, memory_search_entities, memory_search_themes, memory_get_handoffs, memory_get_notifications, memory_mark_notifications_read","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-15T00:38:32.517933+01:00","updated_at":"2026-02-15T00:58:47.763739+01:00","closed_at":"2026-02-15T00:58:47.763739+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-95y","depends_on_id":"engram-ai3","type":"blocks","created_at":"2026-02-15T00:39:05.193828+01:00","created_by":"daemon"}]}
{"id":"engram-9jp","title":"Parallel processing with dedup merge pass","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T14:05:00.095489+01:00","updated_at":"2026-02-25T14:05:00.095489+01:00","dependencies":[{"issue_id":"engram-9jp","depends_on_id":"engram-ol6","type":"blocks","created_at":"2026-02-25T14:05:44.461206+01:00","created_by":"daemon"}]}
{"id":"engram-9q5","title":"Comprehensive test expansion: unit, e2e, security, performance","description":"Add full coverage requested by plan: primitive tool unit tests, orchestration wrapper equivalence tests, cross-agent coordination E2E, scope-isolation security tests, and performance/load harness checks. Ensure detailed logging and deterministic assertions for CI use.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-15T00:33:58.990264+01:00","updated_at":"2026-02-15T00:51:19.863974+01:00","closed_at":"2026-02-15T00:51:19.863974+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-9q5","depends_on_id":"engram-4rr","type":"blocks","created_at":"2026-02-15T00:34:11.945326+01:00","created_by":"daemon"},{"issue_id":"engram-9q5","depends_on_id":"engram-djs","type":"blocks","created_at":"2026-02-15T00:34:12.335615+01:00","created_by":"daemon"},{"issue_id":"engram-9q5","depends_on_id":"engram-3sg","type":"blocks","created_at":"2026-02-15T00:34:12.725508+01:00","created_by":"daemon"}]}
{"id":"engram-adj","title":"MCP Tool: memory_get_manifest (tiered context overview)","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-25T14:04:54.775377+01:00","updated_at":"2026-02-25T14:25:43.432683+01:00","closed_at":"2026-02-25T14:25:43.432683+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-adj","depends_on_id":"engram-2kn","type":"blocks","created_at":"2026-02-25T14:05:40.038697+01:00","created_by":"daemon"}]}
{"id":"engram-adw","title":"Phase 1: Progressive Disclosure Layer","status":"closed","priority":0,"issue_type":"epic","created_at":"2026-02-25T14:04:54.397993+01:00","updated_at":"2026-02-25T14:32:51.005735+01:00","closed_at":"2026-02-25T14:32:51.005735+01:00","close_reason":"Phase 1 Progressive Disclosure complete: pinned facts, manifest tool, system prompt integration, pin/unpin tools, summary auto-gen"}
{"id":"engram-aeb","title":"Convex: Reflection cron job (every 4h, dedup + consolidation)","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-25T14:04:55.911951+01:00","updated_at":"2026-02-25T14:23:51.370346+01:00","closed_at":"2026-02-25T14:23:51.370346+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-aeb","depends_on_id":"engram-2kn","type":"blocks","created_at":"2026-02-25T14:05:41.243011+01:00","created_by":"daemon"}]}
{"id":"engram-ai3","title":"Phase 1: Configuration Foundation","description":"Extract hardcoded configs to database tables (system_config, memory_policies). Implement config resolver with priority: scope policy \u003e system config \u003e fallback. Add DELETE operations for 7 entities. Migration script to seed extracted constants.","status":"closed","priority":0,"issue_type":"epic","created_at":"2026-02-15T00:38:07.052306+01:00","updated_at":"2026-02-15T00:51:19.879705+01:00","closed_at":"2026-02-15T00:51:19.879705+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-ai3","depends_on_id":"engram-26p","type":"blocks","created_at":"2026-02-15T00:38:57.246827+01:00","created_by":"daemon"},{"issue_id":"engram-ai3","depends_on_id":"engram-17d","type":"blocks","created_at":"2026-02-15T00:38:57.639198+01:00","created_by":"daemon"},{"issue_id":"engram-ai3","depends_on_id":"engram-jsf","type":"blocks","created_at":"2026-02-15T00:38:58.041626+01:00","created_by":"daemon"},{"issue_id":"engram-ai3","depends_on_id":"engram-6v6","type":"blocks","created_at":"2026-02-15T00:38:58.413548+01:00","created_by":"daemon"},{"issue_id":"engram-ai3","depends_on_id":"engram-46p","type":"blocks","created_at":"2026-02-15T00:38:59.022649+01:00","created_by":"daemon"}]}
{"id":"engram-aih","title":"Phase 3: Version History","description":"Non-destructive memory mutations with rollback. Add versions table to Convex schema (factId, previousContent, changedBy, changedAt, changeType, reason). New MCP tools: memory_history, memory_rollback. Commit-message style mutation logging. Ref: memory/2026-02-25-letta-context-repos.md Phase 3","status":"closed","priority":1,"issue_type":"feature","created_at":"2026-02-25T14:13:54.906766+01:00","updated_at":"2026-02-25T14:32:54.609221+01:00","closed_at":"2026-02-25T14:32:54.609221+01:00","close_reason":"Phase 3 Version History complete: fact_versions table, memory_history, memory_rollback, version snapshots on update"}
{"id":"engram-ajw","title":"Emotional Weight in Ranking","description":"# Emotional Weight in Ranking\n\n## Problem\n`emotionalContext` and `emotionalWeight` fields exist in schema but aren't used in retrieval scoring. Emotionally significant events (\"production went down\", \"shipped v1.0\") rank the same as mundane facts (\"updated package.json\").\n\n## Solution (from GIZIN Pattern)\nActivate emotional weighting in recall ranking:\n1. Boost facts with high emotionalWeight in ranking\n2. Auto-detect emotional context in `observe` tool\n3. Emotionally tagged memories resist decay\n\n## Ranking Integration\n```typescript\n// In mcp-server/src/tools/rank-candidates.ts\nconst emotionalBoost = (fact.emotionalWeight ?? 0) * 0.15; // 15% boost per weight point\nfinalScore = baseScore * (1 + emotionalBoost);\n```\n\n## Auto-Detection in Observe\n```typescript\n// In observe tool, before storing fact:\nfunction detectEmotionalContext(content: string): { context: string, weight: number } {\n  const patterns = {\n    frustrated: /\\b(frustrated|annoying|stuck|broken|fails)\\b/i,\n    proud: /\\b(proud|shipped|launched|success|achieved|completed)\\b/i,\n    critical: /\\b(critical|urgent|emergency|production|outage|down)\\b/i,\n    surprised: /\\b(surprised|unexpected|wow|discovered)\\b/i,\n    confident: /\\b(confident|certain|obviously|clearly)\\b/i,\n  };\n  \n  for (const [context, pattern] of Object.entries(patterns)) {\n    if (pattern.test(content)) {\n      const weight = context === \"critical\" ? 1.0 : \n                     context === \"proud\" ? 0.8 :\n                     context === \"frustrated\" ? 0.6 : 0.4;\n      return { context, weight };\n    }\n  }\n  \n  return { context: \"neutral\", weight: 0 };\n}\n```\n\n## Decay Resistance\n```typescript\n// In decay calculations:\nconst emotionalResistance = fact.emotionalWeight ?? 0;\nconst decayRate = baseDecayRate * (1 - emotionalResistance * 0.5); // Up to 50% slower decay\n```\n\n## Impact\n- Better recall of significant events\n- Aligns with human memory (emotional events more memorable)\n- Minimal implementation (fields already exist)\n- ~15% relevance improvement for event-based queries\n\n## No Dependencies\nCan be implemented immediately.\n\n## References\n- GIZIN emotional memory pattern\n- Current schema: emotionalContext, emotionalWeight fields exist\n- Optimization doc: docs/OPTIMIZATION-2026-02-24.md (section 6)","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-24T09:33:13.795435+01:00","updated_at":"2026-02-25T03:06:38.936568+01:00","closed_at":"2026-02-25T03:06:38.936568+01:00","close_reason":"Closed"}
{"id":"engram-ajw.1","title":"Add emotional boost to rank-candidates.ts","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-24T09:41:24.099453+01:00","updated_at":"2026-02-25T02:56:05.358243+01:00","closed_at":"2026-02-25T02:56:05.358243+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-ajw.1","depends_on_id":"engram-ajw","type":"parent-child","created_at":"2026-02-24T09:41:24.10032+01:00","created_by":"daemon"}]}
{"id":"engram-ajw.2","title":"Auto-detect emotional context in observe","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-24T09:41:24.315298+01:00","updated_at":"2026-02-25T02:56:05.766349+01:00","closed_at":"2026-02-25T02:56:05.766349+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-ajw.2","depends_on_id":"engram-ajw","type":"parent-child","created_at":"2026-02-24T09:41:24.3171+01:00","created_by":"daemon"}]}
{"id":"engram-ajw.3","title":"Add emotional decay resistance","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-24T09:41:24.522845+01:00","updated_at":"2026-02-25T02:56:06.15391+01:00","closed_at":"2026-02-25T02:56:06.15391+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-ajw.3","depends_on_id":"engram-ajw","type":"parent-child","created_at":"2026-02-24T09:41:24.524502+01:00","created_by":"daemon"}]}
{"id":"engram-ajw.4","title":"Unit tests: Emotional weight scoring","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-24T09:41:24.70834+01:00","updated_at":"2026-02-25T11:41:11.857629+01:00","closed_at":"2026-02-25T11:41:11.857629+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-ajw.4","depends_on_id":"engram-ajw","type":"parent-child","created_at":"2026-02-24T09:41:24.709631+01:00","created_by":"daemon"}]}
{"id":"engram-ajw.5","title":"E2E test: Emotional fact prioritization","status":"closed","priority":3,"issue_type":"task","created_at":"2026-02-24T09:41:24.889142+01:00","updated_at":"2026-02-25T03:18:32.707325+01:00","closed_at":"2026-02-25T03:18:32.707325+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-ajw.5","depends_on_id":"engram-ajw","type":"parent-child","created_at":"2026-02-24T09:41:24.890194+01:00","created_by":"daemon"}]}
{"id":"engram-amx","title":"MCP Tool: memory_history (version log per fact)","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-25T14:04:57.43505+01:00","updated_at":"2026-02-25T14:25:13.131684+01:00","closed_at":"2026-02-25T14:25:13.131684+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-amx","depends_on_id":"engram-1pn","type":"blocks","created_at":"2026-02-25T14:05:42.47729+01:00","created_by":"daemon"}]}
{"id":"engram-avb","title":"Schema: Add pinned and summary fields to facts","description":"Add 'pinned: v.optional(v.boolean())' and 'summary: v.optional(v.string())' fields to the facts table in convex/schema.ts. Pinned facts are always loaded into context. Summary provides disclosure without full content.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-25T14:14:22.608818+01:00","updated_at":"2026-02-25T14:17:26.812621+01:00","closed_at":"2026-02-25T14:17:26.812621+01:00","close_reason":"Duplicate of P1 structured set (engram-adw, engram-sws, engram-2kn, engram-adj, engram-aeb, engram-2s3) which has proper dependency chains"}
{"id":"engram-bgw","title":"Profile learning uses first-fact scope instead of agent private scope","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-02-25T11:43:36.044285+01:00","updated_at":"2026-02-25T14:21:25.523055+01:00","closed_at":"2026-02-25T14:21:25.523055+01:00","close_reason":"Closed"}
{"id":"engram-bqy","title":"Explicit emotionalContext doesn't set emotionalWeight in observe","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-02-25T11:43:37.739052+01:00","updated_at":"2026-02-25T14:20:38.322609+01:00","closed_at":"2026-02-25T14:20:38.322609+01:00","close_reason":"Closed"}
{"id":"engram-buv","title":"Consolidation pass: merge near-duplicate facts","description":"Background consolidation that finds semantically similar facts (embedding cosine \u003e 0.9), merges content, archives the weaker duplicate. Extend existing dedup cron in convex/crons/.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-25T14:14:32.938457+01:00","updated_at":"2026-02-25T14:26:52.82464+01:00","closed_at":"2026-02-25T14:26:52.82464+01:00","close_reason":"Consolidation merge library implemented with 23 passing tests"}
{"id":"engram-c48","title":"Phase 2: Tool Primitization","description":"Decompose 11 workflow tools into 25+ atomic primitives. Implement primitives with Convex string-based paths pattern. Add backwards compatibility wrappers. Update MCP tool registry.","status":"closed","priority":0,"issue_type":"epic","created_at":"2026-02-15T00:38:08.383267+01:00","updated_at":"2026-02-15T00:58:50.840153+01:00","closed_at":"2026-02-15T00:58:50.840153+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-c48","depends_on_id":"engram-dt4","type":"blocks","created_at":"2026-02-15T00:39:07.580672+01:00","created_by":"daemon"},{"issue_id":"engram-c48","depends_on_id":"engram-95y","type":"blocks","created_at":"2026-02-15T00:39:08.294758+01:00","created_by":"daemon"},{"issue_id":"engram-c48","depends_on_id":"engram-vgb","type":"blocks","created_at":"2026-02-15T00:39:08.909016+01:00","created_by":"daemon"},{"issue_id":"engram-c48","depends_on_id":"engram-1vx","type":"blocks","created_at":"2026-02-15T00:39:09.355218+01:00","created_by":"daemon"}]}
{"id":"engram-ct9","title":"Weekly defrag: rebalance fact categories","description":"Weekly cron that reviews fact distribution across categories, prunes low-importance dormant facts, reorganizes scope structure. Target: focused knowledge base, not unlimited growth.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-25T14:14:33.144771+01:00","updated_at":"2026-02-25T14:26:45.353029+01:00","closed_at":"2026-02-25T14:26:45.353029+01:00","close_reason":"Closed"}
{"id":"engram-d9j","title":"Summary auto-generation in async enrichment pipeline","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-25T14:04:55.352645+01:00","updated_at":"2026-02-25T14:25:15.064643+01:00","closed_at":"2026-02-25T14:25:15.064643+01:00","close_reason":"Summary auto-generation implemented in enrichment pipeline","dependencies":[{"issue_id":"engram-d9j","depends_on_id":"engram-2kn","type":"blocks","created_at":"2026-02-25T14:05:40.730342+01:00","created_by":"daemon"}]}
{"id":"engram-dcq","title":"Benchmark suite: recall@5, precision@5, MRR","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T14:04:59.122297+01:00","updated_at":"2026-02-25T14:04:59.122297+01:00","dependencies":[{"issue_id":"engram-dcq","depends_on_id":"engram-dlz","type":"blocks","created_at":"2026-02-25T14:05:43.684885+01:00","created_by":"daemon"}]}
{"id":"engram-djs","title":"Close CRUD parity gaps for entity/scope/conversation/session and fact operations","description":"Implement remaining action parity tools/mutations listed in plan: deleteEntity, deleteScope (with safe cascade/prevent rules), deleteConversation, deleteSession, updateFact, archiveFact, boostRelevance, createTheme plus MCP wrappers. Enforce scope authorization checks uniformly and add negative tests for unauthorized operations.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-15T00:33:55.801891+01:00","updated_at":"2026-02-15T00:45:34.510579+01:00","closed_at":"2026-02-15T00:45:34.510579+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-djs","depends_on_id":"engram-wii","type":"blocks","created_at":"2026-02-15T00:34:07.911001+01:00","created_by":"daemon"}]}
{"id":"engram-dlz","title":"Update recall tool with QA-aware search + RRF fusion","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-25T14:04:58.741492+01:00","updated_at":"2026-02-25T14:26:26.586545+01:00","closed_at":"2026-02-25T14:26:26.586545+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-dlz","depends_on_id":"engram-fza","type":"blocks","created_at":"2026-02-25T14:05:43.324935+01:00","created_by":"daemon"}]}
{"id":"engram-doo","title":"Observation pipeline: scored format + importance tiers","description":"Priority-tiered observation pipeline with LLM classification:\n\n**New Convex actions:**\n1. convex/actions/classifyObservation.ts:\n   - Use Claude Haiku for fast classification (\u003c2s)\n   - Prompt: Classify into CRITICAL (decisions/commitments/failures), NOTABLE (insights/learnings), BACKGROUND (routine/minor)\n   - Map tier to priority: CRITICAL→0, NOTABLE→1, BACKGROUND→4\n   - Update fact with priority + observationTier (flat field, see engram-ywx)\n   - If P0-P2: Trigger full enrichment (embeddings, entities, importance)\n   - If P3-P4: Trigger background compression\n\n2. convex/actions/compressBackground.ts:\n   - Generate 1-sentence summary (max 20 words) using Haiku\n   - Store original in observationOriginalContent (flat field)\n   - Set observationCompressed = true (flat field)\n   - Set lifecycleState = \"archived\" (skip indexing/recall)\n\n**MCP tool change (mcp-server/src/tools/observe.ts):**\n- Store observation with lifecycleState=\"active\", trigger classifyObservation action (non-blocking)\n- Agent gets immediate response, classification happens async\n\n**Recall filtering (mcp-server/src/tools/recall.ts):**\n- Add priority filter: default exclude P3-P4 unless explicit priorityFilter arg\n\n**Schema fields (in engram-ywx):**\n- observationTier: v.optional(v.string()) — \"critical\"|\"notable\"|\"background\"\n- observationCompressed: v.optional(v.boolean())\n- observationOriginalContent: v.optional(v.string()) — Content before compression\n\nNOTE: All observation.* fields from original plan are flattened for Convex compatibility.\n\n**Tests:** observation-compression-e2e.test.ts, classification-latency.test.ts (\u003c2s)\n**Performance:** Classification \u003c2s, compression \u003c1s, observation noise reduction \u003e80%\nRef: specs/obsidian-mirror-plan.md §3.1-3.3","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-14T22:56:32.190346+01:00","updated_at":"2026-02-14T23:25:03.926873+01:00","closed_at":"2026-02-14T23:25:03.926873+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-doo","depends_on_id":"engram-ywx","type":"blocks","created_at":"2026-02-14T22:57:19.817066+01:00","created_by":"daemon"}]}
{"id":"engram-dt4","title":"Decompose memory_recall into primitives","description":"Create 4 new tools: memory_vector_search, memory_text_search, memory_bump_access, memory_record_recall. Update mcp-server/src/tools/ and add Convex functions with string-based paths","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-15T00:38:30.78059+01:00","updated_at":"2026-02-15T00:58:47.40422+01:00","closed_at":"2026-02-15T00:58:47.40422+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-dt4","depends_on_id":"engram-ai3","type":"blocks","created_at":"2026-02-15T00:39:04.458429+01:00","created_by":"daemon"}]}
{"id":"engram-dwa","title":"Unit tests: Sleep-time reflection","description":"Tests for: reflection extraction, consolidation merging, defrag pruning, notification delivery, cron scheduling.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-25T14:14:33.567258+01:00","updated_at":"2026-02-25T14:27:34.703316+01:00","closed_at":"2026-02-25T14:27:34.703316+01:00","close_reason":"Closed"}
{"id":"engram-dwd","title":"Phase 5: History Bootstrap","description":"Initialize Engram from existing session history. Session ingestion script reads OpenClaw/Claude Code session logs. Fan-out processing with subagents. Dedup on merge via embedding similarity. Run once for existing history, then hook into reflection agent. Ref: memory/2026-02-25-letta-context-repos.md Phase 5","status":"open","priority":2,"issue_type":"feature","created_at":"2026-02-25T14:14:00.461843+01:00","updated_at":"2026-02-25T14:14:00.461843+01:00"}
{"id":"engram-dxj","title":"Add MCP tool: memory_vault_sync","description":"Add MCP tool: memory_vault_sync for manual vault synchronization:\n\n**New tool: memory_vault_sync (mcp-server/src/tools/vault-sync.ts)**\n\nParameters:\n- direction: 'export' | 'import' | 'both' (default: 'both')\n- force: boolean (default: false) — Force re-export even if vaultPath exists\n- dryRun: boolean (default: false) — Show what would be synced without executing\n\n**Export mode (Convex → Vault):**\n- Fetch all facts with vaultPath == null OR force == true\n- Export to markdown using vault-format.ts\n- Update vaultPath + vaultSyncedAt in Convex\n- Return: {exported: count, skipped: count, errors: []}\n\n**Import mode (Vault → Convex):**\n- Scan vault/ for .md files\n- Parse frontmatter, match by ID\n- Reconcile changes using vault-reconciler.ts\n- Update DB with human edits\n- Return: {imported: count, conflicts: count, errors: []}\n\n**Both mode:**\n- Run export first, then import\n- Return combined stats\n\n**Use cases:**\n- Initial vault population after system install\n- Manual sync after vault-sync daemon crashes\n- Batch import after human edits in Obsidian\n- Dry run to preview sync changes\n\n**Integration:**\n- Uses vault-writer.ts (from engram-7yr)\n- Uses vault-reconciler.ts (from engram-8nz)\n- Uses vault-format.ts (from engram-waf)\n\n**Tests:** vault-sync-e2e.test.ts (export/import/both modes work), dry-run.test.ts (no actual changes)\n**Performance:** Export 1000 facts \u003c10s, import 1000 files \u003c15s\nRef: VAULT_INTEGRATION_PLAN.md Phase 3.2, Phase 3.4","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-14T22:56:26.618882+01:00","updated_at":"2026-02-14T23:24:31.181896+01:00","closed_at":"2026-02-14T23:24:31.181896+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-dxj","depends_on_id":"engram-7yr","type":"blocks","created_at":"2026-02-14T22:57:19.207572+01:00","created_by":"daemon"}]}
{"id":"engram-e1w","title":"Tests: Version creation, rollback, audit trail e2e","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-25T14:04:57.996417+01:00","updated_at":"2026-02-25T14:27:47.098676+01:00","closed_at":"2026-02-25T14:27:47.098676+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-e1w","depends_on_id":"engram-mzl","type":"blocks","created_at":"2026-02-25T14:05:42.984539+01:00","created_by":"daemon"}]}
{"id":"engram-fcm","title":"Script: bootstrap-from-sessions.ts (OpenClaw ingestion)","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T14:04:59.714901+01:00","updated_at":"2026-02-25T14:04:59.714901+01:00","dependencies":[{"issue_id":"engram-fcm","depends_on_id":"engram-w8r","type":"blocks","created_at":"2026-02-25T14:05:44.075015+01:00","created_by":"daemon"}]}
{"id":"engram-fza","title":"QA generation in async enrichment pipeline","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-25T14:04:58.556398+01:00","updated_at":"2026-02-25T14:23:02.072466+01:00","closed_at":"2026-02-25T14:23:02.072466+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-fza","depends_on_id":"engram-7hb","type":"blocks","created_at":"2026-02-25T14:05:43.156027+01:00","created_by":"daemon"}]}
{"id":"engram-g3o","title":"File watcher for two-way sync (fs to Convex)","status":"open","priority":3,"issue_type":"task","created_at":"2026-02-25T14:05:01.046847+01:00","updated_at":"2026-02-25T14:05:01.046847+01:00","dependencies":[{"issue_id":"engram-g3o","depends_on_id":"engram-uw7","type":"blocks","created_at":"2026-02-25T14:05:45.173292+01:00","created_by":"daemon"}]}
{"id":"engram-gd6","title":"Phase 3: Version History","status":"closed","priority":1,"issue_type":"epic","created_at":"2026-02-25T14:04:56.844726+01:00","updated_at":"2026-02-25T14:32:54.582411+01:00","closed_at":"2026-02-25T14:32:54.582411+01:00","close_reason":"Phase 3 Version History complete: fact_versions table, memory_history, memory_rollback, version snapshots on update"}
{"id":"engram-gxr","title":"Vault index pipeline implementation + cron trigger","description":"Vault index pipeline for index-first retrieval:\n\n**New file: mcp-server/src/lib/vault-indexer.ts**\nImplements index generation:\n\n1. generateIndices(convex, vaultRoot) — Master function, creates:\n   - vault/.index/vault-index.md — Master TOC (recent + by-type sections)\n   - vault/.index/by-priority.md — Facts grouped by priority tier (0-4)\n   - vault/.index/by-entity.md — Facts grouped by entity mentions\n\n2. generateMasterIndex(facts, vaultRoot):\n   - Recent section: Last 7 days, sorted by date desc\n   - By Type section: Group by fact.type, sort by importance desc, show top 10 per type\n   - Format: '- [date] **Type**: title → path [importance: X.XX]'\n\n3. generatePriorityIndex(facts, vaultRoot):\n   - 5 sections: Critical (P0), High (P1), Medium (P2), Low (P3), Backlog (P4)\n   - Sort within tier by importance desc, show top 20 per tier\n\n4. generateEntityIndex(facts, vaultRoot):\n   - Build mention count map from wiki-links\n   - Sort entities by mention count desc\n   - For each entity: show top 10 facts by importance\n\n**Recall tool enhancement (mcp-server/src/tools/recall.ts):**\n- searchIndex(query, filters) — Scan relevant index file, extract matching paths, compute confidence\n- Index-first strategy: If 5+ matches AND confidence \u003e0.7 → return index results, else fallback to semantic\n- Confidence = (keyword coverage) × (match density)\n\n**Convex cron (convex/crons/regenerateIndices.ts):**\n- Schedule: */5 * * * * (every 5 minutes)\n- Action: Signal MCP daemon to regenerate indices (webhook or polling flag)\n\n**Tests:** index-generation.test.ts (verify format), index-first-e2e.test.ts (query → index scan → results)\n**Performance:** Index scan \u003c100ms p95, index hit rate \u003e65%, relevance@5 \u003e0.85\nRef: VAULT_INTEGRATION_PLAN.md Phase 5 (sections 5.2-5.7)","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-14T23:00:26.85327+01:00","updated_at":"2026-02-14T23:25:26.47989+01:00","closed_at":"2026-02-14T23:25:26.47989+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-gxr","depends_on_id":"engram-7yr","type":"blocks","created_at":"2026-02-14T23:01:38.333371+01:00","created_by":"daemon"},{"issue_id":"engram-gxr","depends_on_id":"engram-43e","type":"blocks","created_at":"2026-02-14T23:01:44.162538+01:00","created_by":"daemon"}]}
{"id":"engram-h09","title":"Phase 3: Async Enrichment — Cohere Embed 4, entity extraction, synthesis, importance","status":"closed","priority":0,"issue_type":"epic","created_at":"2026-02-12T09:50:59.367012+01:00","updated_at":"2026-02-12T10:06:05.440978+01:00","closed_at":"2026-02-12T10:06:05.440978+01:00","close_reason":"Completed"}
{"id":"engram-hgc","title":"Implement multi-source context gathering in memory_get_context","description":"Complete remaining UNIMPLEMENTED.md item: gather observations by tier plus daily notes/search results/graph-neighbor expansion in get-context.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-14T23:53:27.166421+01:00","updated_at":"2026-02-14T23:54:17.055783+01:00","closed_at":"2026-02-14T23:54:17.055783+01:00","close_reason":"Closed"}
{"id":"engram-hna","title":"Agent-native production refactor plan execution","description":"Execute docs/plans/2026-02-14-refactor-agent-native-architecture-production-ready-plan.md and docs/plans/DEPLOYMENT-VERIFICATION-CHECKLIST.md end-to-end as a dependency-driven bead graph. This epic tracks conversion of remaining plan items into production-ready deliverables: schema/config foundations, primitive tool decomposition, action parity gaps, event propagation, security/performance hardening, docs overhaul, and deployment verification artifacts. Every child bead must be self-contained and include explicit validation outcomes so implementation can run without re-opening the source plan docs.","status":"closed","priority":0,"issue_type":"epic","created_at":"2026-02-15T00:33:52.444203+01:00","updated_at":"2026-02-15T00:39:31.911421+01:00","closed_at":"2026-02-15T00:39:31.911421+01:00","close_reason":"Superseded by engram-ai3, engram-c48, engram-1le (new phase epics with detailed tasks)"}
{"id":"engram-hx4","title":"Implement memory_events propagation and poll primitive","description":"Emit memory_events for storeFact, enrichment completion, and notification creation. Add memory_poll_events tool with watermark-based polling semantics and pagination. Document client polling loop and latency targets (\u003c5s end-to-end propagation). Include event ordering/idempotency tests.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-15T00:33:56.286293+01:00","updated_at":"2026-02-15T00:45:34.509931+01:00","closed_at":"2026-02-15T00:45:34.509931+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-hx4","depends_on_id":"engram-wii","type":"blocks","created_at":"2026-02-15T00:34:08.292966+01:00","created_by":"daemon"}]}
{"id":"engram-ic4","title":"Unit tests: Progressive disclosure","description":"Tests for: pinned fact querying, memory manifest generation, pin/unpin tools, summary field population, system prompt integration with manifest.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-25T14:14:23.922627+01:00","updated_at":"2026-02-25T14:30:02.629938+01:00","closed_at":"2026-02-25T14:30:02.629938+01:00","close_reason":"Closed"}
{"id":"engram-jsf","title":"Create config migration script","description":"Build convex/migrations/001_seed_system_config.ts to extract all hardcoded constants from importance.ts, decay.ts, forget.ts, ranking.ts and seed into system_config table","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-15T00:38:25.098786+01:00","updated_at":"2026-02-15T00:49:13.01154+01:00","closed_at":"2026-02-15T00:49:13.01154+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-jsf","depends_on_id":"engram-17d","type":"blocks","created_at":"2026-02-15T00:38:56.468795+01:00","created_by":"daemon"}]}
{"id":"engram-li6","title":"MCP Tools: memory_pin / memory_unpin with max limit","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-25T14:04:54.960781+01:00","updated_at":"2026-02-25T14:24:11.642808+01:00","closed_at":"2026-02-25T14:24:11.642808+01:00","close_reason":"Duplicate of engram-14c (pin/unpin MCP tools) which is already implemented","dependencies":[{"issue_id":"engram-li6","depends_on_id":"engram-2kn","type":"blocks","created_at":"2026-02-25T14:05:40.208864+01:00","created_by":"daemon"}]}
{"id":"engram-llh","title":"BUG: Centroid subtraction missing in integrateNewFact","status":"closed","priority":0,"issue_type":"bug","created_at":"2026-02-25T11:43:33.711763+01:00","updated_at":"2026-02-25T14:20:42.188767+01:00","closed_at":"2026-02-25T14:20:42.188767+01:00","close_reason":"Closed"}
{"id":"engram-mzl","title":"MCP Tool: memory_rollback (restore from version)","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-25T14:04:57.622662+01:00","updated_at":"2026-02-25T14:28:35.442447+01:00","closed_at":"2026-02-25T14:28:35.442447+01:00","close_reason":"memory_rollback MCP tool implemented with immutable history, optional versionId targeting, and restore audit trail","dependencies":[{"issue_id":"engram-mzl","depends_on_id":"engram-amx","type":"blocks","created_at":"2026-02-25T14:05:42.645895+01:00","created_by":"daemon"}]}
{"id":"engram-naa","title":"KV Store for Deterministic Facts","description":"# KV Store for Deterministic Facts\n\n## Problem\nEngram's current recall uses hybrid text+vector search for all queries. This creates false negatives for **explicit, deterministic facts** like preferences, rules, and configuration data. For example:\n- \"Ryan prefers Bun over npm\" should ALWAYS be recalled when querying about package managers\n- Vector similarity might miss this if embeddings drift or query phrasing changes\n- Sub-5ms deterministic recall is impossible with vector search\n\n## Solution (from Mem0)\nAdd a **key_value_facts** table for exact-match retrieval of deterministic facts. Three-store architecture:\n1. **KV store**: Deterministic facts (preferences, rules, profile data)\n2. **Vector store**: Semantic/fuzzy search (current implementation)\n3. **Graph store**: Relationship traversal (current entities table)\n\n## Schema Changes\n```typescript\n// Add to convex/schema.ts\nkey_value_facts: defineTable({\n  key: v.string(),           // normalized: \"ryan.preference.package_manager\"\n  value: v.string(),         // \"bun\"\n  factId: v.id(\"facts\"),   // backlink to full fact for provenance\n  scopeId: v.id(\"memory_scopes\"),\n  agentId: v.string(),\n  updatedAt: v.number(),\n  metadata: v.optional(v.record(v.string(), metadataValue)), // extensible\n}).index(\"by_key\", [\"key\"])\n  .index(\"by_scope\", [\"scopeId\"])\n  .index(\"by_scope_key\", [\"scopeId\", \"key\"])\n```\n\n## Auto-Classification Logic\nWhen `store_fact` is called, auto-detect deterministic facts:\n- Patterns: \"prefers\", \"always\", \"never\", \"rule:\", \"default is\"\n- Store in BOTH facts table AND key_value_facts\n- Key generation: entity-based (\"ryan.preference.X\") or topic-based (\"project.briefly.build_tool\")\n\n## Recall Flow Changes\n```typescript\n// In recall.ts, add KV lookup BEFORE vector/text search:\n1. Extract entities/keywords from query\n2. Generate potential KV keys\n3. Check key_value_facts for exact matches (\u003c 5ms)\n4. If found: boost these facts to top of results\n5. Fall through to hybrid vector/text for remaining slots\n```\n\n## Impact\n- Eliminates false negatives on explicit preferences/rules\n- Sub-5ms for deterministic recalls (vs 50-200ms for vector)\n- Reduces vector DB load for simple lookups\n- Enables strict policy enforcement (\"always do X\", \"never do Y\")\n\n## References\n- Mem0 architecture: three-store pattern (KV + vector + graph)\n- Current schema: convex/schema.ts (lines 1-100)\n- Current recall: mcp-server/src/tools/recall.ts\n- Optimization doc: docs/OPTIMIZATION-2026-02-24.md (section 1)","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-24T09:33:12.627165+01:00","updated_at":"2026-02-25T01:42:07.953745+01:00","closed_at":"2026-02-25T01:42:07.953745+01:00","close_reason":"Closed"}
{"id":"engram-naa.1","title":"Schema: Add key_value_facts table","description":"Add `key_value_facts` table to `convex/schema.ts` for deterministic fact storage.\n\n**Location**: `convex/schema.ts` (after line 300, before sync_log)\n\n**Schema Definition**:\n```typescript\nkey_value_facts: defineTable({\n  key: v.string(),           // normalized key (e.g., \"ryan.preference.package_manager\")\n  value: v.string(),         // stringified value (\"bun\")\n  factId: v.id(\"facts\"),   // backlink to source fact for full context/provenance\n  scopeId: v.id(\"memory_scopes\"),\n  agentId: v.string(),       // agent that stored this KV\n  updatedAt: v.number(),     // timestamp for cache invalidation\n  metadata: v.optional(v.record(v.string(), metadataValue)), // extensible metadata\n}).index(\"by_key\", [\"key\"])\n  .index(\"by_scope\", [\"scopeId\"])\n  .index(\"by_scope_key\", [\"scopeId\", \"key\"])  // composite for scoped lookups\n  .index(\"by_fact\", [\"factId\"])               // reverse lookup\n```\n\n**Key Generation Rules**:\n- Entity-based: `{entity}.{category}.{attribute}` (e.g., \"ryan.preference.editor\")\n- Project-based: `project.{name}.{setting}` (e.g., \"project.engram.build_tool\")\n- Global: `global.{category}.{key}` (e.g., \"global.policy.max_tokens\")\n\n**Migration**: None needed (new table, no existing data).\n\n**Validation**: Ensure indices are created correctly, test query performance with `convex dev`.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-24T09:33:48.968628+01:00","updated_at":"2026-02-25T01:42:07.925328+01:00","closed_at":"2026-02-25T01:42:07.925328+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-naa.1","depends_on_id":"engram-naa","type":"parent-child","created_at":"2026-02-24T09:33:48.973985+01:00","created_by":"daemon"}]}
{"id":"engram-naa.2","title":"Convex: Implement kv-store.ts mutations","description":"Create `convex/kv-store.ts` with mutations for key-value fact management.\n\n**File**: `convex/kv-store.ts` (new file)\n\n**Exports**:\n1. `setKV(ctx, { key, value, factId, scopeId, agentId })` — Insert or update KV fact\n2. `getKV(ctx, { key, scopeId? })` — Query by exact key, optionally scoped\n3. `deleteKV(ctx, { key, scopeId })` — Remove KV fact\n4. `listKVByScope(ctx, { scopeId, limit? })` — List all KV facts in scope\n5. `batchSetKV(ctx, { entries: Array\u003c{key, value, factId, scopeId, agentId}\u003e })` — Bulk insert\n\n**Implementation Details**:\n```typescript\nimport { mutation, query } from \"./_generated/server\";\nimport { v } from \"convex/values\";\n\nexport const setKV = mutation({\n  args: {\n    key: v.string(),\n    value: v.string(),\n    factId: v.id(\"facts\"),\n    scopeId: v.id(\"memory_scopes\"),\n    agentId: v.string(),\n  },\n  handler: async (ctx, args) =\u003e {\n    // Check if key exists in scope\n    const existing = await ctx.db\n      .query(\"key_value_facts\")\n      .withIndex(\"by_scope_key\", (q) =\u003e q.eq(\"scopeId\", args.scopeId).eq(\"key\", args.key))\n      .unique();\n\n    if (existing) {\n      // Update existing\n      await ctx.db.patch(existing._id, {\n        value: args.value,\n        factId: args.factId,\n        updatedAt: Date.now(),\n      });\n      return { kvId: existing._id, updated: true };\n    } else {\n      // Insert new\n      const kvId = await ctx.db.insert(\"key_value_facts\", {\n        ...args,\n        updatedAt: Date.now(),\n      });\n      return { kvId, updated: false };\n    }\n  },\n});\n\nexport const getKV = query({\n  args: { key: v.string(), scopeId: v.optional(v.id(\"memory_scopes\")) },\n  handler: async (ctx, args) =\u003e {\n    if (args.scopeId) {\n      return await ctx.db\n        .query(\"key_value_facts\")\n        .withIndex(\"by_scope_key\", (q) =\u003e q.eq(\"scopeId\", args.scopeId).eq(\"key\", args.key))\n        .unique();\n    } else {\n      return await ctx.db\n        .query(\"key_value_facts\")\n        .withIndex(\"by_key\", (q) =\u003e q.eq(\"key\", args.key))\n        .first();\n    }\n  },\n});\n```\n\n**Error Handling**: Return `null` for missing keys (not an error), throw on invalid scopeId.\n\n**Testing**: Test upsert logic, scoped vs global queries, batch inserts.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-24T09:33:49.276633+01:00","updated_at":"2026-02-25T01:42:07.962315+01:00","closed_at":"2026-02-25T01:42:07.962315+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-naa.2","depends_on_id":"engram-naa","type":"parent-child","created_at":"2026-02-24T09:33:49.277482+01:00","created_by":"daemon"}]}
{"id":"engram-naa.3","title":"MCP Tool: Update store_fact with auto-classification","description":"Update `mcp-server/src/tools/store-fact.ts` to auto-classify and store deterministic facts in KV store.\n\n**File**: `mcp-server/src/tools/store-fact.ts`\n\n**Changes**:\n1. **Import KV mutations**: `import { internal } from \"@/convex/_generated/api\";`\n2. **Add classification function**:\n```typescript\nfunction isDeterministicFact(content: string): boolean {\n  const patterns = [\n    /\\b(prefers?|preference)\\b/i,\n    /\\b(always|never)\\b/i,\n    /\\brule:/i,\n    /\\bdefault (is|to)\\b/i,\n    /\\b(must|should) (always|never)\\b/i,\n    /\\bsetting:/i,\n  ];\n  return patterns.some(p =\u003e p.test(content));\n}\n\nfunction extractKVKey(content: string, entities: string[]): string | null {\n  // Simple heuristic: first entity + attribute from pattern\n  // Example: \"Ryan prefers Bun\" → entities=[\"ryan\"], content has \"prefers Bun\"\n  // Result: \"ryan.preference.package_manager\" (needs NLP in v2)\n  \n  if (entities.length === 0) return null;\n  const entity = entities[0].toLowerCase().replace(/\\s+/g, \"_\");\n  \n  // Extract category from keywords\n  if (/prefers?|preference/i.test(content)) {\n    // Simple extraction: match \"prefers X\" or \"preference for X\"\n    const match = content.match(/prefers?\\s+([\\w\\-]+)/i);\n    if (match) return `${entity}.preference.${match[1].toLowerCase()}`;\n  }\n  \n  if (/rule:/i.test(content)) {\n    const match = content.match(/rule:\\s*([\\w\\-]+)/i);\n    if (match) return `${entity}.rule.${match[1].toLowerCase()}`;\n  }\n  \n  return null; // Fall back to fact-only storage\n}\n```\n\n3. **After fact insertion, call KV store**:\n```typescript\n// After const factId = await ctx.runMutation(...)\nif (isDeterministicFact(content)) {\n  const kvKey = extractKVKey(content, entityIds);\n  if (kvKey) {\n    await ctx.runMutation(internal.kv.setKV, {\n      key: kvKey,\n      value: content, // Store full content; can compress in v2\n      factId,\n      scopeId: resolvedScopeId,\n      agentId,\n    });\n    console.log(`[store-fact] Auto-stored KV: ${kvKey}`);\n  }\n}\n```\n\n**Testing**: Store \"Ryan prefers Bun over npm\" and verify KV entry created with key \"ryan.preference.bun\".","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-24T09:33:49.572818+01:00","updated_at":"2026-02-25T01:42:07.969515+01:00","closed_at":"2026-02-25T01:42:07.969515+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-naa.3","depends_on_id":"engram-naa","type":"parent-child","created_at":"2026-02-24T09:33:49.573598+01:00","created_by":"daemon"}]}
{"id":"engram-naa.4","title":"MCP Tool: Update recall with KV lookup","description":"Update `mcp-server/src/tools/recall.ts` to check KV store BEFORE vector/text search.\n\n**File**: `mcp-server/src/tools/recall.ts`\n\n**Changes**:\n1. **Import KV query**: `import { internal } from \"@/convex/_generated/api\";`\n2. **Add KV lookup function**:\n```typescript\nasync function kvLookup(\n  ctx: any,\n  query: string,\n  scopeIds: string[]\n): Promise\u003cany[]\u003e {\n  // Extract potential KV keys from query\n  // Example: \"what does ryan prefer for package manager\" → try \"ryan.preference.package_manager\"\n  const kvKeys = extractPotentialKeys(query);\n  const results = [];\n  \n  for (const key of kvKeys) {\n    for (const scopeId of scopeIds) {\n      const kv = await ctx.runQuery(internal.kv.getKV, { key, scopeId });\n      if (kv) {\n        // Fetch the full fact for return\n        const fact = await ctx.runQuery(internal.facts.get, { id: kv.factId });\n        if (fact) results.push({ ...fact, _kvMatch: true });\n      }\n    }\n  }\n  \n  return results;\n}\n\nfunction extractPotentialKeys(query: string): string[] {\n  const keys: string[] = [];\n  \n  // Pattern: \"what does {entity} prefer for {thing}\"\n  const preferMatch = query.match(/what does (\\w+) prefer for ([\\w\\s]+)/i);\n  if (preferMatch) {\n    const entity = preferMatch[1].toLowerCase();\n    const thing = preferMatch[2].toLowerCase().replace(/\\s+/g, \"_\");\n    keys.push(`${entity}.preference.${thing}`);\n  }\n  \n  // Pattern: \"ryan's preference for X\"\n  const possessiveMatch = query.match(/(\\w+)'s preference for ([\\w\\s]+)/i);\n  if (possessiveMatch) {\n    const entity = possessiveMatch[1].toLowerCase();\n    const thing = possessiveMatch[2].toLowerCase().replace(/\\s+/g, \"_\");\n    keys.push(`${entity}.preference.${thing}`);\n  }\n  \n  return keys;\n}\n```\n\n3. **Integrate into recall flow**:\n```typescript\n// In recall() function, BEFORE vectorSearch/textSearch:\nconst kvResults = await kvLookup(ctx, input.query, scopeIds);\n\n// After ranking:\nconst ranked = await rankCandidatesPrimitive({ query, candidates, limit });\n\n// Boost KV results to top\nconst boostedResults = [\n  ...kvResults.map(f =\u003e ({ ...f, _boosted: true })),\n  ...ranked.ranked.filter(r =\u003e !kvResults.some(kv =\u003e kv._id === r._id))\n].slice(0, input.limit);\n```\n\n**Testing**: Query \"what does ryan prefer for package manager\" after storing \"Ryan prefers Bun\", verify Bun fact is top result.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-24T09:33:49.942376+01:00","updated_at":"2026-02-25T02:49:22.242165+01:00","closed_at":"2026-02-25T02:49:22.242165+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-naa.4","depends_on_id":"engram-naa","type":"parent-child","created_at":"2026-02-24T09:33:49.944222+01:00","created_by":"daemon"}]}
{"id":"engram-naa.5","title":"Unit Tests: KV store mutations","description":"Unit tests for KV store mutations in `convex/kv-store.test.ts` (create new file).\n\n**File**: `convex/kv-store.test.ts` (new file)\n\n**Test Coverage**:\n1. **setKV - Insert new**:\n   - Create new KV entry\n   - Verify return value `{ kvId, updated: false }`\n   - Query back and validate fields\n   \n2. **setKV - Update existing**:\n   - Insert KV entry\n   - Call setKV again with same key+scope\n   - Verify return value `{ kvId, updated: true }`\n   - Validate `updatedAt` changed, `value` updated\n   \n3. **getKV - Scoped query**:\n   - Insert KV in scope A and scope B with same key\n   - Query with scopeId=A, verify correct entry returned\n   - Query with scopeId=B, verify different entry\n   \n4. **getKV - Global query** (no scopeId):\n   - Insert KV in multiple scopes\n   - Query without scopeId\n   - Verify returns first match (or most recent?)\n   \n5. **deleteKV**:\n   - Insert KV\n   - Delete by key+scope\n   - Verify getKV returns null\n   \n6. **listKVByScope**:\n   - Insert 5 KV entries in scope\n   - List with limit=3\n   - Verify returns 3 entries\n   \n7. **batchSetKV**:\n   - Insert 100 KV entries in batch\n   - Verify all inserted\n   - Test mixed insert/update batch\n\n**Framework**: Use Convex testing utilities (convex dev test environment).\n\n**Run**: `npm test` (add script to package.json if missing).","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-24T09:33:50.191064+01:00","updated_at":"2026-02-25T03:19:50.034399+01:00","closed_at":"2026-02-25T03:19:50.034399+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-naa.5","depends_on_id":"engram-naa","type":"parent-child","created_at":"2026-02-24T09:33:50.192601+01:00","created_by":"daemon"}]}
{"id":"engram-naa.6","title":"E2E Test: Deterministic fact recall","description":"E2E test for deterministic fact storage and recall in `mcp-server/test/e2e/kv-recall.test.ts`.\n\n**File**: `mcp-server/test/e2e/kv-recall.test.ts` (new file)\n\n**Test Scenario**:\n1. **Setup**:\n   - Register test agent \"test-agent-kv\"\n   - Create scope \"test-kv-scope\"\n   \n2. **Store deterministic fact**:\n   - Call `memory_store_fact` with content: \"Ryan prefers Bun over npm for package management\"\n   - Verify fact created\n   - Verify KV entry created with key containing \"ryan.preference\"\n   \n3. **Recall with exact phrasing**:\n   - Call `memory_recall` with query: \"what does ryan prefer for package manager\"\n   - Verify top result is the Bun preference fact\n   - Verify `_kvMatch: true` or `_boosted: true` flag present\n   \n4. **Recall with different phrasing**:\n   - Call `memory_recall` with query: \"ryan's package manager preference\"\n   - Verify Bun fact still top result (KV lookup generalization)\n   \n5. **Update preference**:\n   - Store new fact: \"Ryan prefers pnpm over Bun now\"\n   - Recall \"ryan's package manager preference\"\n   - Verify pnpm fact is top result (KV updated)\n   \n6. **Scoped isolation**:\n   - Create second scope \"test-kv-scope-2\"\n   - Store \"Alice prefers npm\" in scope 2\n   - Recall in scope 1 with query \"package manager preference\"\n   - Verify only Ryan's preference returned (not Alice's)\n\n**Assertions**:\n- KV lookups complete in \u003c10ms\n- Deterministic facts always rank higher than vector-similar facts\n- Scope isolation maintained\n\n**Run**: `npm run test:e2e`","status":"closed","priority":3,"issue_type":"task","created_at":"2026-02-24T09:33:50.448953+01:00","updated_at":"2026-02-25T03:24:07.195321+01:00","closed_at":"2026-02-25T03:24:07.195321+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-naa.6","depends_on_id":"engram-naa","type":"parent-child","created_at":"2026-02-24T09:33:50.450546+01:00","created_by":"daemon"}]}
{"id":"engram-ne6","title":"Refactor importance/decay/ranking to config-driven resolution","description":"Wire config resolver into importance scoring, decay cron, ranking formula, and prune/forget thresholds so behavior is prompt/config-native rather than hardcoded. Ensure deterministic fallback to hardcoded defaults for backwards compatibility. Add tests that scope-level overrides beat system_config and system_config beats fallback constants.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-15T00:33:53.72684+01:00","updated_at":"2026-02-15T00:49:13.005428+01:00","closed_at":"2026-02-15T00:49:13.005428+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-ne6","depends_on_id":"engram-5y2","type":"blocks","created_at":"2026-02-15T00:34:06.426713+01:00","created_by":"daemon"}]}
{"id":"engram-nsv","title":"Tests: Progressive disclosure unit + integration + e2e","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T14:04:55.539852+01:00","updated_at":"2026-02-25T14:04:55.539852+01:00","dependencies":[{"issue_id":"engram-nsv","depends_on_id":"engram-0v6","type":"blocks","created_at":"2026-02-25T14:05:40.910095+01:00","created_by":"daemon"},{"issue_id":"engram-nsv","depends_on_id":"engram-d9j","type":"blocks","created_at":"2026-02-25T14:05:41.077806+01:00","created_by":"daemon"}]}
{"id":"engram-nt1","title":"Phase 1: Progressive Disclosure Layer","description":"Reduce context window waste by tiering memory. Add pinned field to facts, implement memory manifest tool, add summary field for disclosure without full content load. Biggest impact — context savings. Ref: memory/2026-02-25-letta-context-repos.md Phase 1","status":"closed","priority":0,"issue_type":"feature","created_at":"2026-02-25T14:13:49.479308+01:00","updated_at":"2026-02-25T14:17:26.804494+01:00","closed_at":"2026-02-25T14:17:26.804494+01:00","close_reason":"Duplicate of P1 structured set (engram-adw, engram-sws, engram-2kn, engram-adj, engram-aeb, engram-2s3) which has proper dependency chains"}
{"id":"engram-o6m","title":"MCP Tool: memory_chain_recall (multi-hop QA retrieval)","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-25T14:04:58.927876+01:00","updated_at":"2026-02-25T14:30:01.677644+01:00","closed_at":"2026-02-25T14:30:01.677644+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-o6m","depends_on_id":"engram-dlz","type":"blocks","created_at":"2026-02-25T14:05:43.497973+01:00","created_by":"daemon"}]}
{"id":"engram-ol6","title":"Script: Claude Code history ingestion parser","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T14:04:59.905094+01:00","updated_at":"2026-02-25T14:04:59.905094+01:00","dependencies":[{"issue_id":"engram-ol6","depends_on_id":"engram-fcm","type":"blocks","created_at":"2026-02-25T14:05:44.273369+01:00","created_by":"daemon"}]}
{"id":"engram-pkj","title":"E2E test: Reflection pipeline","description":"Full pipeline: session events accumulate → reflection cron fires → new facts extracted → duplicates merged → agents notified.","status":"open","priority":3,"issue_type":"task","created_at":"2026-02-25T14:14:33.773875+01:00","updated_at":"2026-02-25T14:14:33.773875+01:00"}
{"id":"engram-ppv","title":"Per-Agent Coefficient Profiles","description":"# Per-Agent Coefficient Profiles / Knowledge Axis Weights\n\n## Problem\nAll agents in a scope see the same knowledge subspace with equal weighting. But different agents care about different knowledge dimensions:\n- **Coder agent**: Cares about technical facts (dependencies, APIs, patterns)\n- **Planning agent**: Cares about goals, deadlines, priorities\n- **Research agent**: Cares about concepts, relationships, insights\n\n## Solution (from SHARE Paper - Coefficient-Only Learning)\nGive each agent **lightweight attention weights** over the k principal knowledge axes. Agent retrieval is personalized without rebuilding the entire index.\n\n```\nShared subspace: V_k (k × 1024) — same for all agents\nPer-agent: axis_weights (k weights) — learned from usage\n\nPersonalized similarity:\n  score = Σ (weight_i * coefficient_i * query_coefficient_i)\n```\n\n## Schema Addition\n```typescript\n// New table: agent_knowledge_profiles\nagent_knowledge_profiles: defineTable({\n  agentId: v.string(),\n  scopeId: v.id(\"memory_scopes\"),\n  subspaceId: v.id(\"knowledge_subspaces\"),\n  axisWeights: v.array(v.float64()),  // k weights, one per principal direction\n  learnedFrom: v.number(),             // # recalls used for learning\n  updatedAt: v.number(),\n}).index(\"by_agent_scope\", [\"agentId\", \"scopeId\"])\n  .index(\"by_subspace\", [\"subspaceId\"])\n```\n\n## Learning Algorithm\n```typescript\n// Learn axis weights from recall feedback:\n// 1. When agent recalls facts, track which facts were used\n// 2. Extract coefficient patterns from used facts\n// 3. Update axis weights via simple averaging or gradient descent\n\nexport const learnAgentProfile = mutation({\n  args: {\n    agentId: v.string(),\n    scopeId: v.id(\"memory_scopes\"),\n    usedFactIds: v.array(v.id(\"facts\")),\n  },\n  handler: async (ctx, args) =\u003e {\n    // Get subspace\n    const subspace = await ctx.db\n      .query(\"knowledge_subspaces\")\n      .withIndex(\"by_scope\", q =\u003e q.eq(\"scopeId\", args.scopeId))\n      .first();\n    \n    if (!subspace) return;\n    \n    // Get used facts' coefficients\n    const facts = await ctx.db\n      .query(\"facts\")\n      .filter(q =\u003e /* factIds in usedFactIds */)\n      .collect();\n    \n    const coefficients = facts.map(f =\u003e f.compactEmbedding);\n    \n    // Compute mean absolute coefficient per axis (simple heuristic)\n    const k = subspace.k;\n    const axisWeights = new Array(k).fill(0);\n    \n    for (let i = 0; i \u003c k; i++) {\n      const axisValues = coefficients.map(c =\u003e Math.abs(c[i]));\n      axisWeights[i] = average(axisValues);\n    }\n    \n    // Normalize to sum=1\n    const sum = axisWeights.reduce((a, b) =\u003e a + b, 0);\n    const normalized = axisWeights.map(w =\u003e w / sum);\n    \n    // Update or create profile\n    const existing = await ctx.db\n      .query(\"agent_knowledge_profiles\")\n      .withIndex(\"by_agent_scope\", q =\u003e \n        q.eq(\"agentId\", args.agentId).eq(\"scopeId\", args.scopeId))\n      .first();\n    \n    if (existing) {\n      // Blend with existing (exponential moving average)\n      const blended = existing.axisWeights.map((old, i) =\u003e \n        0.7 * old + 0.3 * normalized[i]\n      );\n      await ctx.db.patch(existing._id, {\n        axisWeights: blended,\n        learnedFrom: existing.learnedFrom + 1,\n        updatedAt: Date.now(),\n      });\n    } else {\n      await ctx.db.insert(\"agent_knowledge_profiles\", {\n        agentId: args.agentId,\n        scopeId: args.scopeId,\n        subspaceId: subspace._id,\n        axisWeights: normalized,\n        learnedFrom: 1,\n        updatedAt: Date.now(),\n      });\n    }\n  },\n});\n```\n\n## Personalized Retrieval\n```typescript\n// In vector search, apply agent's axis weights:\nconst profile = await ctx.db\n  .query(\"agent_knowledge_profiles\")\n  .withIndex(\"by_agent_scope\", q =\u003e \n    q.eq(\"agentId\", agentId).eq(\"scopeId\", scopeId))\n  .first();\n\nif (profile) {\n  // Weighted similarity in compact space\n  const weightedScore = queryCompact.map((q, i) =\u003e \n    profile.axisWeights[i] * q * fact.compactEmbedding[i]\n  ).reduce((a, b) =\u003e a + b, 0);\n  \n  fact._score = weightedScore;\n}\n```\n\n## Impact\n- **100x parameter reduction** (k weights vs 1024-dim index)\n- **Instant agent onboarding** (learn from 10-20 recalls)\n- **Personalized retrieval** without per-agent indices\n- **Interpretable** (can see which knowledge axes agent prefers)\n\n## Dependencies\n- **Requires**: Subspace consolidation (engram-2vw)\n\n## References\n- SHARE paper section 4.3 (coefficient-only learning)\n- Analysis doc: docs/SHARE-PAPER-ANALYSIS.md (section 4)","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-24T09:33:14.743645+01:00","updated_at":"2026-02-25T11:41:24.188337+01:00","closed_at":"2026-02-25T11:41:24.188337+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-ppv","depends_on_id":"engram-2vw","type":"blocks","created_at":"2026-02-24T09:40:59.740333+01:00","created_by":"daemon"}]}
{"id":"engram-ppv.1","title":"Schema: Add agent_knowledge_profiles table","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-24T09:41:29.311232+01:00","updated_at":"2026-02-25T02:49:20.674146+01:00","closed_at":"2026-02-25T02:49:20.674146+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-ppv.1","depends_on_id":"engram-ppv","type":"parent-child","created_at":"2026-02-24T09:41:29.312574+01:00","created_by":"daemon"}]}
{"id":"engram-ppv.2","title":"Implement axis weight learning","description":"Learn per-agent axis weights from recall usage patterns.\n\n**File**: `convex/subspace/agent-profiles.ts` (new file)\n\n**Algorithm**:\n```typescript\nexport const learnAgentProfile = mutation({\n  args: {\n    agentId: v.string(),\n    scopeId: v.id(\"memory_scopes\"),\n    usedFactIds: v.array(v.id(\"facts\")),\n  },\n  handler: async (ctx, args) =\u003e {\n    const subspace = await ctx.db\n      .query(\"knowledge_subspaces\")\n      .withIndex(\"by_scope\", q =\u003e q.eq(\"scopeId\", args.scopeId))\n      .first();\n    \n    if (!subspace) return { error: \"No subspace\" };\n    \n    // Get used facts' coefficients\n    const facts = await ctx.db\n      .query(\"facts\")\n      .filter(q =\u003e /* factIds in args.usedFactIds */)\n      .collect();\n    \n    const coefficients = facts\n      .filter(f =\u003e f.compactEmbedding)\n      .map(f =\u003e f.compactEmbedding);\n    \n    // Compute mean absolute coefficient per axis\n    const k = subspace.k;\n    const axisWeights = new Array(k).fill(0);\n    \n    for (let i = 0; i \u003c k; i++) {\n      axisWeights[i] = coefficients.reduce((sum, c) =\u003e \n        sum + Math.abs(c[i]), 0) / coefficients.length;\n    }\n    \n    // Normalize to sum=1\n    const sum = axisWeights.reduce((a, b) =\u003e a + b, 0);\n    const normalized = axisWeights.map(w =\u003e w / sum);\n    \n    // Upsert profile with exponential moving average\n    const existing = await ctx.db\n      .query(\"agent_knowledge_profiles\")\n      .withIndex(\"by_agent_scope\", q =\u003e\n        q.eq(\"agentId\", args.agentId).eq(\"scopeId\", args.scopeId))\n      .first();\n    \n    if (existing) {\n      const blended = existing.axisWeights.map((old, i) =\u003e\n        0.7 * old + 0.3 * normalized[i]);\n      await ctx.db.patch(existing._id, {\n        axisWeights: blended,\n        learnedFrom: existing.learnedFrom + 1,\n        updatedAt: Date.now(),\n      });\n    } else {\n      await ctx.db.insert(\"agent_knowledge_profiles\", {\n        agentId: args.agentId,\n        scopeId: args.scopeId,\n        subspaceId: subspace._id,\n        axisWeights: normalized,\n        learnedFrom: 1,\n        updatedAt: Date.now(),\n      });\n    }\n  },\n});\n```","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-24T09:41:29.515097+01:00","updated_at":"2026-02-25T11:41:15.332515+01:00","closed_at":"2026-02-25T11:41:15.332515+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-ppv.2","depends_on_id":"engram-ppv","type":"parent-child","created_at":"2026-02-24T09:41:29.516976+01:00","created_by":"daemon"}]}
{"id":"engram-ppv.3","title":"Personalized retrieval with weights","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-24T09:41:29.71842+01:00","updated_at":"2026-02-25T11:41:15.552603+01:00","closed_at":"2026-02-25T11:41:15.552603+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-ppv.3","depends_on_id":"engram-ppv","type":"parent-child","created_at":"2026-02-24T09:41:29.720196+01:00","created_by":"daemon"}]}
{"id":"engram-ppv.4","title":"Auto-learn from recall_feedback","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-24T09:41:29.924344+01:00","updated_at":"2026-02-25T03:09:27.333335+01:00","closed_at":"2026-02-25T03:09:27.333335+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-ppv.4","depends_on_id":"engram-ppv","type":"parent-child","created_at":"2026-02-24T09:41:29.925395+01:00","created_by":"daemon"}]}
{"id":"engram-ppv.5","title":"Unit tests: Profile learning","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-24T09:41:30.126261+01:00","updated_at":"2026-02-25T03:20:22.850931+01:00","closed_at":"2026-02-25T03:20:22.850931+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-ppv.5","depends_on_id":"engram-ppv","type":"parent-child","created_at":"2026-02-24T09:41:30.127192+01:00","created_by":"daemon"}]}
{"id":"engram-ppv.6","title":"E2E test: Per-agent personalization","status":"closed","priority":3,"issue_type":"task","created_at":"2026-02-24T09:41:30.338958+01:00","updated_at":"2026-02-25T03:24:08.845585+01:00","closed_at":"2026-02-25T03:24:08.845585+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-ppv.6","depends_on_id":"engram-ppv","type":"parent-child","created_at":"2026-02-24T09:41:30.344062+01:00","created_by":"daemon"}]}
{"id":"engram-qcu","title":"Create memory_events table and schema","description":"Add memory_events table (eventType, payload, agentId, scopeId, timestamp, watermark) with indices by_agent_watermark and by_scope_watermark","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-15T00:38:38.080181+01:00","updated_at":"2026-02-15T00:58:48.913882+01:00","closed_at":"2026-02-15T00:58:48.913882+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-qcu","depends_on_id":"engram-c48","type":"blocks","created_at":"2026-02-15T00:39:11.470468+01:00","created_by":"daemon"}]}
{"id":"engram-rdm","title":"Phase 4: QA-Pair Representation (Panini-inspired)","description":"Make facts more retrievable and reasoning-friendly. Add qaRepresentation field (question, answer, entities, confidence). Enrichment pipeline generates QA pairs via LLM. Graph-based retrieval chains via temporalLinks + entity graph. Ref: memory/2026-02-25-letta-context-repos.md Phase 4, Panini paper","status":"closed","priority":1,"issue_type":"feature","created_at":"2026-02-25T14:13:57.671413+01:00","updated_at":"2026-02-25T14:32:56.480801+01:00","closed_at":"2026-02-25T14:32:56.480801+01:00","close_reason":"Phase 4 QA-Pair complete: schema fields, heuristic QA generation, QA-aware recall with RRF fusion, chain recall"}
{"id":"engram-rgd","title":"Performance and caching optimization pass","description":"Execute actionable performance tasks from plan: query index audit, mutation batching where practical, config cache refresh strategy in MCP server, cursor-based pagination for large result sets, and performance regression checks. Include before/after p95 metrics capture for critical paths (recall, get-context, store-fact).","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-15T00:33:57.176222+01:00","updated_at":"2026-02-15T00:50:22.714722+01:00","closed_at":"2026-02-15T00:50:22.714722+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-rgd","depends_on_id":"engram-ne6","type":"blocks","created_at":"2026-02-15T00:34:09.100252+01:00","created_by":"daemon"},{"issue_id":"engram-rgd","depends_on_id":"engram-4rr","type":"blocks","created_at":"2026-02-15T00:34:09.513066+01:00","created_by":"daemon"}]}
{"id":"engram-ri0","title":"Convex mirror integration: actions + facts hooks","description":"Convex-side mirror integration (actions + facts hooks). This is the Convex action layer; the MCP-side reconciliation lib is in engram-8nz.\n\n**New Convex actions:**\n1. convex/actions/mirrorToVault.ts — Signal action triggered after storeFact/updateFact. Sets flag for vault-sync daemon to pick up. Simple implementation: log event, daemon polls.\n2. convex/actions/reconcileFromVault.ts — Handle file edits from vault (called by MCP vault-reconciler in engram-8nz). Three-way merge logic:\n   - Fetch DB fact by ID\n   - Compare updatedAt timestamps\n   - Field-level merge: HUMAN_FIELDS (content, tags, entities, priority, type) vs MACHINE_FIELDS (embedding, importance, decayFactor, synthesizedContext)\n   - If conflict detected: return conflict info for MCP layer to create .conflict file\n   - Update DB with merged values\n   - Return {success, conflicts[]}\n\n**Convex functions/facts.ts changes:**\n- Add mutation updateVaultPath(id, vaultPath) — Update fact.vaultPath after mirror\n- Add query getUnmirrored(limit) — Return facts where vaultPath == null, ordered by timestamp desc\n- Modify storeFact — Call mirrorToVault action after insert (non-blocking)\n\n**Field classification constants:**\n- HUMAN_FIELDS = [\"content\", \"tags\", \"entities\", \"priority\", \"type\"]\n- MACHINE_FIELDS = [\"embedding\", \"importance\", \"decayFactor\", \"synthesizedContext\"]\n- IMMUTABLE_FIELDS = [\"_id\", \"timestamp\", \"createdBy\", \"scopeId\"]\n\n**Tests:** reconcile-e2e.test.ts, conflict-e2e.test.ts\n**Performance:** Reconcile \u003c200ms p95\nRef: specs/obsidian-mirror-plan.md §2.2-2.4","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-14T23:00:27.123529+01:00","updated_at":"2026-02-14T23:24:36.042011+01:00","closed_at":"2026-02-14T23:24:36.042011+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-ri0","depends_on_id":"engram-ywx","type":"blocks","created_at":"2026-02-14T23:01:38.347058+01:00","created_by":"daemon"}]}
{"id":"engram-rq3","title":"Streaming Integration with Residual Novelty Detection","description":"# Streaming Integration with Residual Novelty Detection\n\n## Problem\nAfter initial subspace consolidation (engram-2vw), new facts arrive continuously. Re-running full SVD every time is expensive. Need **incremental update** mechanism.\n\n## Solution (from SHARE Paper)\n**Residual-based novelty detection**: When a new fact arrives:\n1. Project onto existing subspace: e_hat = V_k @ (V_k^T @ e_new)\n2. Compute residual: r = e_new - e_hat\n3. If ||r|| \u003e threshold: **novel knowledge** → expand subspace\n4. If ||r|| \u003c threshold: **known knowledge** → store compact coefficients only\n\n## Algorithm\n```typescript\n// convex/subspace/streaming-integrate.ts\nexport const integrateNewFact = mutation({\n  args: { \n    factId: v.id(\"facts\"),\n    embedding: v.array(v.float64()),\n  },\n  handler: async (ctx, args) =\u003e {\n    // 1. Get scope's current subspace\n    const fact = await ctx.db.get(args.factId);\n    const subspace = await ctx.db\n      .query(\"knowledge_subspaces\")\n      .withIndex(\"by_scope\", q =\u003e q.eq(\"scopeId\", fact.scopeId))\n      .first();\n    \n    if (!subspace) {\n      // No subspace yet, store full embedding\n      await ctx.db.patch(args.factId, { embedding: args.embedding });\n      return { mode: \"full\", novelty: null };\n    }\n    \n    // 2. Project onto subspace\n    const Vk = subspace.principalVectors;\n    const coefficients = matmul(args.embedding, transpose(Vk)); // 1 × k\n    const reconstructed = matmul(coefficients, Vk); // 1 × 1024\n    \n    // 3. Compute residual\n    const residual = subtract(args.embedding, reconstructed);\n    const residualNorm = norm(residual);\n    \n    // 4. Novelty threshold (from paper: 60% explained variance → 40% residual OK)\n    const threshold = 0.4;\n    \n    if (residualNorm \u003e threshold) {\n      // Novel knowledge: expand subspace\n      console.log(`[streaming] Novel fact detected: |r|=${residualNorm.toFixed(3)}`);\n      \n      // Expand: add residual as new principal direction\n      const newDirection = normalize(residual);\n      await ctx.db.patch(subspace._id, {\n        principalVectors: [...Vk, newDirection],\n        k: subspace.k + 1,\n        version: subspace.version + 1,\n      });\n      \n      // Recompute coefficients with expanded subspace\n      const newCoefficients = matmul(args.embedding, transpose([...Vk, newDirection]));\n      await ctx.db.patch(args.factId, {\n        compactEmbedding: newCoefficients,\n        embedding: null, // Drop full embedding\n      });\n      \n      return { mode: \"expand\", novelty: residualNorm, newK: subspace.k + 1 };\n      \n    } else {\n      // Known knowledge: store compact only\n      await ctx.db.patch(args.factId, {\n        compactEmbedding: coefficients,\n        embedding: null, // Drop full embedding\n      });\n      \n      return { mode: \"compact\", novelty: residualNorm };\n    }\n  },\n});\n```\n\n## Periodic Re-Merge (Consolidation)\n```typescript\n// After k grows too large (e.g., k \u003e 128), re-run SVD to compress:\n// 1. Reconstruct all embeddings from coefficients\n// 2. Run SVD on reconstructed matrix\n// 3. Keep top-k2 (e.g., k2 = 64)\n// 4. Update all coefficients\n\n// This keeps k bounded while preserving knowledge\n```\n\n## Integration with store_fact\n```typescript\n// In store_fact.ts, after embedding:\nconst embedding = await embedContent(content);\n\n// Instead of storing full embedding:\nconst integration = await ctx.runMutation(internal.subspace.integrateNewFact, {\n  factId,\n  embedding,\n});\n\nconsole.log(`[store-fact] Integrated with mode=${integration.mode}, novelty=${integration.novelty}`);\n```\n\n## Impact\n- **No full SVD re-runs** for new facts (incremental)\n- **Automatic knowledge expansion** (subspace grows with diversity)\n- **Bounded complexity** (periodic re-merge keeps k \u003c 128)\n- **Real-time novelty detection** (which facts are truly new?)\n\n## Dependencies\n- **Requires**: Subspace consolidation (engram-2vw)\n\n## References\n- SHARE paper section 3.2 (incremental subspace update)\n- Analysis doc: docs/SHARE-PAPER-ANALYSIS.md (section 2)","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-24T09:33:14.4946+01:00","updated_at":"2026-02-25T11:41:24.383078+01:00","closed_at":"2026-02-25T11:41:24.383078+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-rq3","depends_on_id":"engram-2vw","type":"blocks","created_at":"2026-02-24T09:40:07.585126+01:00","created_by":"daemon"}]}
{"id":"engram-rq3.1","title":"Implement residual novelty detection","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-24T09:41:28.115716+01:00","updated_at":"2026-02-25T11:41:16.60736+01:00","closed_at":"2026-02-25T11:41:16.60736+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-rq3.1","depends_on_id":"engram-rq3","type":"parent-child","created_at":"2026-02-24T09:41:28.116931+01:00","created_by":"daemon"}]}
{"id":"engram-rq3.2","title":"Subspace expansion on novel facts","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-24T09:41:28.31967+01:00","updated_at":"2026-02-25T11:41:16.804195+01:00","closed_at":"2026-02-25T11:41:16.804195+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-rq3.2","depends_on_id":"engram-rq3","type":"parent-child","created_at":"2026-02-24T09:41:28.320712+01:00","created_by":"daemon"}]}
{"id":"engram-rq3.3","title":"Periodic re-merge cron","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-24T09:41:28.51899+01:00","updated_at":"2026-02-25T03:11:29.840162+01:00","closed_at":"2026-02-25T03:11:29.840162+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-rq3.3","depends_on_id":"engram-rq3","type":"parent-child","created_at":"2026-02-24T09:41:28.520484+01:00","created_by":"daemon"}]}
{"id":"engram-rq3.4","title":"Integrate with store_fact","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-24T09:41:28.718072+01:00","updated_at":"2026-02-25T11:41:17.001505+01:00","closed_at":"2026-02-25T11:41:17.001505+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-rq3.4","depends_on_id":"engram-rq3","type":"parent-child","created_at":"2026-02-24T09:41:28.719377+01:00","created_by":"daemon"}]}
{"id":"engram-rq3.5","title":"Unit tests: Novelty detection","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-24T09:41:28.910783+01:00","updated_at":"2026-02-25T03:12:07.822511+01:00","closed_at":"2026-02-25T03:12:07.822511+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-rq3.5","depends_on_id":"engram-rq3","type":"parent-child","created_at":"2026-02-24T09:41:28.912238+01:00","created_by":"daemon"}]}
{"id":"engram-rq3.6","title":"E2E test: Streaming integration","status":"closed","priority":3,"issue_type":"task","created_at":"2026-02-24T09:41:29.094594+01:00","updated_at":"2026-02-25T03:25:48.456434+01:00","closed_at":"2026-02-25T03:25:48.456434+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-rq3.6","depends_on_id":"engram-rq3","type":"parent-child","created_at":"2026-02-24T09:41:29.095598+01:00","created_by":"daemon"}]}
{"id":"engram-rqm","title":"Schema: Add fact_versions table with indexes","description":"## Background\nVersion history enables non-destructive mutations with rollback. Every fact mutation\ncreates a version snapshot before changing, like git commits.\n\n## Technical Approach\nAdd new table to `convex/schema.ts`:\n```typescript\nfact_versions: defineTable({\n  factId: v.id('facts'),\n  version: v.number(),\n  previousContent: v.string(),\n  previousMetadata: v.optional(v.string()),\n  changedBy: v.string(),\n  changedAt: v.number(),\n  changeType: v.string(), // update|merge|archive|pin|unpin\n  reason: v.string(),\n}).index('by_fact', ['factId', 'version'])\n  .index('by_agent', ['changedBy', 'changedAt'])\n```\n\n## Files to Edit\n- `convex/schema.ts` — add fact_versions table\n\n## Success Criteria\n- Schema deploys cleanly\n- Table accessible from Convex functions\n- See PLAN-CONTEXT-REPOS.md Phase 3.1","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-25T14:04:57.054759+01:00","updated_at":"2026-02-25T14:20:16.279214+01:00","closed_at":"2026-02-25T14:20:16.279214+01:00","close_reason":"Closed"}
{"id":"engram-rr7","title":"Episodic Memory / Episodes Table","description":"# Episodic Memory / Episodes Table\n\n## Problem\nEngram has `temporalLinks` on facts but no **episode abstraction**. Facts are isolated atoms without session-level narrative:\n- Cannot answer \"what happened last Tuesday?\"\n- No temporal grouping of related facts\n- Missing session-level summaries\n- No episode-granularity retrieval\n\n## Solution (from Zep)\nAdd **episodes table** that groups facts within time windows into coherent narratives:\n1. Auto-create episodes from observation sessions\n2. Summarize episode content for coarse retrieval\n3. Retrieve at episode level first, then drill into constituent facts\n4. Support temporal queries (\"last week\", \"this morning\")\n\n## Schema Addition\n```typescript\n// Add to convex/schema.ts\nepisodes: defineTable({\n  scopeId: v.id(\"memory_scopes\"),\n  agentId: v.string(),\n  startTime: v.number(),\n  endTime: v.number(),\n  factIds: v.array(v.id(\"facts\")),\n  summary: v.string(),              // LLM-generated episode summary\n  embedding: v.optional(v.array(v.float64())), // Summary embedding for search\n  importance: v.float64(),\n  tags: v.array(v.string()),\n  episodeType: v.optional(v.string()), // session|event|period\n  context: v.optional(v.string()),   // Additional context\n}).index(\"by_scope_time\", [\"scopeId\", \"startTime\"])\n  .index(\"by_agent\", [\"agentId\", \"startTime\"])\n  .index(\"by_importance\", [\"importance\"])\n  .vectorIndex(\"episode_search\", {\n    vectorField: \"embedding\",\n    dimensions: 1024,\n    filterFields: [\"scopeId\", \"agentId\"],\n  })\n```\n\n## Episode Creation Logic\n```typescript\n// Auto-create from observation_sessions\n// When Observer/Reflector runs:\n1. Gather all facts in time window (e.g., 1 hour, 1 day)\n2. Generate summary via LLM (\"During this period, the agent...\")\n3. Embed summary\n4. Store episode with factIds\n\n// Manual creation via MCP tool\nmemory_create_episode({\n  scopeId,\n  startTime,\n  endTime,\n  summary, // optional, auto-generate if missing\n})\n```\n\n## Episode-Based Recall\n```typescript\n// For temporal queries:\n1. Search episodes by time range + semantic similarity\n2. Return episode summaries first\n3. If user wants details, return constituent factIds\n4. Rank episodes by importance × recency × relevance\n\n// Example:\nQuery: \"What happened last Tuesday?\"\n→ Find episodes with startTime in Tuesday's range\n→ Return top 3 episodes by importance\n→ Each episode includes 5-20 constituent facts\n```\n\n## Impact\n- Enables temporal queries (\"last week\", \"yesterday morning\")\n- Reduces retrieval noise (match at episode level, not fact level)\n- Preserves narrative flow (related facts grouped together)\n- Supports session replay (\"show me what happened in that conversation\")\n\n## Integration with Observation Pipeline\n- Episodes created automatically when Observer compresses background observations\n- Episode summary = compressed representation of observation session\n- Replaces storing 100 individual observations with 1 episode + 5-10 key facts\n\n## References\n- Zep temporal grouping architecture\n- Current observation_sessions: convex/schema.ts (line ~350)\n- Optimization doc: docs/OPTIMIZATION-2026-02-24.md (section 2)","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-24T09:33:13.309723+01:00","updated_at":"2026-02-25T03:17:00.604945+01:00","closed_at":"2026-02-25T03:17:00.604945+01:00","close_reason":"Closed"}
{"id":"engram-rr7.1","title":"Schema: Add episodes table","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-24T09:37:48.188084+01:00","updated_at":"2026-02-25T01:42:07.943144+01:00","closed_at":"2026-02-25T01:42:07.943144+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-rr7.1","depends_on_id":"engram-rr7","type":"parent-child","created_at":"2026-02-24T09:37:48.192906+01:00","created_by":"daemon"}]}
{"id":"engram-rr7.2","title":"Convex: Episode creation mutations","description":"Create Convex mutations for episode management.\n\n**File**: `convex/episodes.ts` (new file)\n\n**Mutations**:\n```typescript\nexport const createEpisode = mutation({\n  args: {\n    scopeId: v.id(\"memory_scopes\"),\n    agentId: v.string(),\n    startTime: v.number(),\n    endTime: v.number(),\n    factIds: v.array(v.id(\"facts\")),\n    summary: v.optional(v.string()),\n  },\n  handler: async (ctx, args) =\u003e {\n    // Auto-generate summary if not provided (TODO: call LLM)\n    const summary = args.summary || \"Episode during \" + new Date(args.startTime).toISOString();\n    \n    const episodeId = await ctx.db.insert(\"episodes\", {\n      scopeId: args.scopeId,\n      agentId: args.agentId,\n      startTime: args.startTime,\n      endTime: args.endTime,\n      factIds: args.factIds,\n      summary,\n      importance: computeImportance(args.factIds),\n      tags: [],\n    });\n    \n    return { episodeId, summary };\n  },\n});\n\nexport const queryEpisodes = query({\n  args: {\n    scopeId: v.id(\"memory_scopes\"),\n    startTime: v.number(),\n    endTime: v.number(),\n  },\n  handler: async (ctx, args) =\u003e {\n    return await ctx.db\n      .query(\"episodes\")\n      .withIndex(\"by_scope_time\", q =\u003e\n        q.eq(\"scopeId\", args.scopeId).gte(\"startTime\", args.startTime))\n      .filter(q =\u003e q.lte(q.field(\"endTime\"), args.endTime))\n      .order(\"desc\")\n      .collect();\n  },\n});\n```","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-24T09:37:48.407306+01:00","updated_at":"2026-02-25T02:49:23.729028+01:00","closed_at":"2026-02-25T02:49:23.729028+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-rr7.2","depends_on_id":"engram-rr7","type":"parent-child","created_at":"2026-02-24T09:37:48.409091+01:00","created_by":"daemon"}]}
{"id":"engram-rr7.3","title":"Auto-create episodes from observation_sessions","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-24T09:37:48.670926+01:00","updated_at":"2026-02-25T02:49:24.132409+01:00","closed_at":"2026-02-25T02:49:24.132409+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-rr7.3","depends_on_id":"engram-rr7","type":"parent-child","created_at":"2026-02-24T09:37:48.671905+01:00","created_by":"daemon"}]}
{"id":"engram-rr7.4","title":"MCP Tool: memory_create_episode","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-24T09:37:48.942876+01:00","updated_at":"2026-02-25T02:56:03.365143+01:00","closed_at":"2026-02-25T02:56:03.365143+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-rr7.4","depends_on_id":"engram-rr7","type":"parent-child","created_at":"2026-02-24T09:37:48.944193+01:00","created_by":"daemon"}]}
{"id":"engram-rr7.5","title":"MCP Tool: memory_recall_episodes (temporal queries)","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-24T09:37:49.232046+01:00","updated_at":"2026-02-25T02:56:03.760528+01:00","closed_at":"2026-02-25T02:56:03.760528+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-rr7.5","depends_on_id":"engram-rr7","type":"parent-child","created_at":"2026-02-24T09:37:49.23275+01:00","created_by":"daemon"}]}
{"id":"engram-rr7.6","title":"Unit tests: Episode creation and retrieval","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-24T09:37:49.613669+01:00","updated_at":"2026-02-25T03:16:14.144225+01:00","closed_at":"2026-02-25T03:16:14.144225+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-rr7.6","depends_on_id":"engram-rr7","type":"parent-child","created_at":"2026-02-24T09:37:49.614652+01:00","created_by":"daemon"}]}
{"id":"engram-rr7.7","title":"E2E test: Temporal episode queries","status":"closed","priority":3,"issue_type":"task","created_at":"2026-02-24T09:37:50.376537+01:00","updated_at":"2026-02-25T03:16:14.158477+01:00","closed_at":"2026-02-25T03:16:14.158477+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-rr7.7","depends_on_id":"engram-rr7","type":"parent-child","created_at":"2026-02-24T09:37:50.38939+01:00","created_by":"daemon"}]}
{"id":"engram-s38","title":"retroactiveReproject agentId attributed to system instead of subspace owner","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-02-25T11:43:36.870458+01:00","updated_at":"2026-02-25T14:20:59.940267+01:00","closed_at":"2026-02-25T14:20:59.940267+01:00","close_reason":"Closed"}
{"id":"engram-sll","title":"Context-budgeted recall with profiles","description":"Context-budgeted recall with explainability:\n\n**New file: mcp-server/src/lib/budget-aware-loader.ts**\n\n1. loadBudgetAwareContext(query, budget, convex) returns LoadedContext with facts, entities, themes, tokenUsage, and explainability array\n   - Budget allocation tiers: Critical 40%, Notable 40%, Background 10%, Entities 10%\n   - Fetch candidates via semantic search (limit 100, over-fetch then filter)\n   - Sort by: priority ASC then importanceScore DESC then timestamp DESC (deterministic)\n   - Fill tiers until budget exhausted\n   - Token estimation: chars/4 (rough estimate)\n\n2. detectQueryIntent(query) returns BudgetConfig based on keywords:\n   - decision/commit queries: 80% Critical, 20% Notable\n   - observation/recent queries: 10% Critical, 30% Notable, 60% Background\n   - Default: 40/40/10/10\n\n3. getInclusionReason(fact) explains why fact was loaded:\n   - Priority 0: Critical decision/commitment\n   - importanceScore \u003e0.8: High importance\n   - Created \u003c24h: Recent creation\n   - Default: Semantic relevance\n\n**Context profiles (from spec §5.1):**\n- Add profile parameter to memory_get_context: \"default\"|\"planning\"|\"incident\"|\"handoff\"\n- Each profile has different source ordering (structural, daily, search, graph, potential, contextual)\n\n**MCP tool enhancement (mcp-server/src/tools/get-context.ts):**\n- Add tokenBudget parameter (default 4000 from spec §5.2)\n- Add profile parameter (default \"default\")\n- Replace simple loader with loadBudgetAwareContext\n- Return facts, entities, themes, tokenUsage by tier, explainability array, truncated flag\n\n**Tests:** budget-aware-context-e2e.test.ts, budget-aware-latency.test.ts (\u003c200ms)\n**Performance:** Context load \u003c200ms, token efficiency 30% better, overflow rate \u003c2%\nRef: specs/obsidian-mirror-plan.md §5.1-5.3","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-14T22:56:50.889612+01:00","updated_at":"2026-02-14T23:25:46.767587+01:00","closed_at":"2026-02-14T23:25:46.767587+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-sll","depends_on_id":"engram-doo","type":"blocks","created_at":"2026-02-14T22:57:21.461965+01:00","created_by":"daemon"},{"issue_id":"engram-sll","depends_on_id":"engram-43e","type":"blocks","created_at":"2026-02-14T22:57:22.260754+01:00","created_by":"daemon"}]}
{"id":"engram-srt","title":"Intent-Aware Retrieval Routing","description":"# Intent-Aware Retrieval Routing\n\n## Problem\nCurrent recall treats all queries the same, using hybrid text+vector search regardless of query intent. This is inefficient:\n- \"What is Ryan's email?\" (lookup) should use KV store, not vector search\n- \"Tell me about the briefly project\" (explore) should use hierarchical traversal\n- \"What happened last Tuesday?\" (temporal) should prioritize episode search + recency\n- \"How is React related to TypeScript?\" (relational) should use graph traversal\n\n## Solution (from Mem0)\nAdd **query intent classification** at recall entry point, then route to appropriate retrieval strategy:\n1. **lookup**: Exact entity/fact retrieval → KV store + entity search\n2. **explore**: Open-ended semantic → vector + hierarchical traversal\n3. **temporal**: Time-based → episode search + recency weighting\n4. **relational**: About connections → graph traversal via entities\n\n## Classification Logic\n```typescript\nfunction classifyIntent(query: string): \"lookup\" | \"explore\" | \"temporal\" | \"relational\" {\n  // Regex + keyword based (cheap, runs in \u003c1ms)\n  \n  if (/^(what is|who is|what does|show me|get|preference|rule|email|phone)/i.test(query)) {\n    return \"lookup\";\n  }\n  \n  if (/\\b(when|last|yesterday|today|this week|timeline|recent|history|ago)\\b/i.test(query)) {\n    return \"temporal\";\n  }\n  \n  if (/\\b(related|connected|depends|works with|between|relationship|links|associated)\\b/i.test(query)) {\n    return \"relational\";\n  }\n  \n  return \"explore\"; // Default: open-ended semantic search\n}\n```\n\n## Routing Implementation\n```typescript\n// In recall.ts:\nconst intent = classifyIntent(input.query);\n\nswitch (intent) {\n  case \"lookup\":\n    // 1. Try KV store first (deterministic facts)\n    const kvResults = await kvLookup(ctx, query, scopeIds);\n    if (kvResults.length \u003e 0) return { facts: kvResults, intent };\n    \n    // 2. Fall back to entity search\n    const entities = await entitySearch({ query, limit: 5 });\n    const facts = await getEntityBacklinks(entities[0]);\n    return { facts, intent };\n    \n  case \"temporal\":\n    // 1. Episode search (when implemented)\n    // 2. Text search with recency boost\n    const recencyWeighted = await textSearch({ query, limit, scopeIds });\n    recencyWeighted.sort((a, b) =\u003e b.timestamp - a.timestamp);\n    return { facts: recencyWeighted, intent };\n    \n  case \"relational\":\n    // Use hierarchical recall (graph traversal)\n    return await hierarchicalRecall({ query, scopeId, limit }, agentId);\n    \n  case \"explore\":\n  default:\n    // Hybrid vector + text (current implementation)\n    return await hybridRecall({ query, scopeId, limit }, agentId);\n}\n```\n\n## Impact\n- 20-30% relevance improvement by matching query type to retrieval strategy\n- Faster lookups (KV vs vector search)\n- Better temporal queries (recency weighting)\n- Reduced vector DB load for non-semantic queries\n\n## Dependencies\n- **Requires**: KV store (engram-naa) for lookup intent\n- **Optional**: Episodes table (engram-rr7) for better temporal queries\n\n## References\n- Mem0 intent-aware retrieval architecture\n- Current recall: mcp-server/src/tools/recall.ts\n- Hierarchical recall: mcp-server/src/tools/hierarchical-recall.ts\n- Optimization doc: docs/OPTIMIZATION-2026-02-24.md (section 3)","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-24T09:33:13.088404+01:00","updated_at":"2026-02-25T02:49:21.850667+01:00","closed_at":"2026-02-25T02:49:21.850667+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-srt","depends_on_id":"engram-naa","type":"blocks","created_at":"2026-02-24T09:37:04.711826+01:00","created_by":"daemon"}]}
{"id":"engram-srt.1","title":"Implement query intent classifier","description":"Implement query intent classifier for routing.\n\n**File**: `mcp-server/src/lib/intent-classifier.ts` (new file)\n\n**Implementation**:\n```typescript\nexport type QueryIntent = \"lookup\" | \"explore\" | \"temporal\" | \"relational\";\n\nexport function classifyIntent(query: string): QueryIntent {\n  // Lookup: What is/Who is/Show me/Get\n  if (/^(what is|who is|what does|show me|get|preference|rule|email|phone)/i.test(query)) {\n    return \"lookup\";\n  }\n  \n  // Temporal: When/Last/Yesterday/Timeline\n  if (/\\b(when|last|yesterday|today|this week|timeline|recent|history|ago|duration)\\b/i.test(query)) {\n    return \"temporal\";\n  }\n  \n  // Relational: Connections/Relationships\n  if (/\\b(related|connected|depends|works with|between|relationship|links|associated|part of)\\b/i.test(query)) {\n    return \"relational\";\n  }\n  \n  return \"explore\"; // Default: open-ended semantic\n}\n```\n\n**Usage in recall.ts**:\n```typescript\nconst intent = classifyIntent(input.query);\nconsole.log(`[recall] Query intent: ${intent}`);\n// Route to appropriate strategy based on intent\n```","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-24T09:37:13.400994+01:00","updated_at":"2026-02-25T02:49:21.095613+01:00","closed_at":"2026-02-25T02:49:21.095613+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-srt.1","depends_on_id":"engram-srt","type":"parent-child","created_at":"2026-02-24T09:37:13.405824+01:00","created_by":"daemon"}]}
{"id":"engram-srt.2","title":"Route to appropriate retrieval strategy","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-24T09:37:13.647975+01:00","updated_at":"2026-02-25T02:49:21.476347+01:00","closed_at":"2026-02-25T02:49:21.476347+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-srt.2","depends_on_id":"engram-srt","type":"parent-child","created_at":"2026-02-24T09:37:13.648711+01:00","created_by":"daemon"}]}
{"id":"engram-srt.3","title":"Unit tests for intent classification","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-24T09:37:13.880394+01:00","updated_at":"2026-02-25T03:09:58.19712+01:00","closed_at":"2026-02-25T03:09:58.19712+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-srt.3","depends_on_id":"engram-srt","type":"parent-child","created_at":"2026-02-24T09:37:13.881531+01:00","created_by":"daemon"}]}
{"id":"engram-srt.4","title":"E2E test for intent-based routing","status":"closed","priority":3,"issue_type":"task","created_at":"2026-02-24T09:37:14.101531+01:00","updated_at":"2026-02-25T03:19:24.46236+01:00","closed_at":"2026-02-25T03:19:24.46236+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-srt.4","depends_on_id":"engram-srt","type":"parent-child","created_at":"2026-02-24T09:37:14.102673+01:00","created_by":"daemon"}]}
{"id":"engram-svy","title":"E2E test: Progressive disclosure pipeline","description":"Full pipeline test: create facts with various pinned states → get_memory_manifest returns correct tiers → build_system_prompt uses manifest → pin/unpin changes disclosure.","status":"open","priority":3,"issue_type":"task","created_at":"2026-02-25T14:14:24.131912+01:00","updated_at":"2026-02-25T14:14:24.131912+01:00"}
{"id":"engram-sws","title":"Phase 2: Sleep-Time Reflection Agent","status":"closed","priority":0,"issue_type":"epic","created_at":"2026-02-25T14:04:55.727336+01:00","updated_at":"2026-02-25T14:32:52.710447+01:00","closed_at":"2026-02-25T14:32:52.710447+01:00","close_reason":"Phase 2 Sleep-Time Reflection complete: reflection cron (4h), enhanced reflect tool (depth/timeWindow), defrag cron (weekly)"}
{"id":"engram-tkk","title":"Implement reflection cron job (4-6h schedule)","description":"Create convex/crons/reflection.ts cron that runs every 4-6 hours. Reads recent memory_events and facts. Extracts new preferences, corrections, patterns. Stores consolidated findings as new facts with type=reflection.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-25T14:14:32.482264+01:00","updated_at":"2026-02-25T14:17:26.818564+01:00","closed_at":"2026-02-25T14:17:26.818564+01:00","close_reason":"Duplicate of P1 structured set (engram-adw, engram-sws, engram-2kn, engram-adj, engram-aeb, engram-2s3) which has proper dependency chains"}
{"id":"engram-tsn","title":"Update enrichment to emit events","description":"Modify convex/functions/facts.ts storeFact, convex/actions/enrich.ts enrichFact to insert memory_events. Add getNextWatermark helper for monotonic sequence","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-15T00:38:43.824825+01:00","updated_at":"2026-02-15T00:58:50.057884+01:00","closed_at":"2026-02-15T00:58:50.057884+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-tsn","depends_on_id":"engram-qcu","type":"blocks","created_at":"2026-02-15T00:39:13.391888+01:00","created_by":"daemon"}]}
{"id":"engram-u51","title":"Engram Phase 1: Foundation — Convex Backend Setup","description":"# Engram Phase 1: Foundation\n\n## Background\nEngram is a unified multi-agent memory system for OpenClaw agents. It provides a shared memory layer where agents store atomic facts, recall context via semantic search, and share knowledge across devices and sessions. Local-first via LanceDB, cloud-synced through Convex, multimodal embeddings via Cohere Embed 4.\n\n## What This Epic Covers\nPhase 1 establishes the foundational data layer in Convex that every subsequent phase builds on:\n- Initialize Convex project\n- Deploy full 10-table schema (all optional future-phase fields included)\n- Implement CRUD mutations/queries for all 10 tables\n- Full-text search on facts with scope/type/agent filtering\n- Scope-based write permission enforcement on storeFact\n- Seed script to populate initial entities and default scopes\n\n## Why Full Schema From Day 1\nThe repo-research agent identified 17 schema discrepancies between PLAN.md (full) and the detailed plan (simplified). Decision: deploy PLAN.md's full schema with all future-phase fields as v.optional(). Convex schema changes require migrations; adding fields later is friction. Optional fields cost nothing until populated. Deploy once, never migrate schema.\n\n## Key Technical Decisions (Locked)\n- Backend: Convex (realtime, native vector search, scheduled functions, free tier)\n- Embeddings: Cohere Embed 4 (1024-dim, multimodal) — NOT OpenAI\n- MCP SDK: @modelcontextprotocol/sdk v1.x (Phase 2, not Phase 1)\n- Access control: Scope-based (NOT per-fact ACLs)\n- Memory lifecycle: 5-state machine (active → dormant → merged → archived → pruned)\n- Decay: Differential by fact type + emotional weight\n- Use lifecycleState (5 states) NOT status (3 states)\n\n## Phase 1 Scope Boundary\nIN SCOPE: Convex project init, schema, CRUD, full-text search, seed script, write permissions\nOUT OF SCOPE: MCP server (Phase 2), embeddings/enrichment (Phase 3), multi-agent (Phase 4), LanceDB sync (Phase 5), migration (Phase 6)\n\n## Success Criteria\n- npx convex dev runs successfully with all 10 tables visible in dashboard\n- Can insert a fact via storeFact mutation with scope write permission check\n- Full-text search on facts.content returns results filtered by scopeId, factType, createdBy\n- All 10 tables have basic CRUD operations\n- Seed script populates initial entities and default scopes\n- Write to scope with writePolicy: \"members\" fails for non-members\n- TypeScript strict mode, no any types in function args/returns\n\n## References\n- PLAN.md — Full schema (lines 89-289), Phase 1 checklist (lines 490-496)\n- docs/plans/2026-02-11-feat-engram-phase1-foundation-plan.md — Detailed Phase 1 plan\n- docs/research/tech-stack-best-practices.md — Convex patterns (1297 lines)\n- docs/INSTITUTIONAL_LEARNINGS.md — 8 critical implementation patterns","status":"closed","priority":0,"issue_type":"epic","created_at":"2026-02-11T22:03:49.903178+01:00","updated_at":"2026-02-12T09:42:34.927436+01:00","closed_at":"2026-02-12T09:42:34.927436+01:00","close_reason":"Closed"}
{"id":"engram-u51.1","title":"Initialize Convex project","description":"# Initialize Convex Project\n\n## What\nRun npx create-convex in the Engram repo root to set up the Convex backend project. This creates the convex/ directory structure that all subsequent tasks depend on.\n\n## Why\nConvex is the cloud backend for Engram. It provides the schema DSL (defineSchema, defineTable, v validators), the function runtime (query, mutation, action), native vector search, scheduled functions (crons), and the ConvexHttpClient for external access. Without this initialization, no other Phase 1 work can proceed.\n\n## Steps\n1. Run npx create-convex in /Volumes/Main SSD/Developer/engram\n2. Select \"create a new project\" when prompted\n3. Verify convex/ directory created with:\n   - _generated/ (auto-generated types — commit per Convex best practices)\n   - tsconfig.json (Convex-specific TypeScript config)\n4. Verify root package.json updated with convex dependency (^1.17.0)\n5. Run npx convex dev to confirm project compiles with empty schema\n6. Update .gitignore if needed (Convex may add entries; convex/_generated/ should be committed)\n\n## Technical Notes\n- The current package.json only has beautiful-mermaid as dependency\n- Node.js 22+ and TypeScript 5.7+ are prerequisites\n- Convex free tier is sufficient for development\n- Environment variable CONVEX_URL will be obtained from this step\n\n## Acceptance Criteria\n- [ ] convex/ directory exists with _generated/ and tsconfig.json\n- [ ] package.json has convex ^1.17.0 in dependencies\n- [ ] npx convex dev compiles without errors (empty schema OK)\n- [ ] CONVEX_URL environment variable available (from .env.local or Convex dashboard)\n\n## Gotchas\n- npx create-convex may prompt for auth — need Convex account (free tier)\n- The generated convex/_generated/ directory SHOULD be committed (Convex convention)\n- Existing .gitignore already has .env and .env.local entries (good)\n\n## Estimate\n15 minutes","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-11T22:04:07.072193+01:00","updated_at":"2026-02-11T22:25:05.646734+01:00","closed_at":"2026-02-11T22:25:05.646755+01:00","dependencies":[{"issue_id":"engram-u51.1","depends_on_id":"engram-u51","type":"parent-child","created_at":"2026-02-11T22:04:07.074429+01:00","created_by":"Ryan Lisse"}]}
{"id":"engram-u51.10","title":"Implement themes CRUD for hierarchical memory","description":"# Implement Themes CRUD\n\n## What\nCreate convex/functions/themes.ts for the themes table. Themes are thematic clusters of related facts — the EverMemOS MemScenes pattern for hierarchical memory.\n\n## Why\nThemes provide a higher-level view of memory. Instead of searching through hundreds of individual facts, agents can retrieve theme summaries. The weekly consolidation cron (Phase 6) will automatically group related facts into themes. Themes have their own embeddings for vector search.\n\n## Schema (from PLAN.md)\n- name: string\n- description: string\n- factIds: Id\u003c\"facts\"\u003e[] (facts in this theme)\n- entityIds: Id\u003c\"entities\"\u003e[] (related entities)\n- scopeId: Id\u003c\"memory_scopes\"\u003e\n- importance: float64\n- lastUpdated: number\n- embedding: float64[]? (1024-dim, for vector search on themes)\n\n## Functions to Implement\n\n### Mutations\n1. create(name, description, factIds, entityIds, scopeId, importance?)\n2. update(themeId, description?, factIds?, entityIds?, importance?)\n   - Update lastUpdated to Date.now()\n3. addFact(themeId, factId) — append to factIds (convenience)\n\n### Queries\n4. get(themeId)\n5. getByScope(scopeId, limit?) — using by_scope index\n6. searchThemes(query) — could use a searchIndex if added, or filter by name\n\n## Acceptance Criteria\n- [ ] Can create themes linked to scopes, facts, and entities\n- [ ] Can update theme description and add facts\n- [ ] getByScope returns themes for a given scope\n\n## Estimate\n20 minutes","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-11T22:06:52.954648+01:00","updated_at":"2026-02-11T22:25:07.618047+01:00","closed_at":"2026-02-11T22:25:07.61805+01:00","dependencies":[{"issue_id":"engram-u51.10","depends_on_id":"engram-u51","type":"parent-child","created_at":"2026-02-11T22:06:52.955901+01:00","created_by":"Ryan Lisse"},{"issue_id":"engram-u51.10","depends_on_id":"engram-u51.2","type":"blocks","created_at":"2026-02-11T22:06:52.957741+01:00","created_by":"Ryan Lisse"}]}
{"id":"engram-u51.11","title":"Implement sync_log CRUD for LanceDB sync tracking","description":"# Implement Sync Log CRUD\n\n## What\nCreate convex/functions/sync.ts for the sync_log table. Tracks per-node LanceDB sync status for Phase 5.\n\n## Why\nEach device (Mac Mini, MacBook Air, MacBook Pro) runs its own LanceDB instance. The sync_log tracks what each node has synced, enabling differential sync (only pull facts since last sync). Phase 1 creates the CRUD; Phase 5 will use it for the actual sync daemon.\n\n## Schema (from PLAN.md)\n- nodeId: string — device identifier\n- lastSyncTimestamp: number — when this node last synced\n- factsSynced: number — total facts synced to this node\n- status: string — ok|error|syncing\n\n## Functions to Implement\n\n### Mutations\n1. updateSyncLog(nodeId, lastSyncTimestamp, factsSynced, status)\n   - Upsert by nodeId (create or update)\n\n### Queries\n2. getSyncStatus(nodeId) — using by_node index\n3. getFactsSince(timestamp, scopeId?) — internalQuery\n   - Return facts created/updated after timestamp\n   - Filter by scope if provided\n   - Used by sync daemon in Phase 5\n\n## Acceptance Criteria\n- [ ] Can create/update sync log entries\n- [ ] getSyncStatus returns correct status for a node\n- [ ] getFactsSince returns facts after a given timestamp\n\n## Estimate\n15 minutes","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-11T22:07:04.289418+01:00","updated_at":"2026-02-11T22:25:07.810836+01:00","closed_at":"2026-02-11T22:25:07.810844+01:00","dependencies":[{"issue_id":"engram-u51.11","depends_on_id":"engram-u51","type":"parent-child","created_at":"2026-02-11T22:07:04.290514+01:00","created_by":"Ryan Lisse"},{"issue_id":"engram-u51.11","depends_on_id":"engram-u51.2","type":"blocks","created_at":"2026-02-11T22:07:04.29189+01:00","created_by":"Ryan Lisse"}]}
{"id":"engram-u51.12","title":"Write seed script to populate initial entities and scopes","description":"# Write Seed Script\n\n## What\nCreate scripts/seed.ts to populate the Convex database with initial entities, default scopes, and sample facts. Uses ConvexHttpClient to call mutations from outside the Convex runtime.\n\n## Why\nThe seed script bootstraps Engram with a useful starting state:\n- Default scopes (global, private-indy) for immediate use\n- Core entities (Ryan, Indy, OpenClaw, Engram, Convex, LanceDB, Cohere) with relationships\n- Sample facts to verify full-text search and CRUD work end-to-end\n- Provides a repeatable setup for development and testing\n\n## Seed Data\n\n### Default Scopes\n1. global — readPolicy: \"all\", writePolicy: \"all\", description: \"Shared knowledge base accessible to all agents\"\n2. private-indy — readPolicy: \"members\", writePolicy: \"members\", members: [\"indy\"], description: \"Indy's private memory space\"\n\n### Initial Entities\n| entityId | name | type | relationships |\n|----------|------|------|---------------|\n| entity-ryan | Ryan | person | created_by: entity-openclaw, works_with: entity-indy |\n| entity-indy | Indy | person | works_with: entity-ryan, part_of: entity-openclaw |\n| entity-openclaw | OpenClaw | project | created_by: entity-ryan |\n| entity-engram | Engram | project | part_of: entity-openclaw, depends_on: entity-convex, depends_on: entity-lancedb |\n| entity-convex | Convex | tool | related_to: entity-engram |\n| entity-lancedb | LanceDB | tool | related_to: entity-engram |\n| entity-cohere | Cohere | tool | related_to: entity-engram |\n\n### Sample Facts (3-5 facts for testing)\n- \"Engram uses Cohere Embed 4 for 1024-dim multimodal embeddings\" (factType: decision, scope: global)\n- \"Convex vector search is only available in actions, not queries\" (factType: learning, scope: global)\n- \"Ryan is the creator of OpenClaw and its agent ecosystem\" (factType: observation, scope: global)\n\n## Implementation Pattern\n\n```typescript\n// scripts/seed.ts\nimport { ConvexHttpClient } from \"convex/browser\";\nimport { api } from \"../convex/_generated/api\";\n\nconst client = new ConvexHttpClient(process.env.CONVEX_URL!);\n\nasync function seed() {\n  console.error(\"[seed] Starting...\");\n\n  // 1. Create scopes first (facts need scopeIds)\n  const globalScope = await client.mutation(api.functions.scopes.createScope, {\n    name: \"global\",\n    description: \"Shared knowledge base\",\n    members: [\"indy\"],\n    readPolicy: \"all\",\n    writePolicy: \"all\",\n  });\n\n  // 2. Register agent\n  await client.mutation(api.functions.agents.register, {\n    agentId: \"indy\",\n    name: \"Indy\",\n    capabilities: [\"memory\", \"code\", \"research\"],\n    defaultScope: \"private\",\n  });\n\n  // 3. Create entities\n  for (const entity of entities) {\n    await client.mutation(api.functions.entities.upsert, entity);\n  }\n\n  // 4. Store sample facts\n  for (const fact of sampleFacts) {\n    await client.mutation(api.functions.facts.storeFact, {\n      ...fact,\n      scopeId: globalScope,\n      createdBy: \"indy\",\n    });\n  }\n\n  console.error(\"[seed] Done!\");\n}\n\nseed().catch(console.error);\n```\n\n## Technical Notes\n- Use ConvexHttpClient (not internal functions) — seed runs OUTSIDE Convex runtime\n- Use console.error for all logging (stdout hygiene habit from MCP patterns)\n- Use process.env.CONVEX_URL for Convex connection\n- Run with: npx tsx scripts/seed.ts (or npx convex run scripts/seed if using Convex runner)\n- Batch pattern: Could use a single batch mutation for entities, but individual calls are fine for seed data (~10 items)\n- Make idempotent: upsert for entities, check-before-create for scopes\n\n## Acceptance Criteria\n- [ ] scripts/seed.ts exists and runs without errors\n- [ ] Creates global and private-indy scopes\n- [ ] Creates 7 initial entities with relationships\n- [ ] Stores 3+ sample facts in global scope\n- [ ] Facts are searchable via full-text search after seeding\n- [ ] Script is idempotent (running twice doesn't create duplicates)\n- [ ] All logging goes to stderr\n\n## Estimate\n30 minutes","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-11T22:07:29.875278+01:00","updated_at":"2026-02-11T22:25:07.967342+01:00","closed_at":"2026-02-11T22:25:07.967345+01:00","dependencies":[{"issue_id":"engram-u51.12","depends_on_id":"engram-u51","type":"parent-child","created_at":"2026-02-11T22:07:29.876065+01:00","created_by":"Ryan Lisse"},{"issue_id":"engram-u51.12","depends_on_id":"engram-u51.3","type":"blocks","created_at":"2026-02-11T22:07:29.877179+01:00","created_by":"Ryan Lisse"},{"issue_id":"engram-u51.12","depends_on_id":"engram-u51.4","type":"blocks","created_at":"2026-02-11T22:07:29.878235+01:00","created_by":"Ryan Lisse"},{"issue_id":"engram-u51.12","depends_on_id":"engram-u51.5","type":"blocks","created_at":"2026-02-11T22:07:29.879421+01:00","created_by":"Ryan Lisse"},{"issue_id":"engram-u51.12","depends_on_id":"engram-u51.6","type":"blocks","created_at":"2026-02-11T22:07:29.882011+01:00","created_by":"Ryan Lisse"}]}
{"id":"engram-u51.13","title":"End-to-end verification and testing","description":"# End-to-End Verification\n\n## What\nVerify that all Phase 1 deliverables work correctly together. This is the final quality gate before Phase 2 can begin.\n\n## Why\nPhase 2 (MCP Server) depends entirely on Phase 1's Convex functions working correctly. Every MCP tool call ultimately hits a Convex mutation or query. If the foundation is broken, everything built on it will be broken.\n\n## Verification Checklist\n\n### Schema Verification\n- [ ] npx convex dev compiles without errors\n- [ ] Convex dashboard shows all 10 tables: facts, entities, conversations, sessions, agents, memory_scopes, signals, themes, sync_log\n- [ ] Each table has correct indexes visible in dashboard\n- [ ] vector_search index defined on facts (dimensions: 1024)\n- [ ] theme_search index defined on themes (dimensions: 1024)\n\n### CRUD Round-Trip Tests (via Convex dashboard or convex functions CLI)\n- [ ] Create scope → query scope → add member → verify\n- [ ] Register agent → query agent → update lastSeen → verify\n- [ ] Store fact (with scope write check) → get fact by ID → verify all fields\n- [ ] Store fact to unauthorized scope → verify rejection\n- [ ] Create entity → add relationship → query by entityId → verify\n- [ ] Create session → add conversation → link fact to conversation → verify chain\n- [ ] Record signal on fact → query signals by fact → verify\n- [ ] Create theme → add facts → query by scope → verify\n\n### Full-Text Search Tests\n- [ ] Search \"Cohere\" → returns fact about embeddings\n- [ ] Search \"Convex\" with factType filter → returns only matching type\n- [ ] Search with scopeId filter → returns only facts in that scope\n- [ ] Search with createdBy filter → returns only facts by that agent\n- [ ] Empty search → returns empty array (no crash)\n\n### Write Permission Tests\n- [ ] Agent in scope.members can write to scope with writePolicy: \"members\"\n- [ ] Agent NOT in scope.members is rejected for writePolicy: \"members\"\n- [ ] Any agent can write to scope with writePolicy: \"all\"\n- [ ] Only creator can write to scope with writePolicy: \"creator\"\n\n### Seed Script Verification\n- [ ] Run scripts/seed.ts → no errors\n- [ ] Run scripts/seed.ts AGAIN → no duplicates (idempotent)\n- [ ] Query entities → 7 entities with correct relationships\n- [ ] Query scopes → global and private-indy exist\n- [ ] Search facts → seed facts are searchable\n\n### TypeScript Quality\n- [ ] No any types in function args/returns\n- [ ] TypeScript strict mode enabled in convex/tsconfig.json\n- [ ] All Convex validators use v.* types\n\n## How to Test\n1. npx convex dev — verify compile\n2. npx tsx scripts/seed.ts — run seed\n3. Use Convex dashboard to inspect tables and run queries\n4. Alternatively, use convex CLI: npx convex run functions/facts:searchFacts '{\"query\": \"Cohere\"}'\n\n## Phase 1 Complete When\nALL of the above checks pass. This unblocks Phase 2 (MCP Server).\n\n## Estimate\n30 minutes","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-11T22:07:50.19959+01:00","updated_at":"2026-02-12T09:42:34.780978+01:00","closed_at":"2026-02-12T09:42:34.780991+01:00","dependencies":[{"issue_id":"engram-u51.13","depends_on_id":"engram-u51","type":"parent-child","created_at":"2026-02-11T22:07:50.200464+01:00","created_by":"Ryan Lisse"},{"issue_id":"engram-u51.13","depends_on_id":"engram-u51.4","type":"blocks","created_at":"2026-02-11T22:07:50.201628+01:00","created_by":"Ryan Lisse"},{"issue_id":"engram-u51.13","depends_on_id":"engram-u51.5","type":"blocks","created_at":"2026-02-11T22:07:50.202672+01:00","created_by":"Ryan Lisse"},{"issue_id":"engram-u51.13","depends_on_id":"engram-u51.6","type":"blocks","created_at":"2026-02-11T22:07:50.203805+01:00","created_by":"Ryan Lisse"},{"issue_id":"engram-u51.13","depends_on_id":"engram-u51.7","type":"blocks","created_at":"2026-02-11T22:07:50.204824+01:00","created_by":"Ryan Lisse"},{"issue_id":"engram-u51.13","depends_on_id":"engram-u51.8","type":"blocks","created_at":"2026-02-11T22:07:50.205908+01:00","created_by":"Ryan Lisse"},{"issue_id":"engram-u51.13","depends_on_id":"engram-u51.9","type":"blocks","created_at":"2026-02-11T22:07:50.207039+01:00","created_by":"Ryan Lisse"},{"issue_id":"engram-u51.13","depends_on_id":"engram-u51.10","type":"blocks","created_at":"2026-02-11T22:07:50.208045+01:00","created_by":"Ryan Lisse"},{"issue_id":"engram-u51.13","depends_on_id":"engram-u51.11","type":"blocks","created_at":"2026-02-11T22:07:50.209063+01:00","created_by":"Ryan Lisse"},{"issue_id":"engram-u51.13","depends_on_id":"engram-u51.12","type":"blocks","created_at":"2026-02-11T22:07:50.210063+01:00","created_by":"Ryan Lisse"}]}
{"id":"engram-u51.2","title":"Define complete 10-table schema in convex/schema.ts","description":"# Define Complete 10-Table Schema\n\n## What\nCreate convex/schema.ts with ALL 10 tables from PLAN.md (lines 89-289), including all optional future-phase fields. This is the single most important file in Phase 1 — it defines the entire data model.\n\n## Why\nUsing the full PLAN.md schema from day 1 prevents costly migrations when later phases add lifecycle management, emotional context, temporal links, etc. Convex schema changes require explicit migrations. Optional fields (v.optional()) cost nothing until populated. Deploy once, never migrate schema.\n\n## Schema Decision: Full vs Simplified\nThe repo-research agent found 17 schema discrepancies between PLAN.md and the detailed plan. Key decisions:\n- Use lifecycleState (5 states: active|dormant|merged|archived|pruned), NOT status (3 states)\n- Include factualSummary (SimpleMem compressed representation)\n- Include updatedAt (needed for sync tracking in Phase 5)\n- Include outcomeScore (MemRL learned utility — Phase 4+)\n- Include contributingAgents (collaborative memory provenance — Phase 4+)\n- Include emotionalContext + emotionalWeight (GIZIN emotional memory — Phase 3+)\n- Include temporalLinks (MAGMA multi-graph pattern — Phase 3+)\n- Include forgetScore, mergedInto, consolidatedFrom, supersededBy (lifecycle — Phase 3+)\n- Include conversations.threadFacts (facts-to-conversations linking)\n- Include sessions.conversationIds (session-conversation links)\n- Include agents.telos (PAI purpose/goal)\n- Include memory_scopes.memoryPolicy + idealStateCriteria (ALMA + PAI)\n\n## Tables to Define\n\n### 1. facts (26 fields, 5 indexes + 1 searchIndex + 1 vectorIndex)\nThe core memory unit. Stores atomic facts with embeddings, importance scores, lifecycle state, emotional context, temporal links, and scope.\n- Core fields: content, timestamp, source, entityIds, relevanceScore, accessedCount, importanceScore, createdBy, scopeId, tags, factType, embedding\n- Lifecycle: lifecycleState, mergedInto, consolidatedFrom, supersededBy, forgetScore\n- Emotional: emotionalContext, emotionalWeight\n- Multi-graph: temporalLinks (array of {targetFactId, relation, confidence})\n- Research-informed: factualSummary, updatedAt, outcomeScore, contributingAgents\n- References: conversationId\n- Indexes: by_scope, by_agent, by_type, by_importance, by_lifecycle\n- searchIndex: search_content (field: content, filters: scopeId, factType, createdBy)\n- vectorIndex: vector_search (field: embedding, dimensions: 1024, filters: scopeId)\n\n### 2. entities (name, type, relationships graph, 4 indexes)\n### 3. conversations (sessionId, participants, threadFacts, handoffs, 2 indexes)\n### 4. sessions (agentId, conversationIds, factCount, 2 indexes)\n### 5. agents (agentId, name, capabilities, telos, settings, 1 index)\n### 6. memory_scopes (name, members, policies, ISC, 1 index)\n### 7. signals (factId, agentId, signalType, value, 3 indexes)\n### 8. themes (name, factIds, entityIds, scopeId, embedding, 2 indexes + 1 vectorIndex)\n### 9. sync_log (nodeId, lastSyncTimestamp, status, 1 index)\n\n## Critical Constraints\n- vectorIndex dimensions MUST be 1024 (Cohere Embed 4 output dimension)\n- vectorIndex filterFields only support equality filters via q.eq()\n- searchIndex filterFields support equality filters for pre-filtering\n- All future-phase fields must be v.optional()\n- factType is a string union: decision|observation|plan|error|insight|correction|steering_rule|learning|session_summary\n- lifecycleState string union: active|dormant|merged|archived|pruned\n\n## Source of Truth\nCopy schema definitions EXACTLY from PLAN.md lines 89-289. The schema in the detailed plan (docs/plans/2026-02-11-feat-engram-unified-memory-system-plan.md lines 93-242) is the SIMPLIFIED version — do NOT use it.\n\n## Acceptance Criteria\n- [ ] convex/schema.ts defines all 10 tables\n- [ ] facts table has all 26 fields (most as v.optional())\n- [ ] All indexes defined (5 regular + 1 search + 1 vector on facts, plus others)\n- [ ] vectorIndex dimensions set to 1024\n- [ ] npx convex dev compiles without schema errors\n- [ ] Convex dashboard shows all 10 tables with correct structure\n\n## Estimate\n45 minutes","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-11T22:04:36.166167+01:00","updated_at":"2026-02-11T22:25:05.787684+01:00","closed_at":"2026-02-11T22:25:05.787686+01:00","dependencies":[{"issue_id":"engram-u51.2","depends_on_id":"engram-u51","type":"parent-child","created_at":"2026-02-11T22:04:36.166973+01:00","created_by":"Ryan Lisse"},{"issue_id":"engram-u51.2","depends_on_id":"engram-u51.1","type":"blocks","created_at":"2026-02-11T22:04:36.168035+01:00","created_by":"Ryan Lisse"}]}
{"id":"engram-u51.3","title":"Implement memory_scopes CRUD with write policy enforcement","description":"# Implement Memory Scopes CRUD\n\n## What\nCreate convex/functions/scopes.ts with full CRUD operations for the memory_scopes table, including the critical checkWriteAccess helper used by storeFact. This task must be completed BEFORE facts CRUD because storeFact depends on scope write permission enforcement.\n\n## Why\nScopes are the access control mechanism for Engram. Every fact belongs to a scope. Scopes define who can read and write. The storeFact mutation must check scope writePolicy before inserting — this is a PLAN.md Phase 1 requirement that was missing from the detailed plan.\n\nScope types:\n- private-{agentId}: Only that agent can read/write\n- team-{name}: Team members can read/write\n- project-{name}: Project-scoped with member lists\n- global: Everyone can read/write (public knowledge base)\n\n## Functions to Implement\n\n### Mutations\n1. createScope(name, description, members, readPolicy, writePolicy, retentionDays?)\n   - Validate name uniqueness via by_name index\n   - Default readPolicy: \"members\", writePolicy: \"members\"\n   - Return scope ID\n\n2. addMember(scopeId, agentId)\n   - Append to members array if not already present\n   - idempotent\n\n3. removeMember(scopeId, agentId)\n   - Filter from members array\n   - Error if agent is last member (can't have empty scope)\n\n### Queries\n4. getScope(scopeId) — by Convex _id\n5. getScopeByName(name) — using by_name index\n6. getPermittedScopes(agentId) — return all scopes where agentId is in members OR readPolicy is \"all\"\n\n### Internal Helper (exported as internalQuery)\n7. checkWriteAccess(scopeId, agentId) — returns boolean\n   - If writePolicy === \"all\": return true\n   - If writePolicy === \"members\": return members.includes(agentId)\n   - If writePolicy === \"creator\": return members[0] === agentId (first member is creator)\n\n## Write Policy Logic\n| writePolicy | Who Can Write |\n|-------------|---------------|\n| \"all\" | Any agent |\n| \"members\" | Only agents in scope.members array |\n| \"creator\" | Only first member (scope creator) |\n\n## Technical Notes\n- Use Convex v validators for all args\n- Use shared helper pattern: export both public query and internalQuery versions\n- getPermittedScopes needs to check both membership AND readPolicy === \"all\"\n- For Phase 1, no auth beyond agent ID trust boundary\n\n## Acceptance Criteria\n- [ ] Can create a new scope with name, description, members, policies\n- [ ] Can add/remove members from a scope\n- [ ] getPermittedScopes returns correct scopes for an agent\n- [ ] checkWriteAccess correctly enforces writePolicy for \"all\", \"members\", \"creator\"\n- [ ] Duplicate scope names are rejected\n- [ ] Cannot remove last member from a scope\n\n## Estimate\n30 minutes","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-11T22:04:55.227542+01:00","updated_at":"2026-02-11T22:25:05.911549+01:00","closed_at":"2026-02-11T22:25:05.911551+01:00","dependencies":[{"issue_id":"engram-u51.3","depends_on_id":"engram-u51","type":"parent-child","created_at":"2026-02-11T22:04:55.228438+01:00","created_by":"Ryan Lisse"},{"issue_id":"engram-u51.3","depends_on_id":"engram-u51.2","type":"blocks","created_at":"2026-02-11T22:04:55.229555+01:00","created_by":"Ryan Lisse"}]}
{"id":"engram-u51.4","title":"Implement facts CRUD with write permission and full-text search","description":"# Implement Facts CRUD with Write Permission \u0026 Full-Text Search\n\n## What\nCreate convex/functions/facts.ts — the most important function file. Implements storeFact (with scope write permission check), read queries, full-text search, and access tracking. This is the core memory primitive that all MCP tools will use.\n\n## Why\nFacts are the atomic memory units in Engram. Every memory_store_fact, memory_recall, memory_search MCP tool call ultimately hits these functions. Getting storeFact right is critical — it must enforce scope write permissions, estimate importance without external calls, and prepare for async enrichment (Phase 3).\n\n## Functions to Implement\n\n### Mutations\n\n1. storeFact — THE critical mutation\n   Args: content, source?, entityIds?, tags?, factType?, scopeId, createdBy, conversationId?, emotionalContext?\n   Logic:\n   a. Check write permission: call checkWriteAccess(scopeId, createdBy) — throw if unauthorized\n   b. Estimate importance: keyword-based scoring (no external calls)\n      - High (0.9): decision, error, critical, breaking, failed, security\n      - Medium (0.6): fix, implement, create, build, update, deploy\n      - Low (0.3): note, observation, maybe, consider, minor\n      - Default: 0.5\n   c. Insert fact with defaults: relevanceScore=1.0, accessedCount=0, lifecycleState=\"active\", embedding=undefined\n   d. Comment out enrichment scheduler: // await ctx.scheduler.runAfter(0, internal.actions.enrich.enrichFact, { factId })\n   e. Return { factId, importanceScore }\n\n2. bumpAccess(factId)\n   - Increment accessedCount by 1\n   - Recalculate relevanceScore (simple: min(1.0, 0.5 + accessedCount * 0.05))\n\n3. updateEnrichment(factId, updates) — internalMutation\n   - Patch fact with embedding, factualSummary, entityIds, temporalLinks, etc.\n   - Set updatedAt to Date.now()\n   - Used by Phase 3 enrichment pipeline\n\n### Queries\n\n4. getFact(factId) — query by Convex _id\n5. getByIds(factIds: Id\u003c\"facts\"\u003e[]) — query multiple facts\n6. getByScope(scopeId, limit?, factType?) — using by_scope index\n7. getByAgent(createdBy, limit?) — using by_agent index\n\n### Full-Text Search\n\n8. searchFacts(query, scopeId?, factType?, createdBy?, limit?)\n   - Uses searchIndex(\"search_content\")\n   - Filter by scopeId, factType, createdBy (all optional)\n   - Default limit: 10\n   - Returns matching facts sorted by relevance\n\n## storeFact Implementation Pattern (from detailed plan lines 269-308)\n\n```typescript\nexport const storeFact = mutation({\n  args: {\n    content: v.string(),\n    source: v.optional(v.string()),\n    entityIds: v.optional(v.array(v.string())),\n    tags: v.optional(v.array(v.string())),\n    factType: v.optional(v.string()),\n    scopeId: v.id(\"memory_scopes\"),\n    createdBy: v.string(),\n    conversationId: v.optional(v.id(\"conversations\")),\n    emotionalContext: v.optional(v.string()),\n  },\n  returns: v.object({ factId: v.id(\"facts\"), importanceScore: v.number() }),\n  handler: async (ctx, args) =\u003e {\n    // 1. Check write permission\n    const scope = await ctx.db.get(args.scopeId);\n    if (!scope) throw new Error(\"Scope not found\");\n    if (scope.writePolicy === \"members\" \u0026\u0026 !scope.members.includes(args.createdBy)) {\n      throw new Error(\"Agent not authorized to write to this scope\");\n    }\n    // ... (see Phase 1 plan for full implementation)\n  },\n});\n```\n\n## Full-Text Search Pattern\n\n```typescript\nexport const searchFacts = query({\n  args: {\n    query: v.string(),\n    scopeId: v.optional(v.id(\"memory_scopes\")),\n    factType: v.optional(v.string()),\n    createdBy: v.optional(v.string()),\n    limit: v.optional(v.number()),\n  },\n  handler: async (ctx, args) =\u003e {\n    let search = ctx.db.query(\"facts\").withSearchIndex(\"search_content\", (q) =\u003e {\n      let s = q.search(\"content\", args.query);\n      if (args.scopeId) s = s.eq(\"scopeId\", args.scopeId);\n      if (args.factType) s = s.eq(\"factType\", args.factType);\n      if (args.createdBy) s = s.eq(\"createdBy\", args.createdBy);\n      return s;\n    });\n    return await search.take(args.limit ?? 10);\n  },\n});\n```\n\n## Critical Constraints\n- Mutations retry automatically on transient errors — make storeFact idempotent-safe\n- Don't use console.log (reserved for MCP stdio in Phase 2) — use console.error for debugging\n- The enrichment scheduler call should be COMMENTED OUT in Phase 1 (action doesn't exist yet)\n- Use shared helper pattern for getFact (public query + internalQuery)\n- estimateImportance is a plain function, not a Convex function (no ctx needed)\n\n## Acceptance Criteria\n- [ ] storeFact mutation works with scope write permission check\n- [ ] storeFact rejects unauthorized writes (writePolicy: \"members\" with non-member)\n- [ ] getFact and getByIds return correct facts\n- [ ] searchFacts returns full-text search results with scope/type/agent filtering\n- [ ] bumpAccess increments count and updates relevanceScore\n- [ ] updateEnrichment (internal) can patch fact with enrichment data\n- [ ] estimateImportance returns correct scores for high/medium/low keywords\n\n## Estimate\n60 minutes","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-11T22:05:26.070928+01:00","updated_at":"2026-02-11T22:25:06.042923+01:00","closed_at":"2026-02-11T22:25:06.042925+01:00","dependencies":[{"issue_id":"engram-u51.4","depends_on_id":"engram-u51","type":"parent-child","created_at":"2026-02-11T22:05:26.071768+01:00","created_by":"Ryan Lisse"},{"issue_id":"engram-u51.4","depends_on_id":"engram-u51.2","type":"blocks","created_at":"2026-02-11T22:05:26.072955+01:00","created_by":"Ryan Lisse"},{"issue_id":"engram-u51.4","depends_on_id":"engram-u51.3","type":"blocks","created_at":"2026-02-11T22:05:26.073956+01:00","created_by":"Ryan Lisse"}]}
{"id":"engram-u51.5","title":"Implement entities CRUD with relationship graph","description":"# Implement Entities CRUD\n\n## What\nCreate convex/functions/entities.ts with CRUD operations for the entities table. Entities are named concepts (person, project, company, concept, tool) with a relationship graph.\n\n## Why\nEntities are the knowledge graph nodes in Engram. They represent people (Ryan, Indy), projects (OpenClaw, Engram), tools (Convex, LanceDB, Cohere), etc. Facts link to entities via entityIds array. The entity extraction pipeline (Phase 3) will auto-create entities from fact content.\n\n## Entity Schema (from PLAN.md)\n- entityId: string — human-readable ID like \"entity-ryan\", \"entity-convex\"\n- name: string — display name\n- type: string — person|project|company|concept|tool\n- firstSeen, lastSeen: number — timestamps\n- metadata: v.any() — flexible key-value store\n- relationships: array of {targetId, relationType, since?}\n  - Relation types: created_by|depends_on|works_with|part_of|related_to\n- importanceScore: float64\n- accessCount: number\n- createdBy: string — agent ID\n\n## Functions to Implement\n\n### Mutations\n1. upsert(entityId, name, type, metadata?, createdBy)\n   - Check if entity exists via by_entity_id index\n   - If exists: update lastSeen, merge metadata, increment accessCount\n   - If new: create with firstSeen=lastSeen=Date.now(), accessCount=1, relationships=[], importanceScore=0.5\n\n2. addRelationship(entityId, targetId, relationType, since?)\n   - Find entity via by_entity_id index\n   - Check for duplicate relationship (same targetId + relationType)\n   - Append to relationships array if new\n\n3. updateImportance(entityId, importanceScore) — internalMutation for Phase 3\n\n### Queries\n4. get(entityConvexId) — by Convex _id\n5. getByEntityId(entityId) — using by_entity_id index\n6. getByType(type, limit?) — using by_type index\n7. searchEntities(query, limit?) — full-text search on entity name\n\n## Technical Notes\n- entityId is the human-readable string (e.g., \"entity-ryan\"), NOT the Convex _id\n- The by_entity_id index enables O(1) lookup by string ID\n- metadata is v.any() for flexibility — stores arbitrary key-value data\n- Relationships form a directed graph: entity A relates_to entity B\n- The seed script will create initial entities using upsert\n\n## Acceptance Criteria\n- [ ] Can create new entity via upsert\n- [ ] Upsert on existing entity updates lastSeen and increments accessCount\n- [ ] Can add relationships between entities\n- [ ] Duplicate relationships are ignored (idempotent)\n- [ ] getByEntityId returns correct entity\n- [ ] searchEntities uses full-text search index on name field\n\n## Estimate\n30 minutes","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-11T22:05:44.180853+01:00","updated_at":"2026-02-11T22:25:06.177017+01:00","closed_at":"2026-02-11T22:25:06.177019+01:00","dependencies":[{"issue_id":"engram-u51.5","depends_on_id":"engram-u51","type":"parent-child","created_at":"2026-02-11T22:05:44.181937+01:00","created_by":"Ryan Lisse"},{"issue_id":"engram-u51.5","depends_on_id":"engram-u51.2","type":"blocks","created_at":"2026-02-11T22:05:44.183246+01:00","created_by":"Ryan Lisse"}]}
{"id":"engram-u51.6","title":"Implement agents CRUD with registration","description":"# Implement Agents CRUD\n\n## What\nCreate convex/functions/agents.ts with CRUD operations for the agents table. Agents are the identities that interact with Engram memory.\n\n## Why\nEvery memory operation is attributed to an agent. Agent registration is the first step when an agent connects to Engram. The agents table tracks capabilities, last seen time, fact count, default scope, and optional telos (purpose/goal from PAI pattern).\n\n## Agent Schema (from PLAN.md)\n- agentId: string — human-readable like \"indy\", \"coder-1\", \"ml-worker\"\n- name: string — display name\n- nodeId: string? — OpenClaw node identifier\n- capabilities: string[] — what the agent can do\n- lastSeen: number — timestamp of last activity\n- factCount: number — total facts created by this agent\n- defaultScope: string — \"private\"|\"team\"|\"public\"\n- telos: string? — purpose/goal (PAI pattern, e.g., \"Ship code faster\")\n- settings: any? — agent-specific memory configuration\n\n## Functions to Implement\n\n### Mutations\n1. register(agentId, name, capabilities, defaultScope, nodeId?, telos?, settings?)\n   - Check if agent exists via by_agent_id index\n   - If exists: update lastSeen, capabilities, nodeId, telos, settings\n   - If new: create with lastSeen=Date.now(), factCount=0\n   - Also create private scope \"private-{agentId}\" if not exists\n   - Return agent Convex _id\n\n2. updateLastSeen(agentId) — called on every agent interaction\n   - Update lastSeen to Date.now()\n\n3. incrementFactCount(agentId) — internalMutation\n   - Increment factCount by 1 (called by storeFact)\n\n### Queries\n4. get(agentConvexId) — by Convex _id\n5. getByAgentId(agentId) — using by_agent_id index\n6. listAgents(limit?) — list all agents ordered by lastSeen\n\n## Technical Notes\n- Agent registration should be idempotent (re-registering updates, doesn't create duplicate)\n- The register mutation should also create the agent's private scope (depends on scopes CRUD existing, but can be done inline)\n- factCount is denormalized for quick access — incremented by storeFact\n- defaultScope determines where facts go when scopeId is not explicitly provided\n\n## Acceptance Criteria\n- [ ] Can register a new agent with capabilities and telos\n- [ ] Re-registration updates existing agent (idempotent)\n- [ ] updateLastSeen correctly timestamps\n- [ ] getByAgentId returns correct agent via index\n- [ ] listAgents returns agents sorted by lastSeen\n\n## Estimate\n25 minutes","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-11T22:06:00.876537+01:00","updated_at":"2026-02-11T22:25:06.309545+01:00","closed_at":"2026-02-11T22:25:06.309547+01:00","dependencies":[{"issue_id":"engram-u51.6","depends_on_id":"engram-u51","type":"parent-child","created_at":"2026-02-11T22:06:00.878563+01:00","created_by":"Ryan Lisse"},{"issue_id":"engram-u51.6","depends_on_id":"engram-u51.2","type":"blocks","created_at":"2026-02-11T22:06:00.88125+01:00","created_by":"Ryan Lisse"}]}
{"id":"engram-u51.7","title":"Implement conversations CRUD with handoff tracking","description":"# Implement Conversations CRUD\n\n## What\nCreate convex/functions/conversations.ts for the conversations table. Conversations thread facts together, track participants, and record agent handoffs.\n\n## Why\nConversations provide temporal grouping of facts within a session. They track which agents participated and when handoffs occurred (Agent A passes context to Agent B). The threadFacts array links facts to their conversation for retrieval.\n\n## Schema (from PLAN.md)\n- sessionId: Id\u003c\"sessions\"\u003e\n- participants: string[] (agent IDs)\n- threadFacts: Id\u003c\"facts\"\u003e[] (linked facts)\n- contextSummary: string\n- importance: float64\n- tags: string[]\n- handoffs: array of {fromAgent, toAgent, timestamp, contextSummary}\n\n## Functions to Implement\n\n### Mutations\n1. create(sessionId, participants, contextSummary?, tags?) — create new conversation\n2. addFact(conversationId, factId) — append to threadFacts array\n3. addHandoff(conversationId, fromAgent, toAgent, contextSummary) — record agent handoff\n4. updateSummary(conversationId, contextSummary) — update context summary\n\n### Queries\n5. get(conversationId) — by Convex _id\n6. getBySession(sessionId) — using by_session index\n\n## Conversation Boundary Logic (from SpecFlow analysis)\n- Time gap \u003e 30 minutes OR agent explicitly calls boundary → new conversation\n- This logic lives in the MCP server (Phase 2), not in Convex functions\n\n## Acceptance Criteria\n- [ ] Can create conversations linked to sessions\n- [ ] Can add facts to conversation threadFacts\n- [ ] Can record handoffs between agents\n- [ ] getBySession returns conversations for a session\n\n## Estimate\n20 minutes","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-11T22:06:14.540067+01:00","updated_at":"2026-02-11T22:25:07.17318+01:00","closed_at":"2026-02-11T22:25:07.173182+01:00","dependencies":[{"issue_id":"engram-u51.7","depends_on_id":"engram-u51","type":"parent-child","created_at":"2026-02-11T22:06:14.541431+01:00","created_by":"Ryan Lisse"},{"issue_id":"engram-u51.7","depends_on_id":"engram-u51.2","type":"blocks","created_at":"2026-02-11T22:06:14.542843+01:00","created_by":"Ryan Lisse"}]}
{"id":"engram-u51.8","title":"Implement sessions CRUD","description":"# Implement Sessions CRUD\n\n## What\nCreate convex/functions/sessions.ts for the sessions table. Sessions track agent activity periods.\n\n## Schema (from PLAN.md)\n- agentId: string\n- startTime: number\n- lastActivity: number\n- conversationIds: Id\u003c\"conversations\"\u003e[]\n- factCount: number\n- contextSummary: string\n- parentSession: Id\u003c\"sessions\"\u003e? (for nested/resumed sessions)\n- nodeId: string? (OpenClaw node)\n\n## Functions to Implement\n\n### Mutations\n1. create(agentId, contextSummary?, parentSession?, nodeId?) — start new session\n2. updateActivity(sessionId) — update lastActivity timestamp\n3. addConversation(sessionId, conversationId) — append to conversationIds\n4. incrementFactCount(sessionId) — internalMutation\n\n### Queries\n5. get(sessionId)\n6. getByAgent(agentId, limit?) — using by_agent index, ordered by startTime\n7. getActive(agentId) — most recent session for an agent\n\n## Acceptance Criteria\n- [ ] Can create sessions linked to agents\n- [ ] updateActivity correctly timestamps\n- [ ] getByAgent returns sessions ordered by startTime\n- [ ] getActive returns the most recent session\n\n## Estimate\n20 minutes","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-11T22:06:26.582497+01:00","updated_at":"2026-02-11T22:25:07.321437+01:00","closed_at":"2026-02-11T22:25:07.32144+01:00","dependencies":[{"issue_id":"engram-u51.8","depends_on_id":"engram-u51","type":"parent-child","created_at":"2026-02-11T22:06:26.583468+01:00","created_by":"Ryan Lisse"},{"issue_id":"engram-u51.8","depends_on_id":"engram-u51.2","type":"blocks","created_at":"2026-02-11T22:06:26.584755+01:00","created_by":"Ryan Lisse"}]}
{"id":"engram-u51.9","title":"Implement signals CRUD for feedback tracking","description":"# Implement Signals CRUD\n\n## What\nCreate convex/functions/signals.ts for the signals table. Signals capture feedback on facts — explicit ratings and implicit sentiment from the PAI feedback loop pattern.\n\n## Why\nSignals are how agents communicate which memories were useful. This enables learned utility scoring (MemRL pattern) in later phases. The PAI system uses explicit ratings (1-10) and implicit sentiment (-1.0 to 1.0) to steer memory retrieval.\n\n## Schema (from PLAN.md)\n- factId: Id\u003c\"facts\"\u003e? (optional — some signals are session-level)\n- sessionId: Id\u003c\"sessions\"\u003e? (optional)\n- agentId: string\n- signalType: string — explicit_rating|implicit_sentiment|failure\n- value: number — 1-10 for ratings, -1.0 to 1.0 for sentiment\n- comment: string?\n- confidence: float64?\n- context: string?\n- timestamp: number\n\n## Functions to Implement\n\n### Mutations\n1. recordSignal(factId?, sessionId?, agentId, signalType, value, comment?, confidence?, context?)\n   - Validate: at least one of factId or sessionId must be provided\n   - Validate: value range depends on signalType (1-10 for rating, -1.0-1.0 for sentiment)\n   - Set timestamp to Date.now()\n\n### Queries\n2. getByFact(factId, limit?) — using by_fact index\n3. getByAgent(agentId, limit?) — using by_agent index\n4. getByType(signalType, limit?) — using by_type index\n\n## Acceptance Criteria\n- [ ] Can record signals with fact or session reference\n- [ ] Validation rejects signals without factId AND sessionId\n- [ ] getByFact returns signals for a specific fact\n- [ ] getByAgent returns signals from a specific agent\n\n## Estimate\n20 minutes","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-11T22:06:39.616471+01:00","updated_at":"2026-02-11T22:25:07.467715+01:00","closed_at":"2026-02-11T22:25:07.467719+01:00","dependencies":[{"issue_id":"engram-u51.9","depends_on_id":"engram-u51","type":"parent-child","created_at":"2026-02-11T22:06:39.617434+01:00","created_by":"Ryan Lisse"},{"issue_id":"engram-u51.9","depends_on_id":"engram-u51.2","type":"blocks","created_at":"2026-02-11T22:06:39.618846+01:00","created_by":"Ryan Lisse"}]}
{"id":"engram-u6k","title":"Phase 5: History Bootstrap","status":"open","priority":2,"issue_type":"epic","created_at":"2026-02-25T14:04:59.523418+01:00","updated_at":"2026-02-25T14:04:59.523418+01:00"}
{"id":"engram-u6o","title":"Phase 4: Multi-Agent + Crons — multi-scope recall, 7 cron jobs, handoff tracking","status":"closed","priority":0,"issue_type":"epic","created_at":"2026-02-12T09:51:00.561603+01:00","updated_at":"2026-02-12T10:06:05.587025+01:00","closed_at":"2026-02-12T10:06:05.587025+01:00","close_reason":"Completed"}
{"id":"engram-uah","title":"Vault file watcher: MD→Convex bidirectional import","description":"Vault file watcher for bidirectional sync (integrated in vault-sync daemon):\n\n**Implementation (part of mcp-server/src/daemons/vault-sync.ts from engram-7yr):**\n\nUsing chokidar to watch vault/ directory:\n- Watch patterns: vault/**/*.md (ignore dot files)\n- Events: change, add (not delete - archive in DB instead)\n- Options: ignoreInitial: true (don't trigger on startup), persistent: true\n\n**On file change:**\n1. Read file content\n2. Parse frontmatter + body\n3. Extract fact ID from frontmatter.id\n4. Call reconcileFromVault action (from engram-8nz)\n5. If conflicts detected: create .conflict file\n6. If success: log sync event\n\n**Error handling:**\n- Parse errors: log + skip file\n- Missing ID: log warning + skip\n- Convex connection errors: exponential backoff retry\n- File permission errors: log + skip\n\n**Reconciliation flow:**\nvault file edited → chokidar detects change → reconcileFromVault action → three-way merge → DB updated OR conflict file created\n\n**Performance:**\n- Event handling \u003c100ms\n- Reconciliation \u003c200ms p95\n- No blocking of other daemon operations\n\n**Tests:** file-watcher-e2e.test.ts (edit file → event triggered → DB updated), conflict-detection-e2e.test.ts (concurrent edits → conflict file)\nRef: VAULT_INTEGRATION_PLAN.md Phase 3.3, Phase 3.4","status":"closed","priority":3,"issue_type":"task","created_at":"2026-02-14T22:57:02.366752+01:00","updated_at":"2026-02-14T23:25:41.78073+01:00","closed_at":"2026-02-14T23:25:41.78073+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-uah","depends_on_id":"engram-dxj","type":"blocks","created_at":"2026-02-14T22:57:23.950393+01:00","created_by":"daemon"},{"issue_id":"engram-uah","depends_on_id":"engram-8nz","type":"blocks","created_at":"2026-02-14T23:07:58.964782+01:00","created_by":"daemon"}]}
{"id":"engram-uqr","title":"Phase 2: MCP Server — 12 tools, stdio transport, ConvexHttpClient","status":"closed","priority":0,"issue_type":"epic","created_at":"2026-02-12T09:50:57.325257+01:00","updated_at":"2026-02-12T10:06:05.281267+01:00","closed_at":"2026-02-12T10:06:05.281267+01:00","close_reason":"Completed"}
{"id":"engram-uuf","title":"Convex: Query for pinned facts by scope","description":"Add Convex query function that fetches all pinned facts for a given scope. Add index by_pinned or filter on existing indexes. Used by get_memory_manifest and build_system_prompt.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-25T14:14:23.070116+01:00","updated_at":"2026-02-25T14:24:11.883115+01:00","closed_at":"2026-02-25T14:24:11.883115+01:00","close_reason":"Covered by engram-adj (manifest tool queries pinned facts by scope)"}
{"id":"engram-uvz","title":"Hierarchical Vault Index Improvement","description":"# Hierarchical Vault Index Improvement (PageIndex Pattern)\n\n## Problem\nCurrent `hierarchical-recall.ts` uses entity graph traversal, but the **vault index** (`vault-index.md`) is a flat file, not a proper hierarchical tree. PageIndex paper shows tree traversal (98.7% accuracy) \u003e vector search for structured knowledge.\n\n## Solution\nGenerate hierarchical vault index as a proper tree structure:\n- **Level 1**: Scopes\n- **Level 2**: Entity types (person|project|concept|tool)\n- **Level 3**: Entities\n- **Level 4**: Facts\n\nUse this as first traversal layer in hierarchical recall before vector search.\n\n## Index Structure\n```markdown\n# Vault Index\n\n## Scope: private-ryan\n### People\n- Ryan\n  - Facts: [ryan's email is ...], [ryan prefers bun], ...\n- Alice\n  - Facts: [alice works on project X], ...\n  \n### Projects\n- Briefly\n  - Facts: [briefly is a daily briefing tool], [uses TypeScript], ...\n- Engram\n  - Facts: [engram is multi-agent memory], ...\n\n### Tools\n- Bun\n  - Facts: [bun is a JS runtime], [ryan prefers bun], ...\n\n## Scope: team-ml\n...\n```\n\n## Generation Logic\n```typescript\n// convex/vault/generate-index.ts\nexport const generateIndex = mutation(async (ctx) =\u003e {\n  const scopes = await ctx.db.query(\"memory_scopes\").collect();\n  let index = \"# Vault Index\\n\\n\";\n  \n  for (const scope of scopes) {\n    index += `## Scope: ${scope.name}\\n`;\n    \n    const entities = await ctx.db\n      .query(\"entities\")\n      .filter(q =\u003e /* entities in scope */)\n      .collect();\n    \n    const byType = groupBy(entities, \"type\");\n    \n    for (const [type, typeEntities] of Object.entries(byType)) {\n      index += `### ${capitalize(type)}s\\n`;\n      \n      for (const entity of typeEntities) {\n        index += `- ${entity.name}\\n`;\n        const facts = await getFactsByEntity(ctx, entity.entityId);\n        for (const fact of facts.slice(0, 5)) {\n          index += `  - ${truncate(fact.content, 80)}\\n`;\n        }\n      }\n    }\n  }\n  \n  return index;\n});\n```\n\n## Integration with Hierarchical Recall\n```typescript\n// In hierarchical-recall.ts:\n// 1. Load cached index from Convex\nconst index = await ctx.runQuery(internal.vault.getIndex);\n\n// 2. Parse index to find relevant sections\nconst sections = parseIndexSections(index, query);\n\n// 3. Extract entity IDs from matched sections\nconst entityIds = extractEntityIds(sections);\n\n// 4. Use these as anchor points for graph traversal\nconst facts = await traverseFromAnchors(entityIds, maxDepth);\n```\n\n## Caching \u0026 Rebuild\n```typescript\n// Rebuild index on:\n- New fact added (if entity not in index)\n- Entity created\n- Manual trigger via MCP tool\n\n// Cache in Convex system_config:\nawait ctx.db.insert(\"system_config\", {\n  key: \"vault_index\",\n  value: indexContent,\n  category: \"cache\",\n  updatedAt: Date.now(),\n});\n\n// Invalidate on mutation via memory_events listener\n```\n\n## Impact\n- Faster hierarchical recall (tree traversal vs brute-force entity search)\n- Human-readable vault structure (can view in Obsidian)\n- 98.7% retrieval accuracy (PageIndex benchmark)\n- Reduces vector search load\n\n## References\n- PageIndex paper (tree traversal for RAG)\n- Current hierarchical-recall.ts: mcp-server/src/tools/hierarchical-recall.ts\n- Optimization doc: docs/OPTIMIZATION-2026-02-24.md (section 7)","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-24T09:33:14.013495+01:00","updated_at":"2026-02-25T11:41:12.270402+01:00","closed_at":"2026-02-25T11:41:12.270402+01:00","close_reason":"Closed"}
{"id":"engram-uvz.1","title":"Implement hierarchical index generation","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-24T09:41:25.117644+01:00","updated_at":"2026-02-25T02:56:06.546747+01:00","closed_at":"2026-02-25T02:56:06.546747+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-uvz.1","depends_on_id":"engram-uvz","type":"parent-child","created_at":"2026-02-24T09:41:25.119062+01:00","created_by":"daemon"}]}
{"id":"engram-uvz.2","title":"Cache index in system_config","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-24T09:41:25.354499+01:00","updated_at":"2026-02-25T02:56:06.953748+01:00","closed_at":"2026-02-25T02:56:06.953748+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-uvz.2","depends_on_id":"engram-uvz","type":"parent-child","created_at":"2026-02-24T09:41:25.355836+01:00","created_by":"daemon"}]}
{"id":"engram-uvz.3","title":"Integrate index with hierarchical-recall","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-24T09:41:25.552126+01:00","updated_at":"2026-02-25T03:00:55.586616+01:00","closed_at":"2026-02-25T03:00:55.586616+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-uvz.3","depends_on_id":"engram-uvz","type":"parent-child","created_at":"2026-02-24T09:41:25.553261+01:00","created_by":"daemon"}]}
{"id":"engram-uvz.4","title":"Auto-rebuild on mutations","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-24T09:41:25.736116+01:00","updated_at":"2026-02-25T03:10:10.347151+01:00","closed_at":"2026-02-25T03:10:10.347151+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-uvz.4","depends_on_id":"engram-uvz","type":"parent-child","created_at":"2026-02-24T09:41:25.737591+01:00","created_by":"daemon"}]}
{"id":"engram-uvz.5","title":"Unit tests: Index generation","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-24T09:41:26.015112+01:00","updated_at":"2026-02-25T03:11:44.138427+01:00","closed_at":"2026-02-25T03:11:44.138427+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-uvz.5","depends_on_id":"engram-uvz","type":"parent-child","created_at":"2026-02-24T09:41:26.016182+01:00","created_by":"daemon"}]}
{"id":"engram-uvz.6","title":"E2E test: Index-based retrieval","status":"closed","priority":3,"issue_type":"task","created_at":"2026-02-24T09:41:26.214234+01:00","updated_at":"2026-02-25T03:18:01.031385+01:00","closed_at":"2026-02-25T03:18:01.031385+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-uvz.6","depends_on_id":"engram-uvz","type":"parent-child","created_at":"2026-02-24T09:41:26.215241+01:00","created_by":"daemon"}]}
{"id":"engram-uw7","title":"Export facts to filesystem as markdown with YAML frontmatter","status":"open","priority":3,"issue_type":"task","created_at":"2026-02-25T14:05:00.856192+01:00","updated_at":"2026-02-25T14:05:00.856192+01:00","dependencies":[{"issue_id":"engram-uw7","depends_on_id":"engram-li6","type":"blocks","created_at":"2026-02-25T14:05:45.002637+01:00","created_by":"daemon"}]}
{"id":"engram-vdr","title":"Sleep-time fact extraction from session history","description":"Implement the reflection agent logic: read recent session events (last 4-6h), use LLM to extract facts/preferences/corrections, store via Engram MCP tools. Run as Convex action for compute headroom.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-25T14:14:32.708189+01:00","updated_at":"2026-02-25T14:17:26.819972+01:00","closed_at":"2026-02-25T14:17:26.819972+01:00","close_reason":"Duplicate of P1 structured set (engram-adw, engram-sws, engram-2kn, engram-adj, engram-aeb, engram-2s3) which has proper dependency chains"}
{"id":"engram-vgb","title":"Decompose memory_summarize and memory_prune","description":"Split into primitives: memory_list_stale_facts, memory_mark_facts_merged, memory_mark_facts_pruned. Let agents compose filtering logic","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-15T00:38:34.44353+01:00","updated_at":"2026-02-15T00:58:48.155158+01:00","closed_at":"2026-02-15T00:58:48.155158+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-vgb","depends_on_id":"engram-ai3","type":"blocks","created_at":"2026-02-15T00:39:05.773245+01:00","created_by":"daemon"}]}
{"id":"engram-vj6","title":"MCP tools: query_vault + memory_export_graph","description":"New MCP tools for vault querying and graph export:\n\n**1. memory_query_vault (mcp-server/src/tools/query-vault.ts):**\n- Direct vault file queries without Convex\n- Parameters: query (string), filters (type, priority, scope)\n- Scan vault index files for matches\n- Load facts from markdown files\n- Use case: Fast local queries when Convex unavailable\n- Returns: facts[], source: 'vault', latencyMs\n\n**2. memory_export_graph (mcp-server/src/tools/export-graph.ts):**\n- Export memory graph to Obsidian-compatible JSON\n- Uses graph-exporter.ts (in engram-43e)\n- Parameters: outputPath (default: vault/.obsidian/graph.json)\n- Format: {nodes: [{id, label, type, group}], links: [{source, target, type}]}\n- Includes fact→entity edges (type: 'mentions')\n- Includes entity→entity edges (type: relationship type)\n- Use case: Obsidian graph visualization\n- Returns: {success, nodeCount, linkCount, outputPath}\n\n**MCP server registration (mcp-server/src/index.ts):**\n- Register both tools with MCP protocol\n- Add to tool list in server initialization\n\n**Tests:** query-vault.test.ts (vault queries work), export-graph.test.ts (valid Obsidian JSON format)\n**Performance:** Query vault \u003c150ms, graph export \u003c2s for 10k facts\nRef: VAULT_INTEGRATION_PLAN.md Phase 1.3.2, Phase 4.5","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-14T23:00:31.532522+01:00","updated_at":"2026-02-14T23:25:09.113482+01:00","closed_at":"2026-02-14T23:25:09.113482+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-vj6","depends_on_id":"engram-43e","type":"blocks","created_at":"2026-02-14T23:01:44.167653+01:00","created_by":"daemon"},{"issue_id":"engram-vj6","depends_on_id":"engram-gxr","type":"blocks","created_at":"2026-02-14T23:02:12.639062+01:00","created_by":"daemon"}]}
{"id":"engram-w3u","title":"Phase 6: Filesystem Mirror","status":"open","priority":3,"issue_type":"epic","created_at":"2026-02-25T14:05:00.667643+01:00","updated_at":"2026-02-25T14:05:00.667643+01:00"}
{"id":"engram-w8r","title":"OpenClaw cron: Sleep-time reflection agent (Haiku, 6h)","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-25T14:04:56.283616+01:00","updated_at":"2026-02-25T14:28:32.074387+01:00","closed_at":"2026-02-25T14:28:32.074387+01:00","close_reason":"Added sleep-time reflection cron to OpenClaw plugin (6h Haiku schedule)","dependencies":[{"issue_id":"engram-w8r","depends_on_id":"engram-2s3","type":"blocks","created_at":"2026-02-25T14:05:41.626684+01:00","created_by":"daemon"},{"issue_id":"engram-w8r","depends_on_id":"engram-0v6","type":"blocks","created_at":"2026-02-25T14:05:41.806442+01:00","created_by":"daemon"}]}
{"id":"engram-waf","title":"Create vault-format.ts: Fact↔Markdown serialization","description":"Create mcp-server/src/lib/vault-format.ts with comprehensive markdown serialization:\n\n**Main Functions:**\n1. generateFrontmatter(fact) — Convert fact to YAML frontmatter with all fields (id, type, scope, priority, lifecycleState, tags, entities, timestamps, importance, confidence, importanceTier, vaultPath, observationTier, etc.). Omit null/undefined. Format dates as ISO 8601.\n2. generateMarkdownBody(fact) — Convert content to markdown with H1 title, context blockquote, main content, Related Facts section (wiki-links), Entities section, Provenance footer.\n3. parseFrontmatter(fileContent) — Parse YAML + body, validate required fields, convert date strings to Date objects, handle malformed YAML gracefully, return {frontmatter, body, errors}.\n4. generateFilename(fact) — Format: {slugified-title}-{short-id}.md (max 50 chars slug, 8-char convexId prefix). Use slugify npm package.\n5. computeFolderPath(fact, vaultRoot) — Route by factType using FACT_TYPE_TO_FOLDER map from spec §1.2. Scope prefix: private-{agentId}/, team-{teamId}/, etc.\n6. extractWikiLinks(content) — Find all [[Name]] links, return array of {name, startIndex, endIndex}.\n\n**Dependencies:** js-yaml, slugify\n**Tests:** Unit tests for each function, golden tests for round-trip (fact → markdown → fact)\n**Performance target:** Format/parse \u003c10ms per fact\nRef: specs/obsidian-mirror-plan.md §1.1-1.3, Phase 2","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-14T22:56:13.806981+01:00","updated_at":"2026-02-14T23:24:31.240256+01:00","closed_at":"2026-02-14T23:24:31.240256+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-waf","depends_on_id":"engram-ywx","type":"blocks","created_at":"2026-02-14T22:57:17.70699+01:00","created_by":"daemon"},{"issue_id":"engram-waf","depends_on_id":"engram-46g","type":"blocks","created_at":"2026-02-14T23:07:29.234192+01:00","created_by":"daemon"}]}
{"id":"engram-wii","title":"Add config/event schema foundations (system_config, memory_policies, memory_events, agent identity fields)","description":"Implement schema primitives required by the plan: system_config table for prompt-native behavior, memory_policies for scope overrides, and memory_events for reactive propagation. Extend agents with identity context fields used during context injection. Add indexes exactly as needed for lookup paths (by_key, by_category, by_scope_key, by_created/by_scope_created). Include migration-safe defaults and verification queries proving no existing fact/entity/agent records are invalidated.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-15T00:33:52.882727+01:00","updated_at":"2026-02-15T00:45:34.331922+01:00","closed_at":"2026-02-15T00:45:34.331922+01:00","close_reason":"Closed"}
{"id":"engram-wxn","title":"Tests: Export, file watcher, round-trip sync e2e","status":"open","priority":3,"issue_type":"task","created_at":"2026-02-25T14:05:01.423015+01:00","updated_at":"2026-02-25T14:05:01.423015+01:00","dependencies":[{"issue_id":"engram-wxn","depends_on_id":"engram-1vt","type":"blocks","created_at":"2026-02-25T14:05:45.521843+01:00","created_by":"daemon"}]}
{"id":"engram-wzv","title":"Phase 6: Migration + Polish — import, crons, benchmarks","status":"closed","priority":0,"issue_type":"epic","created_at":"2026-02-12T10:02:09.557812+01:00","updated_at":"2026-02-12T10:07:30.097634+01:00","closed_at":"2026-02-12T10:07:30.097634+01:00","close_reason":"Completed"}
{"id":"engram-x95","title":"Rust/WASM acceleration scaffold (deferred)","description":"Track deferred rust/wasm acceleration milestone from specs/UNIMPLEMENTED.md.","status":"closed","priority":4,"issue_type":"task","created_at":"2026-02-14T23:53:28.300356+01:00","updated_at":"2026-02-14T23:54:17.055068+01:00","closed_at":"2026-02-14T23:54:17.055068+01:00","close_reason":"Closed"}
{"id":"engram-ykh","title":"Entity backlinks + graph edge consistency","description":"Entity backlinks consistency and graph edge validation:\n\n**This task focuses on maintaining consistency between:**\n1. Fact content wiki-links [[Entity]]\n2. Entity backlinks array in entities table\n3. Fact entities array field\n4. Graph export edges\n\n**Implementation (convex/functions/entities.ts):**\n\n1. updateBacklinks(factId, entityNames):\n   - For each entity name in the fact\n   - Find entity by name\n   - Add to entity.backlinks if not exists: {factId, factType, linkedAt}\n   - Remove from backlinks if entity no longer mentioned\n\n2. validateBacklinks() — Periodic validation:\n   - For each entity, check backlinks array\n   - Verify each backlinked fact still exists and mentions entity\n   - Remove stale backlinks (fact deleted or entity removed)\n   - Log inconsistencies\n\n3. rebuildBacklinks() — Full rebuild:\n   - Clear all entity.backlinks arrays\n   - Scan all active facts\n   - Extract wiki-links from content\n   - Rebuild backlinks from scratch\n   - Use case: Fix inconsistencies after bugs\n\n**Integration points:**\n- storeFact: call updateBacklinks after insert\n- updateFact: call updateBacklinks after update\n- deleteFact: call updateBacklinks to remove\n\n**Convex cron (optional):**\n- Schedule validateBacklinks() daily\n- Fix inconsistencies automatically\n- Log warnings for manual review\n\n**Tests:** backlink-consistency.test.ts (store/update/delete maintains consistency), backlink-validation.test.ts (detect and fix stale backlinks)\n**Performance:** Update backlinks \u003c50ms, full rebuild \u003c30s for 10k entities\nRef: VAULT_INTEGRATION_PLAN.md Phase 4.4","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-14T23:00:31.71662+01:00","updated_at":"2026-02-14T23:25:09.08173+01:00","closed_at":"2026-02-14T23:25:09.08173+01:00","close_reason":"Closed","dependencies":[{"issue_id":"engram-ykh","depends_on_id":"engram-43e","type":"blocks","created_at":"2026-02-14T23:01:58.67423+01:00","created_by":"daemon"}]}
{"id":"engram-yu1","title":"Reflection notifications to active agents","description":"After reflection produces new facts, notify subscribed agents via the notification system. Agents see what was learned while they were idle.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-25T14:14:33.351326+01:00","updated_at":"2026-02-25T14:26:55.411185+01:00","closed_at":"2026-02-25T14:26:55.411185+01:00","close_reason":"Closed"}
{"id":"engram-ywx","title":"Schema changes: add vault mirror fields to facts table","description":"Add vault mirror fields to convex/schema.ts facts table:\n\n**New fields:**\n1. vaultPath (v.optional(v.string())) — Relative path in vault (e.g., \"private-indy/decisions/2026-02-14-foo.md\")\n2. vaultSyncedAt (v.optional(v.number())) — Last sync timestamp\n3. confidence (v.optional(v.float64())) — 0.0-1.0 confidence score (from spec §1.4)\n4. importanceTier (v.optional(v.string())) — \"structural\"|\"potential\"|\"contextual\"\n5. observationTier (v.optional(v.string())) — \"critical\"|\"notable\"|\"background\" (flat field, no dot notation)\n6. observationCompressed (v.optional(v.boolean())) — Whether background compression ran\n7. observationOriginalContent (v.optional(v.string())) — Content before compression\n\n**New indices:**\n- .index(\"by_vault_path\", [\"vaultPath\"])\n- .index(\"by_vault_synced\", [\"vaultSyncedAt\"])\n- .index(\"by_observation_tier\", [\"observationTier\", \"timestamp\"])\n- .index(\"unmirrored\", [\"vaultPath\", \"lifecycleState\"])\n\nNOTE: Convex does not support dot-notation field names. All observation.* fields from the original plan are flattened to observationTier, observationCompressed, observationOriginalContent.\nNOTE: facts table uses \"timestamp\" not \"createdAt\", and \"lifecycleState\" not \"status\".\n\nPerformance target: Schema migration \u003c1s, zero downtime.\nRef: specs/obsidian-mirror-plan.md §1.4, VAULT_INTEGRATION_PLAN.md Phase 1.3.1, Phase 2.8, Phase 6.8","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-14T22:56:07.968133+01:00","updated_at":"2026-02-14T23:24:12.789753+01:00","closed_at":"2026-02-14T23:24:12.789753+01:00","close_reason":"Closed"}
{"id":"engram-zcq","title":"BUG: incrementalUpdate Welford variance formula overcounts old variance","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-02-25T11:43:35.266326+01:00","updated_at":"2026-02-25T14:20:54.990227+01:00","closed_at":"2026-02-25T14:20:54.990227+01:00","close_reason":"Closed"}
