Base directory for this skill: /Users/cortex-air/.claude/skills/beads-workflow

# Beads Workflow — From Plan to Actionable Tasks

> **Core Principle:** "Check your beads N times, implement once" — where N is as many as you can stomach.
>
> Beads are so detailed and polished that you can mechanically unleash a big swarm of agents to implement them, and it will come out just about perfectly.

---

## What Are Beads?

Beads are **epics/tasks/subtasks with dependency structure**, optimized for AI coding agents. Think of them as Jira or Linear, but designed for machines.

Key properties:
- **Self-contained** — Never need to refer back to the original markdown plan
- **Self-documenting** — Include background, reasoning, justifications, considerations
- **Dependency-aware** — Explicit structure of what blocks what
- **Rich descriptions** — Long markdown comments, not short bullet points

---

## Why Beads Work

```
┌─────────────────────────────────────────────────────────────┐
│ MARKDOWN PLAN (~3,500 lines)                                │
│   └─► Fits in context window                                │
│   └─► Models reason about entire system at once             │
├─────────────────────────────────────────────────────────────┤
│                    ↓ CONVERT TO BEADS ↓                     │
├─────────────────────────────────────────────────────────────┤
│ BEADS (distributed tasks)                                   │
│   └─► Each bead is self-contained                           │
│   └─► Any agent can pick up any bead                        │
│   └─► BV (Beads Viewer) handles prioritization              │
│   └─► Agent Mail handles coordination                       │
└─────────────────────────────────────────────────────────────┘
```

---

## Converting Plans to Beads

### THE EXACT PROMPT — Plan to Beads Conversion (Claude Code + Opus 4.5)

```
OK so now read ALL of PLAN_TO_CREATE_GH_PAGES_WEB_EXPORT_APP.md; please take ALL of that and elaborate on it and use it to create a comprehensive and granular set of beads for all this with tasks, subtasks, and dependency structure overlaid, with detailed comments so that the whole thing is totally self-contained and self-documenting (including relevant background, reasoning/justification, considerations, etc.-- anything we'd want our "future self" to know about the goals and intentions and thought process and how it serves the over-arching goals of the project.). The beads should be so detailed that we never need to consult back to the original markdown plan document. Remember to ONLY use the `bd` tool to create and modify the beads and add the dependencies. Use ultrathink.
```

**Note:** Replace `PLAN_TO_CREATE_GH_PAGES_WEB_EXPORT_APP.md` with your actual plan filename.

### Alternative Shorter Version

```
OK so please take ALL of that and elaborate on it more and then create a comprehensive and granular set of beads for all this with tasks, subtasks, and dependency structure overlaid, with detailed comments so that the whole thing is totally self-contained and self-documenting (including relevant background, reasoning/justification, considerations, etc.-- anything we'd want our "future self" to know about the goals and intentions and thought process and how it serves the over-arching goals of the project.)  Use only the `bd` tool to create and modify the beads and add the dependencies. Use ultrathink.
```

### What This Creates

- Tasks and subtasks with clear scope
- Dependency links (what must complete before what)
- Detailed descriptions with:
  - Background context
  - Reasoning and justification
  - Technical considerations
  - How it serves project goals

---

## Polishing Beads

### Why Polish?

Even after initial conversion, beads continue to improve with review. You get incremental improvements even at round 6+.

### THE EXACT PROMPT — Polish Beads (Full Version)

```
Reread AGENTS dot md so it's still fresh in your mind. Then read ALL of PLAN_TO_CREATE_GH_PAGES_WEB_EXPORT_APP.md . Use ultrathink. Check over each bead super carefully-- are you sure it makes sense? Is it optimal? Could we change anything to make the system work better for users? If so, revise the beads. It's a lot easier and faster to operate in "plan space" before we start implementing these things! DO NOT OVERSIMPLIFY THINGS! DO NOT LOSE ANY FEATURES OR FUNCTIONALITY! Also make sure that as part of the beads we include comprehensive unit tests and e2e test scripts with great, detailed logging so we can be sure that everything is working perfectly after implementation. It's critical that EVERYTHING from the markdown plan be embedded into the beads so that we never need to refer back to the markdown plan and we don't lose any important context or ideas or insights into the new features planned and why we are making them.
```

### THE EXACT PROMPT — Polish Beads (Standard Version)

```
Reread AGENTS dot md so it's still fresh in your mind. Check over each bead super carefully-- are you sure it makes sense? Is it optimal? Could we change anything to make the system work better for users? If so, revise the beads. It's a lot easier and faster to operate in "plan space" before we start implementing these things!

DO NOT OVERSIMPLIFY THINGS! DO NOT LOSE ANY FEATURES OR FUNCTIONALITY!

Also, make sure that as part of these beads, we include comprehensive unit tests and e2e test scripts with great, detailed logging so we can be sure that everything is working perfectly after implementation. Remember to ONLY use the `bd` tool to create and modify the beads and to add the dependencies to beads. Use ultrathink.
```

### Polishing Protocol

1. Run the polishing prompt
2. Review changes
3. Repeat until steady-state (typically 6-9 rounds)
4. If it flatlines, start a fresh CC session
5. Optionally have Codex with GPT 5.2 do a final round

---

## Fresh Session Technique

If polishing starts to flatline, start a brand new Claude Code session:

### THE EXACT PROMPT — Re-establish Context

```
First read ALL of the AGENTS dot md file and README dot md file super carefully and understand ALL of both! Then use your code investigation agent mode to fully understand the code, and technical architecture and purpose of the project.  Use ultrathink.
```

### THE EXACT PROMPT — Then Review Beads

```
We recently transformed a markdown plan file into a bunch of new beads. I want you to very carefully review and analyze these using `bd` and `bv`.
```

Then follow up with the standard polish prompt.

---

## Cross-Model Review

For extra polish, have different models review the beads:

| Model | Strength |
|-------|----------|
| **Claude Code + Opus 4.5** | Primary creation and refinement |
| **Codex + GPT 5.2** | Final review pass |
| **Gemini CLI** | Alternative perspective |

---

## Bead Quality Checklist

Before implementation, verify each bead:

- [ ] **Self-contained** — Can be understood without external context
- [ ] **Clear scope** — One coherent piece of work
- [ ] **Dependencies explicit** — Links to blocking/blocked beads
- [ ] **Testable** — Clear success criteria
- [ ] **Includes tests** — Unit tests and e2e tests in scope
- [ ] **Preserves features** — Nothing from the plan was lost
- [ ] **Not oversimplified** — Complexity preserved where needed

---

## Using bd (Beads CLI)

### Basic Commands

```bash
# Initialize beads in project
bd init

# Create a new bead
bd create "Implement user authentication" -t feature -p 1

# Add dependencies
bd depend BD-123 BD-100  # BD-123 depends on BD-100

# Update status
bd update BD-123 --status in_progress

# Close a bead
bd close BD-123 --reason "Completed and tested"

# List ready beads (no blockers)
bd ready --json
```

### Robot Mode for Agents

```bash
# Get triage recommendations
bv --robot-triage

# Get the single top pick
bv --robot-next

# Get parallel execution tracks
bv --robot-plan

# Get graph insights (PageRank, bottlenecks, cycles)
bv --robot-insights
```

**CRITICAL:** Never run bare `bv` — it launches interactive TUI. Always use `--robot-*` flags.

---

## Integration with Agent Mail

### Conventions

- Use Beads issue ID as Mail `thread_id`: `send_message(..., thread_id="bd-123")`
- Prefix subjects: `[bd-123] Starting auth refactor`
- Include issue ID in file reservation `reason`: `file_reservation_paths(..., reason="bd-123")`

### Typical Flow

```bash
# 1. Pick ready work
bd ready --json

# 2. Reserve files
file_reservation_paths(project_key, agent_name, ["src/**"], reason="bd-123")

# 3. Announce start
send_message(..., thread_id="bd-123", subject="[bd-123] Starting work")

# 4. Work on the bead
# ...

# 5. Complete
bd close bd-123 --reason "Completed"
release_file_reservations(project_key, agent_name)
```

---

## Test Coverage Beads

### THE EXACT PROMPT — Add Test Coverage

```
Do we have full unit test coverage without using mocks/fake stuff? What about complete e2e integration test scripts with great, detailed logging? If not, then create a comprehensive and granular set of beads for all this with tasks, subtasks, and dependency structure overlaid with detailed comments.
```

---

## When Beads Are Ready

Your beads are ready for implementation when:

1. **Steady-state reached** — Multiple polishing rounds yield minimal changes
2. **Cross-model reviewed** — At least one alternative model has reviewed
3. **No cycles** — `bv --robot-insights | jq '.Cycles'` returns empty
4. **Tests included** — Each feature bead has associated test beads
5. **Dependencies clean** — Graph makes logical sense

---

## Example Bead Structure

A well-formed bead looks like:

```
ID: BD-123
Title: Implement OAuth2 login flow
Type: feature
Priority: P1
Status: open

Dependencies: [BD-100 (User model), BD-101 (Session management)]
Blocks: [BD-200 (Protected routes), BD-201 (User dashboard)]

Description:
Implement OAuth2 login flow supporting Google and GitHub providers.

## Background
This is the primary authentication mechanism for the application.
Users should be able to sign in with existing Google/GitHub accounts
to reduce friction.

## Technical Approach
- Use NextAuth.js for OAuth2 implementation
- Store provider tokens encrypted in Supabase
- Create unified user record on first login
- Handle account linking for multiple providers

## Success Criteria
- User can click "Sign in with Google/GitHub"
- OAuth flow completes and redirects to dashboard
- User record created/updated in database
- Session cookie set correctly
- Logout clears session properly

## Test Plan
- Unit: Token encryption/decryption
- Unit: User record creation
- E2E: Full OAuth flow (mock provider)
- E2E: Account linking scenario

## Considerations
- Handle provider API rate limits
- Graceful degradation if provider is down
- GDPR compliance for EU users
```

---

## Complete Prompt Reference

### Plan to Beads (Full)
```
OK so now read ALL of PLAN_TO_CREATE_GH_PAGES_WEB_EXPORT_APP.md; please take ALL of that and elaborate on it and use it to create a comprehensive and granular set of beads for all this with tasks, subtasks, and dependency structure overlaid, with detailed comments so that the whole thing is totally self-contained and self-documenting (including relevant background, reasoning/justification, considerations, etc.-- anything we'd want our "future self" to know about the goals and intentions and thought process and how it serves the over-arching goals of the project.). The beads should be so detailed that we never need to consult back to the original markdown plan document. Remember to ONLY use the `bd` tool to create and modify the beads and add the dependencies. Use ultrathink.
```

### Plan to Beads (Short)
```
OK so please take ALL of that and elaborate on it more and then create a comprehensive and granular set of beads for all this with tasks, subtasks, and dependency structure overlaid, with detailed comments so that the whole thing is totally self-contained and self-documenting (including relevant background, reasoning/justification, considerations, etc.-- anything we'd want our "future self" to know about the goals and intentions and thought process and how it serves the over-arching goals of the project.)  Use only the `bd` tool to create and modify the beads and add the dependencies. Use ultrathink.
```

### Polish Beads (Full)
```
Reread AGENTS dot md so it's still fresh in your mind. Then read ALL of PLAN_TO_CREATE_GH_PAGES_WEB_EXPORT_APP.md . Use ultrathink. Check over each bead super carefully-- are you sure it makes sense? Is it optimal? Could we change anything to make the system work better for users? If so, revise the beads. It's a lot easier and faster to operate in "plan space" before we start implementing these things! DO NOT OVERSIMPLIFY THINGS! DO NOT LOSE ANY FEATURES OR FUNCTIONALITY! Also make sure that as part of the beads we include comprehensive unit tests and e2e test scripts with great, detailed logging so we can be sure that everything is working perfectly after implementation. It's critical that EVERYTHING from the markdown plan be embedded into the beads so that we never need to refer back to the markdown plan and we don't lose any important context or ideas or insights into the new features planned and why we are making them.
```

### Polish Beads (Standard)
```
Reread AGENTS dot md so it's still fresh in your mind. Check over each bead super carefully-- are you sure it makes sense? Is it optimal? Could we change anything to make the system work better for users? If so, revise the beads. It's a lot easier and faster to operate in "plan space" before we start implementing these things!

DO NOT OVERSIMPLIFY THINGS! DO NOT LOSE ANY FEATURES OR FUNCTIONALITY!

Also, make sure that as part of these beads, we include comprehensive unit tests and e2e test scripts with great, detailed logging so we can be sure that everything is working perfectly after implementation. Remember to ONLY use the `bd` tool to create and modify the beads and to add the dependencies to beads. Use ultrathink.
```

### Fresh Session — Context
```
First read ALL of the AGENTS dot md file and README dot md file super carefully and understand ALL of both! Then use your code investigation agent mode to fully understand the code, and technical architecture and purpose of the project.  Use ultrathink.
```

### Fresh Session — Review
```
We recently transformed a markdown plan file into a bunch of new beads. I want you to very carefully review and analyze these using `bd` and `bv`.
```

### Add Test Coverage
```
Do we have full unit test coverage without using mocks/fake stuff? What about complete e2e integration test scripts with great, detailed logging? If not, then create a comprehensive and granular set of beads for all this with tasks, subtasks, and dependency structure overlaid with detailed comments.
```

---

## Common Mistakes

1. **Oversimplifying** — Preserve complexity where it's needed
2. **Losing features** — Every plan feature should become beads
3. **Skipping tests** — Include unit and e2e test beads
4. **Single review** — Keep polishing until truly steady-state
5. **Missing dependencies** — Make all blocking relationships explicit
6. **Short descriptions** — Beads should be verbose and self-documenting


ARGUMENTS:   - docs/plans/2026-02-27-feat-cross-agent-engram-mcp-enforcement-plan.md:1

---

Base directory for this skill: /Users/cortex-air/.claude/plugins/cache/every-marketplace/compound-engineering/2.30.0/skills/brainstorming

# Brainstorming

This skill provides detailed process knowledge for effective brainstorming sessions that clarify **WHAT** to build before diving into **HOW** to build it.

## When to Use This Skill

Brainstorming is valuable when:
- Requirements are unclear or ambiguous
- Multiple approaches could solve the problem
- Trade-offs need to be explored with the user
- The user hasn't fully articulated what they want
- The feature scope needs refinement

Brainstorming can be skipped when:
- Requirements are explicit and detailed
- The user knows exactly what they want
- The task is a straightforward bug fix or well-defined change

## Core Process

### Phase 0: Assess Requirement Clarity

Before diving into questions, assess whether brainstorming is needed.

**Signals that requirements are clear:**
- User provided specific acceptance criteria
- User referenced existing patterns to follow
- User described exact behavior expected
- Scope is constrained and well-defined

**Signals that brainstorming is needed:**
- User used vague terms ("make it better", "add something like")
- Multiple reasonable interpretations exist
- Trade-offs haven't been discussed
- User seems unsure about the approach

If requirements are clear, suggest: "Your requirements seem clear. Consider proceeding directly to planning or implementation."

### Phase 1: Understand the Idea

Ask questions **one at a time** to understand the user's intent. Avoid overwhelming with multiple questions.

**Question Techniques:**

1. **Prefer multiple choice when natural options exist**
   - Good: "Should the notification be: (a) email only, (b) in-app only, or (c) both?"
   - Avoid: "How should users be notified?"

2. **Start broad, then narrow**
   - First: What is the core purpose?
   - Then: Who are the users?
   - Finally: What constraints exist?

3. **Validate assumptions explicitly**
   - "I'm assuming users will be logged in. Is that correct?"

4. **Ask about success criteria early**
   - "How will you know this feature is working well?"

**Key Topics to Explore:**

| Topic | Example Questions |
|-------|-------------------|
| Purpose | What problem does this solve? What's the motivation? |
| Users | Who uses this? What's their context? |
| Constraints | Any technical limitations? Timeline? Dependencies? |
| Success | How will you measure success? What's the happy path? |
| Edge Cases | What shouldn't happen? Any error states to consider? |
| Existing Patterns | Are there similar features in the codebase to follow? |

**Exit Condition:** Continue until the idea is clear OR user says "proceed" or "let's move on"

### Phase 2: Explore Approaches

After understanding the idea, propose 2-3 concrete approaches.

**Structure for Each Approach:**

```markdown
### Approach A: [Name]

[2-3 sentence description]

**Pros:**
- [Benefit 1]
- [Benefit 2]

**Cons:**
- [Drawback 1]
- [Drawback 2]

**Best when:** [Circumstances where this approach shines]
```

**Guidelines:**
- Lead with a recommendation and explain why
- Be honest about trade-offs
- Consider YAGNI—simpler is usually better
- Reference codebase patterns when relevant

### Phase 3: Capture the Design

Summarize key decisions in a structured format.

**Design Doc Structure:**

```markdown
---
date: YYYY-MM-DD
topic: <kebab-case-topic>
---

# <Topic Title>

## What We're Building
[Concise description—1-2 paragraphs max]

## Why This Approach
[Brief explanation of approaches considered and why this one was chosen]

## Key Decisions
- [Decision 1]: [Rationale]
- [Decision 2]: [Rationale]

## Open Questions
- [Any unresolved questions for the planning phase]

## Next Steps
→ `/workflows:plan` for implementation details
```

**Output Location:** `docs/brainstorms/YYYY-MM-DD-<topic>-brainstorm.md`

### Phase 4: Handoff

Present clear options for what to do next:

1. **Proceed to planning** → Run `/workflows:plan`
2. **Refine further** → Continue exploring the design
3. **Done for now** → User will return later

## YAGNI Principles

During brainstorming, actively resist complexity:

- **Don't design for hypothetical future requirements**
- **Choose the simplest approach that solves the stated problem**
- **Prefer boring, proven patterns over clever solutions**
- **Ask "Do we really need this?" when complexity emerges**
- **Defer decisions that don't need to be made now**

## Incremental Validation

Keep sections short—200-300 words maximum. After each section of output, pause to validate understanding:

- "Does this match what you had in mind?"
- "Any adjustments before we continue?"
- "Is this the direction you want to go?"

This prevents wasted effort on misaligned designs.

## Anti-Patterns to Avoid

| Anti-Pattern | Better Approach |
|--------------|-----------------|
| Asking 5 questions at once | Ask one at a time |
| Jumping to implementation details | Stay focused on WHAT, not HOW |
| Proposing overly complex solutions | Start simple, add complexity only if needed |
| Ignoring existing codebase patterns | Research what exists first |
| Making assumptions without validating | State assumptions explicitly and confirm |
| Creating lengthy design documents | Keep it concise—details go in the plan |

## Integration with Planning

Brainstorming answers **WHAT** to build:
- Requirements and acceptance criteria
- Chosen approach and rationale
- Key decisions and trade-offs

Planning answers **HOW** to build it:
- Implementation steps and file changes
- Technical details and code patterns
- Testing strategy and verification

When brainstorm output exists, `/workflows:plan` should detect it and use it as input, skipping its own idea refinement phase.


ARGUMENTS: how can we implement obsidian and qmd

---

proceed

---

lets start implementing using the /agent-swarm-workflow

---

Base directory for this skill: /Users/cortex-air/.claude/skills/agent-swarm-workflow

# Agent Swarm Workflow — Parallel Implementation

> **Core Insight:** Every agent is fungible and a generalist. They all use the same base model and read the same AGENTS.md. Simply telling one it's a "frontend agent" doesn't make it better at frontend.
>
> The swarm is distributed, robust, and self-organizing through Agent Mail and Beads.

---

## Prerequisites

Before starting a swarm:

1. **Comprehensive plan** created (see `planning-workflow` skill)
2. **Polished beads** ready (see `beads-workflow` skill)
3. **AGENTS.md** configured with all tool blurbs
4. **Agent Mail server** running (`am` or `~/projects/mcp_agent_mail/scripts/run_server_with_token.sh`)
5. **NTM** available for session management

---

## The Swarm Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                         BEADS                               │
│     (Task graph with dependencies, priorities, status)      │
└─────────────────────────────────────────────────────────────┘
                              │
                    ┌─────────┴─────────┐
                    ▼                   ▼
┌─────────────────────────────┐  ┌─────────────────────────┐
│        BV                   │  │     AGENT MAIL          │
│  (What to work on)          │  │  (Coordination layer)   │
└─────────────────────────────┘  └─────────────────────────┘
         │                            │
         └──────────────┬─────────────┘
                        ▼
┌─────────────────────────────────────────────────────────────┐
│                    NTM + AGENTS                             │
│  ┌─────┐  ┌─────┐  ┌─────┐  ┌─────┐  ┌─────┐  ┌─────┐     │
│  │ CC  │  │ CC  │  │ Cod │  │ Gmi │  │ CC  │  │ Cod │     │
│  └─────┘  └─────┘  └─────┘  └─────┘  └─────┘  └─────┘     │
└─────────────────────────────────────────────────────────────┘
```

---

## Starting the Swarm

### Using NTM (Named Tmux Manager)

```bash
# Spawn a swarm with multiple agents
ntm spawn myproject --cc=3 --cod=2 --gmi=1

# Send initial prompt to all Claude Code agents
ntm send myproject --cc "$(cat initial_prompt.txt)"

# Or send to all agents
ntm send myproject --all "$(cat initial_prompt.txt)"
```

### Manual Setup

Create tmux sessions/panes for each agent in your project folder.

---

## THE EXACT PROMPT — Initial Agent Marching Orders

Give each agent this EXACT prompt to start:

```
First read ALL of the AGENTS dot md file and README dot md file super carefully and understand ALL of both! Then use your code investigation agent mode to fully understand the code, and technical architecture and purpose of the project. Then register with MCP Agent Mail and introduce yourself to the other agents.

Be sure to check your agent mail and to promptly respond if needed to any messages; then proceed meticulously with your next assigned beads, working on the tasks systematically and meticulously and tracking your progress via beads and agent mail messages.

Don't get stuck in "communication purgatory" where nothing is getting done; be proactive about starting tasks that need to be done, but inform your fellow agents via messages when you do so and mark beads appropriately.

When you're not sure what to do next, use the bv tool mentioned in AGENTS dot md to prioritize the best beads to work on next; pick the next one that you can usefully work on and get started. Make sure to acknowledge all communication requests from other agents and that you are aware of all active agents and their names.  Use ultrathink.
```

---

## The Implementation Loop

### THE EXACT PROMPT — Move to Next Bead

Once agents complete a bead, use this prompt to keep them moving:

```
Reread AGENTS dot md so it's still fresh in your mind.   Use ultrathink.   Use bv with the robot flags (see AGENTS dot md for info on this) to find the most impactful bead(s) to work on next and then start on it. Remember to mark the beads appropriately and communicate with your fellow agents. Pick the next bead you can actually do usefully now and start coding on it immediately; communicate what you're working on to your fellow agents and mark beads appropriately as you work. And respond to any agent mail messages you've received.
```

### THE EXACT PROMPT — Self-Review After Bead Completion

Have agents review their work before moving on:

```
Great, now I want you to carefully read over all of the new code you just wrote and other existing code you just modified with "fresh eyes" looking super carefully for any obvious bugs, errors, problems, issues, confusion, etc. Carefully fix anything you uncover. Use ultrathink.
```

**Keep running this until they stop finding bugs.**

---

## Handling Context Compaction

### THE EXACT PROMPT — Post-Compaction

When an agent does a compaction, immediately follow up with:

```
Reread AGENTS dot md so it's still fresh in your mind.   Use ultrathink.
```

This re-establishes the critical context about tools and workflows.

---

## Quality Review Prompts

### THE EXACT PROMPT — Cross-Agent Review

Periodically have agents review each other's work:

```
Ok can you now turn your attention to reviewing the code written by your fellow agents and checking for any issues, bugs, errors, problems, inefficiencies, security problems, reliability issues, etc. and carefully diagnose their underlying root causes using first-principle analysis and then fix or revise them if necessary? Don't restrict yourself to the latest commits, cast a wider net and go super deep! Use ultrathink.
```

### THE EXACT PROMPT — Random Code Exploration

For deep quality checks:

```
I want you to sort of randomly explore the code files in this project, choosing code files to deeply investigate and understand and trace their functionality and execution flows through the related code files which they import or which they are imported by.

Once you understand the purpose of the code in the larger context of the workflows, I want you to do a super careful, methodical, and critical check with "fresh eyes" to find any obvious bugs, problems, errors, issues, silly mistakes, etc. and then systematically and meticulously and intelligently correct them.

Be sure to comply with ALL rules in AGENTS dot md and ensure that any code you write or revise conforms to the best practice guides referenced in the AGENTS dot md file. Use ultrathink.
```

---

## Committing Work

### THE EXACT PROMPT — Commit Changes

Have agents commit logically grouped changes:

```
Now, based on your knowledge of the project, commit all changed files now in a series of logically connected groupings with super detailed commit messages for each and then push. Take your time to do it right. Don't edit the code at all. Don't commit obviously ephemeral files. Use ultrathink.
```

---

## Post-Bead Completion Prompts

### THE EXACT PROMPT — Add Test Coverage

```
Do we have full unit test coverage without using mocks/fake stuff? What about complete e2e integration test scripts with great, detailed logging? If not, then create a comprehensive and granular set of beads for all this with tasks, subtasks, and dependency structure overlaid with detailed comments.
```

### THE EXACT PROMPT — UI/UX Scrutiny

```
Great, now I want you to super carefully scrutinize every aspect of the application workflow and implementation and look for things that just seem sub-optimal or even wrong/mistaken to you, things that could very obviously be improved from a user-friendliness and intuitiveness standpoint, places where our UI/UX could be improved and polished to be slicker, more visually appealing, and more premium feeling and just ultra high quality, like Stripe-level apps.
```

### THE EXACT PROMPT — Deep UI/UX Enhancement

```
I still think there are strong opportunities to enhance the UI/UX look and feel and to make everything work better and be more intuitive, user-friendly, visually appealing, polished, slick, and world class in terms of following UI/UX best practices like those used by Stripe, don't you agree? And I want you to carefully consider desktop UI/UX and mobile UI/UX separately while doing this and hyper-optimize for both separately to play to the specifics of each modality. I'm looking for true world-class visual appeal, polish, slickness, etc. that makes people gasp at how stunning and perfect it is in every way.  Use ultrathink.
```

---

## Agent Mail Integration

### How Agents Coordinate

Each agent:
1. **Registers** with Agent Mail at session start
2. **Reserves files** before editing (`file_reservation_paths`)
3. **Announces work** via messages with bead ID in `thread_id`
4. **Checks inbox** between tasks
5. **Releases reservations** when done

### File Reservation Pattern

```python
# Before starting work on a bead
file_reservation_paths(
    project_key="/path/to/project",
    agent_name="GreenCastle",
    paths=["src/auth/**/*.ts"],
    ttl_seconds=3600,
    exclusive=True,
    reason="bd-123"
)

# After completing work
release_file_reservations(project_key, agent_name)
```

### Communication Pattern

```python
# Announce starting a bead
send_message(
    project_key, agent_name,
    to=["BlueLake", "RedMountain"],  # Other active agents
    subject="[bd-123] Starting auth module",
    body_md="I'm taking bd-123. Reserved src/auth/**.",
    thread_id="bd-123"
)

# Update on completion
send_message(
    project_key, agent_name,
    to=["BlueLake", "RedMountain"],
    subject="[bd-123] Completed",
    body_md="Auth module done and tested. Released file reservations.",
    thread_id="bd-123"
)
```

---

## Using BV for Task Selection

### Key Commands

```bash
# THE MEGA-COMMAND: Start here
bv --robot-triage

# Just get the single top pick
bv --robot-next

# Get parallel execution tracks (for multi-agent)
bv --robot-plan

# Check for cycles (MUST FIX if found)
bv --robot-insights | jq '.Cycles'

# Find bottlenecks
bv --robot-insights | jq '.bottlenecks'
```

**CRITICAL:** Never run bare `bv` — it launches interactive TUI that blocks the session.

---

## Quality Loops

### Run Until Clean

Keep running these prompts in rounds until they consistently come back with no changes:

1. **Self-review** — Agent reviews own code
2. **Cross-review** — Agent reviews other agents' code
3. **Random exploration** — Deep dive into random code paths

### The Steady-State Signal

When all three types of reviews return clean (no bugs found, no changes made), the code is likely solid.

---

## Complete Prompt Reference

### Initial Marching Orders
```
First read ALL of the AGENTS dot md file and README dot md file super carefully and understand ALL of both! Then use your code investigation agent mode to fully understand the code, and technical architecture and purpose of the project. Then register with MCP Agent Mail and introduce yourself to the other agents.

Be sure to check your agent mail and to promptly respond if needed to any messages; then proceed meticulously with your next assigned beads, working on the tasks systematically and meticulously and tracking your progress via beads and agent mail messages.

Don't get stuck in "communication purgatory" where nothing is getting done; be proactive about starting tasks that need to be done, but inform your fellow agents via messages when you do so and mark beads appropriately.

When you're not sure what to do next, use the bv tool mentioned in AGENTS dot md to prioritize the best beads to work on next; pick the next one that you can usefully work on and get started. Make sure to acknowledge all communication requests from other agents and that you are aware of all active agents and their names.  Use ultrathink.
```

### Move to Next Bead
```
Reread AGENTS dot md so it's still fresh in your mind.   Use ultrathink.   Use bv with the robot flags (see AGENTS dot md for info on this) to find the most impactful bead(s) to work on next and then start on it. Remember to mark the beads appropriately and communicate with your fellow agents. Pick the next bead you can actually do usefully now and start coding on it immediately; communicate what you're working on to your fellow agents and mark beads appropriately as you work. And respond to any agent mail messages you've received.
```

### Self-Review
```
Great, now I want you to carefully read over all of the new code you just wrote and other existing code you just modified with "fresh eyes" looking super carefully for any obvious bugs, errors, problems, issues, confusion, etc. Carefully fix anything you uncover. Use ultrathink.
```

### Cross-Review
```
Ok can you now turn your attention to reviewing the code written by your fellow agents and checking for any issues, bugs, errors, problems, inefficiencies, security problems, reliability issues, etc. and carefully diagnose their underlying root causes using first-principle analysis and then fix or revise them if necessary? Don't restrict yourself to the latest commits, cast a wider net and go super deep! Use ultrathink.
```

### Random Exploration
```
I want you to sort of randomly explore the code files in this project, choosing code files to deeply investigate and understand and trace their functionality and execution flows through the related code files which they import or which they are imported by.

Once you understand the purpose of the code in the larger context of the workflows, I want you to do a super careful, methodical, and critical check with "fresh eyes" to find any obvious bugs, problems, errors, issues, silly mistakes, etc. and then systematically and meticulously and intelligently correct them.

Be sure to comply with ALL rules in AGENTS dot md and ensure that any code you write or revises conforms to the best practice guides referenced in the AGENTS dot md file. Use ultrathink.
```

### Post-Compaction
```
Reread AGENTS dot md so it's still fresh in your mind.   Use ultrathink.
```

### Commit Changes
```
Now, based on your knowledge of the project, commit all changed files now in a series of logically connected groupings with super detailed commit messages for each and then push. Take your time to do it right. Don't edit the code at all. Don't commit obviously ephemeral files. Use ultrathink.
```

### Test Coverage
```
Do we have full unit test coverage without using mocks/fake stuff? What about complete e2e integration test scripts with great, detailed logging? If not, then create a comprehensive and granular set of beads for all this with tasks, subtasks, and dependency structure overlaid with detailed comments.
```

### UI/UX Scrutiny
```
Great, now I want you to super carefully scrutinize every aspect of the application workflow and implementation and look for things that just seem sub-optimal or even wrong/mistaken to you, things that could very obviously be improved from a user-friendliness and intuitiveness standpoint, places where our UI/UX could be improved and polished to be slicker, more visually appealing, and more premium feeling and just ultra high quality, like Stripe-level apps.
```

### Deep UI/UX Enhancement
```
I still think there are strong opportunities to enhance the UI/UX look and feel and to make everything work better and be more intuitive, user-friendly, visually appealing, polished, slick, and world class in terms of following UI/UX best practices like those used by Stripe, don't you agree? And I want you to carefully consider desktop UI/UX and mobile UI/UX separately while doing this and hyper-optimize for both separately to play to the specifics of each modality. I'm looking for true world-class visual appeal, polish, slickness, etc. that makes people gasp at how stunning and perfect it is in every way.  Use ultrathink.
```

---

## The Flywheel in Action

```
PLAN ──► BEADS ──► SWARM ──► REVIEW ──► COMMIT
  │                  │          │         │
  │                  │          └────┬────┘
  │                  │               │
  │                  └───── REPEAT ──┘
  │                              │
  │         v2 PLAN ◄────────────┘
  │              │
  └──────────────┘
```

Each cycle improves:
- **CASS** remembers solutions
- **CM** distills patterns
- **UBS** catches more issues
- **BV** shows graph health

---

## FAQ

**Q: How do agents know what to work on?**
A: They use `bv --robot-triage` or `bv --robot-next` to find the highest-impact ready bead.

**Q: How do they avoid conflicts?**
A: File reservations in Agent Mail. Exclusive reservations block others; the pre-commit guard enforces it.

**Q: What if an agent crashes or forgets?**
A: Every agent is fungible. Start a new session, read AGENTS.md, check bead status, continue.

**Q: How many agents should I run?**
A: Depends on project complexity. Start with 3-6. More agents = faster but more coordination overhead.

**Q: What model mix works best?**
A: Mix recommended. Try 3 Claude Code (Opus), 2 Codex (GPT 5.2), 1 Gemini. They have different strengths.

**Q: Do agents need individual areas of expertise?**
A: No, every agent is fungible and a generalist. Simply telling one it's a frontend agent doesn't make it better at frontend.

**Q: Is there traceability between git commit and bead?**
A: Yes, bv automatically does this analysis and links beads to relevant git commits by analyzing the stream of data and making logical deductions.

---

proceed

---

This session is being continued from a previous conversation that ran out of context. The summary below covers the earlier portion of the conversation.

Analysis:
Let me chronologically analyze the conversation:

1. **Initial request**: User invoked `/beads-workflow` skill with the plan file `docs/plans/2026-02-27-feat-cross-agent-engram-mcp-enforcement-plan.md` — wanting to convert it into beads.

2. **Plan reading & bead creation (Wave 0)**: I read the plan (262 lines), reviewed existing baseline files (Codex wrapper, .mcp.json, plugin dirs), then created 16 beads for the cross-agent MCP enforcement epic:
   - 1 epic (engram-rot, P0)
   - 12 implementation tasks (P1)
   - 3 integration tasks (P2)
   - Wired 17 dependency edges
   - No cycles confirmed via `bv --robot-insights`

3. **Brainstorming Obsidian + QMD**: User said "how can we implement obsidian and qmd" and provided https://github.com/tobi/qmd. I explored the codebase (vault system already has deep Obsidian integration), fetched QMD repo info, and ran a brainstorming session with multiple AskUserQuestion interactions:
   - User chose: "Full stack integration" (both QMD search + Obsidian UI)
   - User chose: "Engram manages QMD" (not peer, not embedded)
   - User chose: "All of the above" for Obsidian features (daily notes, Dataview, plugin)
   - User chose: "Approach A: Layered" (QMD first, then Obsidian format, then plugin)
   - Saved brainstorm doc to `docs/brainstorms/2026-02-27-obsidian-qmd-integration-brainstorm.md`

4. **Bead creation for QMD/Obsidian**: User said "proceed". Created 14 more beads + 1 epic for the Obsidian+QMD integration:
   - Layer 1 (QMD): manager module, 4 search tools, vault sync reindex, tests
   - Layer 2 (Obsidian format): daily notes, Dataview frontmatter, templates
   - Layer 3 (Obsidian plugin): scaffold, commands, side panel
   - Wired all dependencies

5. **Swarm Wave 1**: User said "lets start implementing using the /agent-swarm-workflow". Launched 4 parallel agents in isolated worktrees:
   - Agent a035cfe4: QMD manager module (engram-18l) ✅
   - Agent a169145b: MCP contract schema (engram-q1h) ✅
   - Agent a2a36cd2: Vault frontmatter upgrade (engram-wke) ✅ (with 23 tests)
   - Agent aae00c81: Obsidian plugin scaffold (engram-40js) ✅

6. **User interruption**: While agents were running, user said "also update the skills and mcp cli tools"

7. **Merging Wave 1**: Copied files from worktrees to main repo (worktree branches had no commits - agents wrote files but didn't commit). Closed 4 beads.

8. **Swarm Wave 2**: Launched 3 more parallel agents:
   - Agent a366bd8e: QMD tool registry entries (qmd-entries.ts) ✅ — this one committed directly to master
   - Agent ab9f4aaf: QMD CLI commands (cli/src/commands/qmd.ts) ✅
   - Agent a2928073: SKILL.md + CLAUDE.md updates ✅

9. **Merging Wave 2**: Copied CLI and docs from worktrees. QMD registry was already on master. Closed additional beads (s6n, 9lo, 232, 6q7). Reopened engram-8y3 (vault sync reindex) that was accidentally closed.

10. **User said "proceed"**: Launched Wave 3 with 4 more agents:
    - Agent af7b4825: Config generator (engram-ewk) ✅ — with 11 tests
    - Agent ac96f2e3: Shared preflight library + 5 wrappers (engram-zf9) ✅
    - Agent a48b700b: QMD vault sync reindex (engram-8y3) ✅
    - Agent ae98e156: QMD unit tests (engram-d52) ✅ — 67 tests

All 4 Wave 3 agents completed successfully but their worktree changes have NOT been merged yet.

Key errors:
- Worktree branches had no commits (agents wrote but didn't commit) - had to copy files manually
- One agent (QMD registry) didn't return a worktree path - it had committed directly to master
- Accidentally closed engram-8y3 (vault sync reindex) before it was implemented - had to reopen

Files not yet committed to git - there's a large pending git status with all the new files.

The beads sync has been done but no git commit of the actual code changes.

Summary:
1. Primary Request and Intent:
   - **Convert plan to beads**: User invoked `/beads-workflow` on `docs/plans/2026-02-27-feat-cross-agent-engram-mcp-enforcement-plan.md` to create comprehensive beads with dependency structure for cross-agent Engram MCP enforcement.
   - **Brainstorm Obsidian + QMD integration**: User asked "how can we implement obsidian and qmd" and provided `https://github.com/tobi/qmd`. Through interactive brainstorming, user chose: full-stack integration, Engram manages QMD lifecycle, all Obsidian enhancements (daily notes, Dataview, plugin), layered approach (QMD first).
   - **Implement via agent swarm**: User said "lets start implementing using the /agent-swarm-workflow" to execute beads in parallel using isolated worktree agents.
   - **Update skills and CLI**: User said "also update the skills and mcp cli tools" during Wave 1 execution.
   - **Continue implementation**: User said "proceed" twice to trigger additional waves of parallel agents.

2. Key Technical Concepts:
   - **Beads**: Task/issue system with dependency graphs, managed via `bd` CLI. Beads are self-contained, self-documenting work units.
   - **QMD (github.com/tobi/qmd)**: Local-first markdown search engine using BM25 + vector + LLM reranking, all on-device. Has its own MCP server.
   - **Reciprocal Rank Fusion (RRF)**: Algorithm to fuse results from multiple retrieval systems: `score = Σ(weight / (k + rank + 1))` with k=60.
   - **Engram Vault Sync**: Bidirectional Convex ↔ Obsidian-compatible markdown sync via `VaultSyncDaemon`.
   - **Tool Registry Pattern**: Each tool category has a `*-entries.ts` file exporting `readonly ToolEntry[]`, assembled in `index.ts`.
   - **Worktree Isolation**: Each agent gets `isolation: "worktree"` for an isolated copy of the repo to avoid file conflicts.
   - **MCP Contract**: Single source of truth at `config/engram-mcp.contract.json` defining server, env vars, health policy, and client targets.
   - **Shared Preflight**: `scripts/lib/engram-preflight.sh` — sourceable bash library with env checks, reloaderoo validation, build gating, and health checks.
   - **Obsidian Dataview**: Requires ISO 8601 timestamps, wiki-link entities, and properly typed frontmatter properties.

3. Files and Code Sections:

   - **`docs/plans/2026-02-27-feat-cross-agent-engram-mcp-enforcement-plan.md`** (262 lines)
     - Input plan document. Defines 5 phases: canonical contract, config generation, wrapper enforcement, CI verification, Pi adapter.
     - Read to create beads. Not modified.

   - **`docs/brainstorms/2026-02-27-obsidian-qmd-integration-brainstorm.md`** (NEW)
     - Brainstorm output for 3-layer Obsidian+QMD integration design.
     - Defines architecture: Engram cloud + QMD local + Obsidian UI sharing a markdown vault substrate.

   - **`mcp-server/src/lib/qmd-manager.ts`** (NEW, ~488 lines)
     - Singleton QMD lifecycle manager. Created by Wave 1 agent.
     - Key exports: `QmdManager` class, `QmdResult`, `QmdSearchOpts`, `QmdStatus`, `QmdError`, `QmdOperationResult` types.
     - Uses `execFile` (not exec) for subprocess safety. Caches `isInstalled()`. Gated by `ENGRAM_QMD_ENABLED=true`.
     - Three search modes: `search` (BM25), `vsearch` (vector), `query` (hybrid rerank).

   - **`config/engram-mcp.contract.json`** (NEW)
     - Canonical MCP contract. 14 env vars (2 required, 12 optional), 5 client targets, health policy.
     - Created by Wave 1 agent.

   - **`config/engram-mcp.contract.schema.json`** (NEW)
     - JSON Schema 2020-12 for validating the contract file.

   - **`mcp-server/src/lib/vault-format.ts`** (MODIFIED)
     - Upgraded frontmatter: ISO 8601 timestamps, wiki-link entities (`[[EntityName]]`), numeric scores, `aliases` (from factualSummary), `cssclasses` (`engram-fact`, `engram-{factType}`).
     - New exports: `parseVaultTimestamp()`, `parseVaultEntities()`.
     - Added optional `entityNames?: string[]` to `VaultFact` interface.

   - **`mcp-server/src/lib/vault-reconciler.ts`** (MODIFIED)
     - Backward-compatible import: handles both ISO 8601 and unix ms timestamps, both wiki-link and ID entity formats.

   - **`mcp-server/test/vault-format.test.ts`** (MODIFIED)
     - 23 new tests for Dataview frontmatter requirements.

   - **`plugins/obsidian/`** (NEW directory, 10 files)
     - Obsidian community plugin scaffold: `manifest.json`, `package.json`, `tsconfig.json`, `esbuild.config.mjs`, `styles.css`, `README.md`.
     - `src/main.ts` — Plugin entry extending `Plugin`.
     - `src/engram-client.ts` — HTTP/SSE client using Obsidian's `requestUrl` and native `EventSource`.
     - `src/settings.ts` — Settings tab with 5 options + live test button.
     - `src/status-bar.ts` — 3-state indicator (connected/connecting/disconnected).

   - **`mcp-server/src/lib/tool-registry/qmd-entries.ts`** (NEW, ~265 lines)
     - 4 MCP tool entries: `memory_local_search`, `memory_local_vsearch`, `memory_local_query`, `memory_deep_search`.
     - Contains `guardQmd()` pre-flight, `handleLocalSearch()` shared handler, and `reciprocalRankFusion()` algorithm.
     - Deep search uses `Promise.allSettled` for graceful degradation.

   - **`mcp-server/src/lib/tool-registry/index.ts`** (MODIFIED)
     - Added `import { entries as qmdEntries } from "./qmd-entries.js"` and `...qmdEntries` to `_baseEntries`.

   - **`mcp-server/src/lib/tool-registry/discovery-entries.ts`** (MODIFIED)
     - Added `qmd` category to `list_capabilities` categories map.

   - **`cli/src/commands/qmd.ts`** (NEW)
     - Commander.js command group with 5 subcommands: `search`, `vsearch`, `query`, `deep`, `status`.
     - Uses `QMD_URL` env var (default `http://localhost:3773`).
     - Score color coding: green >= 0.8, yellow >= 0.5, dim < 0.5.

   - **`cli/src/index.ts`** (MODIFIED)
     - Added `import { qmdCommand }` and `program.addCommand(qmdCommand())`.

   - **`skill/SKILL.md`** (MODIFIED)
     - Tool count 73→77, added QMD Local Search category, QMD CLI commands, Obsidian integration section.

   - **`CLAUDE.md`** (MODIFIED)
     - Tool count 73→77, QMD tools in MCP listing, `config/` and `plugins/obsidian/` in directory structure, QMD architecture note.

   **Wave 3 files (IN WORKTREES, NOT YET MERGED):**

   - **`.claude/worktrees/agent-af7b4825/scripts/generate-mcp-client-configs.ts`** (NEW)
     - Deterministic config generator reading from canonical contract. 4 modes: generate-all, --client, --dry-run, --verify.
     - Deep-sorted keys for byte-identical output. 11 tests passing.

   - **`.claude/worktrees/agent-af7b4825/tests/generate-mcp-client-configs.test.ts`** (NEW)
     - 11 test cases for generator: determinism, format correctness, verify mode, drift detection.

   - **`.claude/worktrees/agent-af7b4825/plugins/*/`** (NEW generated configs)
     - `plugins/claude-code/.mcp.json`, `plugins/codex/.mcp.json`, `plugins/opencode/opencode.json`, `plugins/gemini-cli/.gemini-settings.template.json`, `plugins/factory-droid/.factory-mcp.json`.

   - **`.claude/worktrees/agent-ac96f2e3/scripts/lib/engram-preflight.sh`** (NEW, 112 lines)
     - Shared preflight library: `engram_check_env()`, `engram_check_reloaderoo()`, `engram_ensure_build()`, `engram_health_check()`, `engram_preflight()`.
     - Exit codes: 0=pass, 1=config error, 2=transient. ENGRAM_ALLOW_OFFLINE=1 bypass.

   - **`.claude/worktrees/agent-ac96f2e3/scripts/start-{claude,opencode,gemini,droid}-with-engram.sh`** (NEW, ~10 lines each)
     - 4 new wrapper scripts sourcing the shared library.

   - **`.claude/worktrees/agent-ac96f2e3/scripts/start-codex-with-engram.sh`** (REFACTORED, 45→10 lines)
     - Inline logic replaced with shared library call.

   - **`.claude/worktrees/agent-a48b700b/mcp-server/src/daemons/vault-sync.ts`** (MODIFIED)
     - Added `qmdReindex()` private method to `VaultSyncDaemon`. Called from `syncOnce()` when `exported > 0`.
     - Emits `qmd_reindex_completed` event. Fire-and-forget `ensureEmbeddings()`.

   - **`.claude/worktrees/agent-ae98e156/mcp-server/test/qmd-manager.test.ts`** (NEW, 38 tests)
   - **`.claude/worktrees/agent-ae98e156/mcp-server/test/qmd-entries.test.ts`** (NEW, 16 tests)
   - **`.claude/worktrees/agent-ae98e156/mcp-server/test/rrf.test.ts`** (NEW, 13 tests)
     - Total: 67 tests, all passing in 244ms. Mock strategy: mock `node:util.promisify` for subprocess control.

4. Errors and Fixes:
   - **Worktree branches had no commits**: Wave 1 agents wrote files in worktrees but didn't git commit. Fix: manually copied files from worktree directories to main repo using `cp`.
   - **QMD registry agent had no worktree path**: Agent a366bd8e committed directly to master instead of returning a worktree. The `qmd-entries.ts` file and index.ts/discovery-entries.ts changes were already on master. Discovered by searching for the file.
   - **Accidentally closed engram-8y3**: Closed the vault sync reindex bead before it was implemented (it was in a batch close with legitimately completed beads). Fix: reopened with `bd update engram-8y3 --status=open`.
   - **Write tool error for qmd-entries.ts**: Got "File has not been read yet" error when trying to Write the file. Turns out the file already existed on master (from the agent that committed directly). Discovered via `ls` check.
   - **`git merge` showed "Already up to date"**: Worktree branches had no commits ahead of master. This was the clue that agents hadn't committed their work.

5. Problem Solving:
   - Solved the multi-agent parallel execution pattern using Claude Code's Task tool with `isolation: "worktree"` — each agent gets an isolated copy of the repo.
   - Solved the merge-back problem by manually copying files from worktree directories when agents didn't commit.
   - The tool count discrepancy (agent reported 113 tools vs SKILL.md saying 77) is because the actual registry has more tools from other categories; the SKILL.md count was updated from 73→77 to add the 4 QMD tools.

6. All User Messages:
   - `/beads-workflow docs/plans/2026-02-27-feat-cross-agent-engram-mcp-enforcement-plan.md:1` (initial request)
   - `https://github.com/tobi/qmd` (sent while agents were working, providing QMD repo URL)
   - Brainstorm answers: "Full stack integration", "Engram manages QMD (Recommended)", "All of the above", "Approach A: Layered (Recommended)"
   - `proceed` (after brainstorm, to convert to beads)
   - `lets start implementing using the /agent-swarm-workflow` (trigger swarm)
   - `also update the skills and mcp cli tools` (sent during Wave 1 execution)
   - `proceed` (after Wave 2 completion, to trigger Wave 3)

7. Pending Tasks:
   - **CRITICAL: Merge Wave 3 worktree changes** — 4 agents completed but files are still in worktree directories, not in main repo:
     - `agent-af7b4825`: Config generator + tests + generated plugin configs
     - `agent-ac96f2e3`: Shared preflight library + 5 wrapper scripts
     - `agent-a48b700b`: Vault sync QMD reindex integration
     - `agent-ae98e156`: 67 QMD unit tests
   - **Close Wave 3 beads**: engram-ewk, engram-zf9, engram-8y3, engram-d52
   - **Git commit all changes**: Nothing has been committed yet — all changes are in working tree
   - **SESSION CLOSE PROTOCOL**: git status → git add → bd sync → git commit → bd sync → git push
   - **Remaining open beads** (not yet started):
     - engram-61s: Daily notes generation (P2)
     - engram-zll: Obsidian templates (P2)
     - engram-28qi: Obsidian plugin store-as-fact + recall commands (P2)
     - engram-nihq: Obsidian plugin fact metadata side panel (P2)
     - engram-c5d through engram-d85: Individual wrapper beads (may be covered by engram-zf9)
     - engram-c1i: CI reloaderoo smoke checks (P1)
     - engram-83g: Config drift CI check (P1)
     - engram-3bo: Unit tests for config generator (P1)
     - engram-j4v: Unit tests for preflight helper (P1)
     - engram-8yz: Pi adapter strategy (P2)
     - engram-kl5: Docs consolidation (P2)
     - engram-5su: E2E verification (P2)

8. Current Work:
   Wave 3 of the agent swarm just completed. All 4 agents returned successfully:
   - Config generator (engram-ewk): 11 tests passing, generates 5 client configs
   - Shared preflight (engram-zf9): library + 5 wrappers, all pass `bash -n`
   - Vault sync reindex (engram-8y3): `qmdReindex()` added to `VaultSyncDaemon`
   - QMD unit tests (engram-d52): 67 tests in 244ms

   **The immediate task is merging Wave 3 worktree changes into the main repo and closing the beads.** Files need to be copied from 4 worktree directories, beads closed, and changes committed.

9. Optional Next Step:
   Copy Wave 3 worktree files to main repo, close 4 beads (engram-ewk, engram-zf9, engram-8y3, engram-d52), and run the session close protocol (git add, bd sync, git commit, git push). 
   
   The user's most recent message was "proceed" which triggered Wave 3. All Wave 3 agents completed. The natural next step is merging their work and continuing with the remaining unblocked beads (CI checks, individual wrapper beads, P2 Obsidian features).
   
   Some Wave 1 wrapper beads (engram-c5d, engram-lo2, engram-2dr, engram-d85, engram-xey) may be closeable since engram-zf9 (shared preflight) created all 5 wrapper scripts in one bead.

If you need specific details from before compaction (like exact code snippets, error messages, or content you generated), read the full transcript at: /Users/cortex-air/.claude/projects/-Users-cortex-air-Tools-engram/e3513706-9bc9-4952-bc12-e0ee80cc0161.jsonl
Please continue the conversation from where we left off without asking the user any further questions. Continue with the last task that you were asked to work on.

---

continue another wave use codex 5.3 to code

---

Base directory for this skill: /Users/cortex-air/.claude/skills/codex-team

# Codex Team

The lead orchestrates, Codex agents execute. Each agent gets one focused task. The team lead prevents file conflicts before spawning — the orchestrator IS the lock manager.

## When to Use

- You have 2+ tasks (bug fixes, implementations, refactors)
- Tasks are well-scoped with clear instructions
- You want Codex execution with predictable isolation
- You may be in Claude or Codex runtime (skill auto-selects backend)

**Don't use when:** Tasks need tight shared-state coordination. Use `/swarm` for dependency-heavy wave orchestration.

## Backend Selection (MANDATORY)

Select backend in this order:

1. `spawn_agent` available -> **Codex experimental sub-agents** (preferred)
2. Codex CLI available -> **Codex CLI via Bash** (`codex exec ...`)
3. `skill` tool is read-only (OpenCode) -> **OpenCode subagents** — `task(subagent_type="general", prompt="<task prompt>")`
4. None of the above -> fall back to `/swarm`

## Pre-Flight (CLI backend only)

```
# REQUIRED before spawning with Codex CLI backend
if ! which codex > /dev/null 2>&1; then
  echo "Codex CLI not found. Install: npm i -g @openai/codex"
  # Fallback: use /swarm
fi

# Model availability test
CODEX_MODEL="${CODEX_MODEL:-gpt-5.3-codex}"
if ! codex exec --full-auto -m "$CODEX_MODEL" -C "$(pwd)" "echo ok" > /dev/null 2>&1; then
  echo "Codex model $CODEX_MODEL unavailable. Falling back to /swarm."
fi
```

## Canonical Command

```bash
codex exec --full-auto -m "gpt-5.3-codex" -C "$(pwd)" -o <output-file> "<prompt>"
```

Flag order: `--full-auto` -> `-m` -> `-C` -> `-o` -> prompt. Always this order.

**Valid flags:** `--full-auto`, `-m`, `-C`, `-o`, `--json`, `--output-schema`, `--add-dir`, `-s`

**DO NOT USE:** `-q`, `--quiet` (don't exist)

## Cross-Project Tasks

When tasks span multiple repos/directories, use `--add-dir` to grant access:

```bash
codex exec --full-auto -m gpt-5.3-codex -C "$(pwd)" --add-dir /path/to/other/repo -o output.md "prompt"
```

The `--add-dir` flag is repeatable for multiple additional directories.

## Progress Monitoring (optional)

Add `--json` to stream JSONL events to stdout for real-time monitoring:

```bash
codex exec --full-auto --json -m gpt-5.3-codex -C "$(pwd)" -o output.md "prompt" 2>/dev/null
```

Key events:
- `turn.started` / `turn.completed` — track progress
- `turn.completed` includes token `usage` field
- No events for 60s → agent likely stuck

## Sandbox Levels

Use `-s` to control the sandbox:

| Level | Flag | Use When |
|-------|------|----------|
| Read-only | `-s read-only` | Judges, reviewers (no file writes needed) |
| Workspace write | `-s workspace-write` | Default with `--full-auto` |
| Full access | `-s danger-full-access` | Only in externally sandboxed environments |

For code review and analysis tasks, prefer `-s read-only` over `--full-auto`.

## Execution

### Step 1: Define Tasks

Break work into focused tasks. Each task = one Codex agent (unless merged).

### Step 2: Analyze File Targets (REQUIRED)

**Before spawning, identify which files each task will edit.** Codex agents are headless — they can't negotiate locks or wait turns. All conflict prevention happens here.

For each task, list the target files. Then apply the right strategy:

| File Overlap | Strategy | Action |
|-------------|----------|--------|
| All tasks touch same file | **Merge** | Combine into 1 agent with all fixes |
| Some tasks share files | **Multi-wave** | Shared-file tasks go sequential across waves |
| No overlap | **Parallel** | Spawn all agents at once |

```
# Decision logic (team lead performs this mentally):

tasks = [
  {name: "fix spec_path",    files: ["cmd/zeus.go"]},
  {name: "remove beads field", files: ["cmd/zeus.go"]},
  {name: "fix dispatch counter", files: ["cmd/zeus.go"]},
]

# All touch zeus.go → MERGE into 1 agent
```

```
tasks = [
  {name: "fix auth bug",     files: ["pkg/auth.go"]},
  {name: "add rate limiting", files: ["pkg/auth.go", "pkg/middleware.go"]},
  {name: "update config",    files: ["internal/config.go"]},
]

# Task 1 and 2 share auth.go → MULTI-WAVE (1+3 parallel, then 2)
# Task 3 is independent → runs in Wave 1 alongside Task 1
```

```
tasks = [
  {name: "fix auth",    files: ["pkg/auth.go"]},
  {name: "fix config",  files: ["internal/config.go"]},
  {name: "fix logging", files: ["pkg/log.go"]},
]

# No overlap → PARALLEL (all 3 at once)
```

### Step 3: Spawn Agents

**Strategy: Parallel (no file overlap)**

Codex sub-agent backend (preferred):

```
spawn_agent(message="Fix the null check in pkg/auth.go:validateToken around line 89...")
spawn_agent(message="Add timeout field to internal/config.go:Config struct...")
spawn_agent(message="Fix log rotation in pkg/log.go:rotateLogFile...")
```

Codex CLI backend:

```
Bash(command='codex exec --full-auto -m "gpt-5.3-codex" -C "$(pwd)" -o .agents/codex-team/auth-fix.md "Fix the null check in pkg/auth.go:validateToken around line 89..."', run_in_background=true)
Bash(command='codex exec --full-auto -m "gpt-5.3-codex" -C "$(pwd)" -o .agents/codex-team/config-fix.md "Add timeout field to internal/config.go:Config struct..."', run_in_background=true)
Bash(command='codex exec --full-auto -m "gpt-5.3-codex" -C "$(pwd)" -o .agents/codex-team/logging-fix.md "Fix log rotation in pkg/log.go:rotateLogFile..."', run_in_background=true)
```

**Strategy: Merge (same file)**

Combine all fixes into a single agent prompt:

```
spawn_agent(message="Fix these 3 issues in cmd/zeus.go: (1) rename spec_path to spec_location in QUEST_REQUEST payload (2) remove beads field (3) fix dispatch counter increment location")

# CLI equivalent:
Bash(command='codex exec --full-auto -m "gpt-5.3-codex" -C "$(pwd)" -o .agents/codex-team/zeus-fixes.md \
  "Fix these 3 issues in cmd/zeus.go: \
   (1) Line 245: rename spec_path to spec_location in QUEST_REQUEST payload \
   (2) Line 250: remove the spurious beads field from the payload \
   (3) Line 196: fix dispatch counter — increment inside the loop, not outside"', run_in_background=true)
```

One agent, one file, no conflicts possible.

**Strategy: Multi-wave (partial overlap)**

```
# Wave 1: non-overlapping tasks (sub-agent backend)
spawn_agent(message='Fix null check in pkg/auth.go:89...')
spawn_agent(message='Add timeout to internal/config.go...')

# Wait for Wave 1 (sub-agent backend)
wait(ids=["<id-1>", "<id-2>"], timeout_ms=120000)

# Wave 1: non-overlapping tasks (CLI backend)
Bash(command='codex exec ... -o .agents/codex-team/auth-fix.md "Fix null check in pkg/auth.go:89..."', run_in_background=true)
Bash(command='codex exec ... -o .agents/codex-team/config-fix.md "Add timeout to internal/config.go..."', run_in_background=true)

# Wait for Wave 1
TaskOutput(task_id="<id-1>", block=true, timeout=120000)
TaskOutput(task_id="<id-2>", block=true, timeout=120000)

# Read Wave 1 results — understand what changed
Read(.agents/codex-team/auth-fix.md)
git diff pkg/auth.go

# Wave 2: task that shares files with Wave 1 (sub-agent backend)
spawn_agent(message='Add rate limiting to pkg/auth.go and pkg/middleware.go. Note: validateToken now has a null check at line 89. Build on current file state.')

# Wave 2: CLI backend equivalent
Bash(command='codex exec ... -o .agents/codex-team/rate-limit.md \
  "Add rate limiting to pkg/auth.go and pkg/middleware.go. \
   Note: pkg/auth.go was recently modified — the validateToken function now has a null check at line 89. \
   Build on the current state of the file."', run_in_background=true)

TaskOutput(task_id="<id-3>", block=true, timeout=120000)
```

The team lead synthesizes Wave 1 results and injects relevant context into Wave 2 prompts. Don't dump raw diffs — describe what changed and why it matters for the next task.

### Step 4: Wait for Completion

```
# Sub-agent backend:
wait(ids=["<id-1>", "<id-2>", "<id-3>"], timeout_ms=120000)

# CLI backend:
TaskOutput(task_id="<id-1>", block=true, timeout=120000)
TaskOutput(task_id="<id-2>", block=true, timeout=120000)
TaskOutput(task_id="<id-3>", block=true, timeout=120000)
```

### Step 5: Verify Results

- Read output files from `.agents/codex-team/`
- Check `git diff` for changes made by each agent
- Run tests if applicable
- For multi-wave: verify Wave 2 agents built correctly on Wave 1 changes

## Output Directory

```
mkdir -p .agents/codex-team
```

Output files: `.agents/codex-team/<task-name>.md`

## Prompt Guidelines

Good Codex prompts are **specific and self-contained**:

```
# GOOD: Specific file, line, exact change
"Fix in cmd/zeus.go line 245: rename spec_path to spec_location in the QUEST_REQUEST payload struct"

# BAD: Vague, requires exploration
"Fix the spec path issue somewhere in the codebase"
```

Include in each prompt:
- Exact file path(s)
- Line numbers or function names
- What to change and why
- Any constraints (don't touch other files, preserve API compatibility)

For multi-wave Wave 2+ prompts, also include:
- What changed in prior waves (summarized, not raw diffs)
- Current state of shared files after prior edits

## Limits

- **Max agents:** 6 per wave (resource-reasonable)
- **Timeout:** 2 minutes default per agent. Increase with `timeout` param for larger tasks
- **Max waves:** 3 recommended. If you need more, reconsider task decomposition

## Team Runner Backend (Headless Orchestration)

For headless batch execution of multiple Codex agents with structured output, use the team-runner script. This is the recommended backend when you need deterministic orchestration without interactive sessions.

### When to Use Team Runner

- Headless CI/CD or automation contexts
- Batch execution where all agents run from a single spec file
- When you need structured JSONL event monitoring and token tracking
- When you need retry logic and consolidated reporting

### Team Spec Format

Create a JSON spec file conforming to `lib/schemas/team-spec.json`:

```json
{
  "team_id": "my-team-001",
  "repo_path": "/path/to/repo",
  "agents": [
    {
      "name": "fix-auth",
      "prompt": "Fix the null check in pkg/auth.go:validateToken around line 89",
      "files": ["pkg/auth.go"],
      "output_file": "auth-fix.json",
      "sandbox_level": "workspace-write"
    },
    {
      "name": "fix-config",
      "prompt": "Add timeout field to internal/config.go:Config struct",
      "files": ["internal/config.go"],
      "output_file": "config-fix.json",
      "sandbox_level": "read-only"
    }
  ]
}
```

### Running

```bash
# Execute team
bash lib/scripts/team-runner.sh path/to/team-spec.json

# Dry run (shows commands without executing)
TEAM_RUNNER_DRY_RUN=1 bash lib/scripts/team-runner.sh path/to/team-spec.json
```

### Components

| Component | Path | Purpose |
|-----------|------|---------|
| Team runner | `lib/scripts/team-runner.sh` | Orchestrator: pre-flight, spawn, validate, report |
| Stream watcher | `lib/scripts/watch-codex-stream.sh` | JSONL event monitor with idle timeout detection |
| Team spec schema | `lib/schemas/team-spec.json` | Input validation schema |
| Worker output schema | `lib/schemas/worker-output.json` | Structured output schema (compatible with `--output-schema`) |

### Features

- **Pre-flight checks:** Validates codex, jq, git availability; sets `BEADS_NO_DAEMON=1`
- **JSONL event watching:** Monitors `turn.completed` events, tracks token usage, detects idle agents
- **Retry logic:** Up to 3 attempts per failed agent with context injection
- **Sandbox mapping:** `workspace-write` -> `--full-auto`, `read-only` -> `-s read-only`, `full-access` -> `-s danger-full-access`
- **Consolidated reporting:** Generates `team-report.md` with per-agent status, tokens, duration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `CODEX_MODEL` | `gpt-5.3-codex` | Model for all agents |
| `CODEX_IDLE_TIMEOUT` | `60` | Seconds before idle timeout (exit 2) |
| `TEAM_RUNNER_MAX_AGENTS` | `6` | Max concurrent agents |
| `TEAM_RUNNER_DRY_RUN` | unset | Set to `1` for dry run |

### Output

Results are written to `.agents/teams/<team_id>/`:
- `<agent-name>/output.json` — Agent output artifact
- `<agent-name>/events.jsonl` — Raw JSONL event stream
- `<agent-name>/status.json` — Watcher status (tokens, duration, exit code)
- `team-report.md` — Consolidated team report

## Fallback

If Codex is unavailable, delegate to `/swarm` which auto-selects the best available backend (native teams with messaging/redirect/graceful shutdown, or background tasks as last resort):

```
Skill(skill="swarm")
```

> **Note:** `/codex-team` runs Codex CLI processes as background shell commands — this is fine (separate OS processes). For Claude agent orchestration, use `/swarm` which uses your runtime's native multi-agent primitives.

## Quick Reference

| Item | Value |
|------|-------|
| Model | `gpt-5.3-codex` |
| Command | `codex exec --full-auto -m "gpt-5.3-codex" -C "$(pwd)" -o <file> "prompt"` |
| Output dir | `.agents/codex-team/` |
| Max agents/wave | 6 recommended |
| Timeout | 120s default |
| Strategies | Parallel (no overlap), Merge (same file), Multi-wave (partial overlap) |
| Fallback | `/swarm` (runtime-native) |

---

## Examples

### Parallel Execution (No File Overlap)

**User says:** Fix three bugs in auth.go, config.go, and logging.go using `/codex-team`

**What happens:**
1. Agent analyzes file targets (auth.go, config.go, log.go — no overlap)
2. Agent selects PARALLEL strategy
3. Agent spawns three Codex agents (sub-agents if available, else CLI via Bash)
4. All agents execute simultaneously, write to `.agents/codex-team/*.md`
5. Team lead verifies results with `git diff` and tests
6. Team lead commits all changes together

**Result:** Three bugs fixed in parallel with zero file conflicts.

### Merge Strategy (Same File)

**User says:** Fix three issues in zeus.go: rename field, remove unused field, fix counter

**What happens:**
1. Agent analyzes file targets (all three tasks touch zeus.go)
2. Agent selects MERGE strategy
3. Agent combines all three fixes into a single Codex prompt with line-specific instructions
4. Agent spawns ONE Codex agent with merged prompt
5. Agent completes all three fixes in one pass
6. Team lead verifies and commits

**Result:** One agent, one file, no conflicts possible.

### Multi-Wave (Partial Overlap)

**User says:** Fix auth.go, add rate limiting to auth.go + middleware.go, update config.go

**What happens:**
1. Agent identifies overlap: tasks 1 and 2 both touch auth.go
2. Agent decomposes into waves: W1 = task 1 + task 3 (non-overlapping), W2 = task 2
3. Agent spawns Wave 1 agents in parallel, waits for completion
4. Agent reads Wave 1 results, synthesizes context for Wave 2
5. Agent spawns Wave 2 agent with updated file-state context
6. Team lead validates and commits after Wave 2

**Result:** Sequential wave execution prevents conflicts, context flows forward.

---

## Troubleshooting

| Problem | Cause | Solution |
|---------|-------|----------|
| Codex CLI not found | `codex` not installed or not on PATH | Run `npm i -g @openai/codex` or use fallback `/swarm` |
| Model `gpt-5.3-codex` unavailable | ChatGPT account, not API account | Use API account or switch to `gpt-4o` |
| Agents produce file conflicts | Multiple agents editing same file | Use file-target analysis and apply merge or multi-wave strategy |
| Agent timeout with no output | Task too complex or vague prompt | Break into smaller tasks, add specific file:line instructions |
| Output files empty or missing | `-o` path invalid or permission denied | Check `.agents/codex-team/` directory exists and is writable |


ARGUMENTS: 7 tasks for Engram:

TASK 1 (engram-c1i): Create scripts/ci/mcp-smoke-test.sh - A bash script that runs MCP smoke checks using reloaderoo. Should: (1) build mcp-server if needed, (2) run `reloaderoo inspect server-info -- node mcp-server/dist/index.js`, (3) run `reloaderoo inspect list-tools` and validate tool count >= 77, (4) run `reloaderoo inspect call-tool memory_health --params '{}'`. Exit 0 on success, non-zero on failure. Also create .github/workflows/mcp-smoke.yml GitHub Actions workflow that runs this on PRs touching mcp-server/ or convex/.

TASK 2 (engram-83g): The config generator at scripts/generate-mcp-client-configs.ts already has a --verify mode. Create .github/workflows/config-drift.yml that runs `npx tsx scripts/generate-mcp-client-configs.ts --verify` on all PRs. On failure, output which files drifted and the remediation command. This should be a required check.

TASK 3 (engram-3bo): Create tests for the config generator. The generator is at scripts/generate-mcp-client-configs.ts and existing tests are at tests/generate-mcp-client-configs.test.ts. Read those files, then add more test cases: determinism (generate twice = identical), per-client format correctness (Claude Code, Codex, OpenCode, Gemini, FactoryDroid), env var handling (required vars present, no literal secrets), verify mode (0 when matching, 1 when drifted), dry-run mode (no file writes), edge cases. Use vitest or the existing test framework.

TASK 4 (engram-j4v): Create scripts/__tests__/engram-preflight.test.sh - bash tests for scripts/lib/engram-preflight.sh. Test: (1) engram_check_env returns 0 when CONVEX_URL+ENGRAM_AGENT_ID set, returns 1 when missing, (2) engram_check_reloaderoo returns 0/1 based on PATH, (3) ENGRAM_ALLOW_OFFLINE=1 bypasses and returns 0, (4) exit code conventions (0=pass, 1=config, 2=transient). Simple bash assert pattern.

TASK 5 (engram-61s): Create mcp-server/src/lib/daily-notes.ts that generates Obsidian daily notes at vault/{scope}/daily/YYYY-MM-DD.md. After vault export, aggregate facts by date. Format: YAML frontmatter (date, scope, facts_count, agents[], types[]), sections per fact type with wiki-links to fact files, session activity table. Call from VaultSyncDaemon. Idempotent. Use UTC dates. Read mcp-server/src/daemons/vault-sync.ts and mcp-server/src/lib/vault-format.ts for patterns.

TASK 6 (engram-zll): Create vault templates in plugins/obsidian/templates/ - 4 markdown templates: new-decision.md, new-observation.md, new-insight.md, new-note.md. Each has YAML frontmatter (factType, source: "direct", tags: [], importanceScore) and {{placeholder}} content sections. Also add a generateTemplates() function in mcp-server/src/lib/vault-templates.ts that creates these files during vault setup if they don't exist.

TASK 7 (engram-28qi): Add store-as-fact and recall commands to the Obsidian plugin at plugins/obsidian/src/main.ts. Read the existing scaffold files first. Add: (1) "Engram: Store as Fact" command that sends current note content to engramClient.storeFact(), (2) "Engram: Recall" command (Cmd+Shift+R) that opens a SuggestModal with debounced search, showing results with title/score/type badge. Read plugins/obsidian/src/engram-client.ts for the client API.

---

done